[
  {
    "claim_id": 1,
    "claim": "Summary: This paper introduces Self-Retrieval, an end-to-end IR system driven entirely by a single LLM.",
    "evidence": [
      "Recently, information retrieval (IR) systems and large language models (LLMs) have witnessed a growing synergy, with advancements in one field driving progress in the other [13, 56]. On one hand, IR systems have proven effective in augmenting LLMs and mitigating challenges such as hallucinations and outdated knowledge [22, 16]. By providing accurate, up-to-date external knowledge, IR systems significantly enhance the reliability and performance of LLMs. On the other hand, the powerful language understanding and generation capabilities of LLMs have been leveraged to enhance almost all components of traditional IR systems-indexing, retrieval [42, 9, 26], and reranking [58, 27, 40]. Through the integration of LLMs into the IR pipeline, these systems achieve substantially improved retrieval accuracy [57, 1].\n\nHowever, current IR systems typically adopt a pipeline architecture where different components operate in isolation, limiting LLMs' role to specific components rather than leveraging their full\n\n∗ Equally Contribution.\n\n† Corresponding authors.\n\n3 The code of this work is available at https://github.com/icip-cas/SelfRetrieval .\n\n: The Self-Retrieval framework consists of three key components: (1) corpus indexing through self-supervised learning, (2) passage generation via constrained decoding, (3) passage ranking using self-assessment scoring.\n\n<!-- image -->\n\npotential across the entire system. This fragmented approach creates several challenges: it hinders knowledge sharing between components, prevents deep integration of LLMs' diverse capabilities, and results in complex implementations with potentially sub-optimal performance. These limitations underscore the need for a more unified approach that fully integrates LLMs across all components of the IR system. Such an approach would not only maximize the utility of LLMs' capabilities but also simplify system implementation while potentially achieving better performance through enhanced component synergy.",
      "We evaluate Self-Retrieval on three representative retrieval benchmarks: NQ, TriviaQA, and MS MARCO. Experimental results demonstrate that Self-Retrieval substantially outperforms existing sparse retrieval, dense retrieval, and generative retrieval methods on both document-level and passage-level retrieval tasks. Furthermore, our experiments on retrieval-augmented generation tasks reveal that Self-Retrieval considerably enhances downstream performance. Additionally, larger LLMs lead to progressively better performance in Self-Retrieval, showing clear scaling benefits. These results demonstrate the effectiveness of Self-Retrieval across different retrieval tasks and application scenarios.\n\nThe potential impacts of this paper may include the following aspects. First, we introduce SelfRetrieval, an end-to-end architecture that consolidates the entire information retrieval system within a single large language model. This unified approach demonstrates substantial performance improvements over existing IR methods. Second, the corpus internalization and indexing mechanism of Self-Retrieval establishes a new paradigm to memorize, organize and retrieve the learned documents (at least part of them) during the pre-training phase, paving the way for more transparent and trustworthy text generation from LLMs. Third, as a LLM-driven retrieval system, Self-Retrieval offers inherent advantages in terms of compatibility, consistency, and interaction with LLMs' internal knowledge. Through experiments on RAG, we demonstrate how this natural compatibility leads to superior performance, suggesting broader potential for enhancing various LLM-based applications.\n\n## 2 Related Work",
      "Training Self-Retrieval unifies the three distinct tasks of information retrieval - indexing, retrieval, and reranking - into text generation tasks, trained using cross-entropy loss in an auto-regressive manner. Specifically, Self-Retrieval first internalizes the corpus into its parameters through selfsupervised learning as introduced in Section 3.1. Subsequently, in addition to a portion of selfsupervised instances, it incorporates two different types of data to build retrieval and reranking abilities:\n\n- Retrieval data: Utilizes supervised query-passage pairs from the dataset, where the model learns to generate both document titles and passage content in response to input queries.\n- Reranking data: Employs positive and negative examples to train the model in relevance assessment between queries and passages.\n\nThis auto-regressive training approach enables Self-Retrieval to integrate traditionally separate IR components into a unified language model, establishing an end-to-end IR system.\n\nFurthermore, leveraging the universal language generation capabilities of LLMs, we can seamlessly integrate downstream task components, such as readers in RAG, into Self-Retrieval. This integration can be achieved by simply appending the golden answer after the assessment in Self-Retrieval. Consequently, the LLM can function as a comprehensive RAG system, effectively reducing the knowledge gap between IR system and reader modules.\n\nInference During inference, given an input query, Self-Retrieval aims to obtain the relevant passages that are sorted based on the relevance to query. Firstly, the model generates i document titles through constrained beam search. Secondly, for each title, it generates j passages using beam search. Finally, the resulting i × j passages are scored using the self-assessment mechanism and reranked to produce the final output.\n\n## 4 Experimental Results\n\n## 4.1 Experimental Setup",
      "LLMfor IR Recent studies have explored leveraging LLMs to enhance various components of IR systems, including query rewriting, retrieval, and reranking. For query rewriting, LLMs have been employed to generate pseudo-documents for query expansion [46] and to rewrite queries based on conversational context [15]. In the retrieval stage, researchers have explored augmenting data by generating pseudo-queries [6, 17] or relevance labels [25] using LLMs, as well as employing LLMs directly as generative retrievers [42, 5]. Regarding reranking, LLMs have been utilized in two ways: serving as rerankers directly [27, 40] and augmenting the reranking dataset [12]. While these methods have advanced specific components within the IR pipeline, Self-Retrieval distinguishes itself by presenting an end-to-end architecture driven entirely by a single LLM, eliminating the need for external components.\n\nDense retrieval Dense retrieval models retrieve information by matching dense vector representations of queries and documents [19]. In this paradigm, an encoder transforms both queries and documents into dense vectors, with relevance determined by their vector distance. Various strategies have been proposed to enhance dense retrievers, including designing loss functions [45], multivector [38], training with synthetic queries [33, 47], and leveraging large-scale query-document pairs [30, 50]. Recent work has also explored using large language models to generate dense vectors for both queries and documents [29]. However, the fundamental limitation of dense retrieval lies in its limited interaction with LLMs, as the compression of natural language into dense vectors inherently constrains the utilization of LLMs' sophisticated language understanding and semantic inference capabilities."
    ],
    "verification": {
      "result": "Supported",
      "justification": "The evidence directly supports the claim about Self-Retrieval being an end-to-end IR system driven by a single LLM.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 2,
    "claim": "The authors provide experimental evidence showing that Self-Retrieval outperforms traditional sparse, dense, and generative retrieval methods on benchmark datasets like NQ and TriviaQA.",
    "evidence": [
      "We evaluate Self-Retrieval on three representative retrieval benchmarks: NQ, TriviaQA, and MS MARCO. Experimental results demonstrate that Self-Retrieval substantially outperforms existing sparse retrieval, dense retrieval, and generative retrieval methods on both document-level and passage-level retrieval tasks. Furthermore, our experiments on retrieval-augmented generation tasks reveal that Self-Retrieval considerably enhances downstream performance. Additionally, larger LLMs lead to progressively better performance in Self-Retrieval, showing clear scaling benefits. These results demonstrate the effectiveness of Self-Retrieval across different retrieval tasks and application scenarios.\n\nThe potential impacts of this paper may include the following aspects. First, we introduce SelfRetrieval, an end-to-end architecture that consolidates the entire information retrieval system within a single large language model. This unified approach demonstrates substantial performance improvements over existing IR methods. Second, the corpus internalization and indexing mechanism of Self-Retrieval establishes a new paradigm to memorize, organize and retrieve the learned documents (at least part of them) during the pre-training phase, paving the way for more transparent and trustworthy text generation from LLMs. Third, as a LLM-driven retrieval system, Self-Retrieval offers inherent advantages in terms of compatibility, consistency, and interaction with LLMs' internal knowledge. Through experiments on RAG, we demonstrate how this natural compatibility leads to superior performance, suggesting broader potential for enhancing various LLM-based applications.\n\n## 2 Related Work",
      "Scaling corpus size Recent studies [35, 53] have demonstrated that generative retrieval methods such as DSI or NCI experience more significant performance degradation compared to dense retrieval methods when scaled to larger corpora. To explore the impact of corpus size on Self-Retrieval,\n\n: Impact of model capacity on SelfRetrieval performance.\n\n<!-- image -->\n\n<!-- image -->\n\n: Reranking performance comparison when processing top-100 passages.\n\n<!-- image -->\n\n: Scalability analysis of retrieval performance for Self-Retrieval and BGE-FT across varying corpus sizes.\n\nwe expand our experiments from 10K to 200K documents, scaling the number of passages from 290K to 3M.  illustrates the performance trends of BGE-FT and our Self-Retrieval 3B model on the NQ and TriviaQA datasets with increasing corpus sizes. While both models show performance decrease with larger corpus sizes, Self-Retrieval maintains a degradation rate comparable to BGE-FT. As the number of documents continues to increase, the degradation rate gradually diminishes, demonstrating Self-Retrieval's potential scalability to larger document collections. This observation indicates that Self-Retrieval effectively addresses some of the inherent limitations of generative retrieval approaches in large-scale scenarios.",
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "Analysis on reranking In this part, we conduct an in-depth analysis of the reranking performance of Self-Retrieval reranker module in comparison with the fine-tuned BGE-Reranker. We employ DPR-FT, SEAL and GritLM to retrieve 100 passages on TriviaQA, followed by reranking the retrieved results using both approaches. We evaluate performance using MRR@5 as the metric. The experimental results are presented in . The results reveal two key findings: (1) Reranking plays a crucial role in information retrieval systems, significantly enhancing the ranking performance across all models. (2) The Self-Retrieval reranker consistently outperforms the fine-tuned BGE Reranker in most scenarios, demonstrating its robustness and effectiveness. These findings demonstrate that Self-Retrieval performs effectively both as a complete IR system and as a reranker component.\n\nIn Appendix C, we conduct additional experiments with a chunk size of 100 words, demonstrating Self-Retrieval's adaptability to different text segmentation strategies. In Appendix E, we further discuss Self-Retrieval's computational efficiency.\n\n## 5 Conclusion\n\nIn this paper, we propose Self-Retrieval, an end-to-end LLM-driven information retrieval architecture that unifies indexing, retrieval, and reranking in a single LLM. This approach enables the LLM to internalize the corpus, generate relevant content, and perform self-assessment within a unified framework. Unlike previous works that incorporate LLMs into individual IR components, SelfRetrieval provides a unified framework for the entire IR procedure, facilitating knowledge sharing and deep collaboration among different components. Experimental results demonstrate that SelfRetrieval achieves strong performance across various retrieval benchmarks and application scenarios. In future work, we plan to extend our method to further enhance the reliability and trustworthiness of LLM generation.\n\n## Limitations"
    ],
    "verification": {
      "result": "Supported",
      "justification": "The evidence directly supports the claim about Self-Retrieval outperforming methods on NQ and TriviaQA.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 3,
    "claim": "The integration of all IR functions into a single LLM is a novel contribution that leverages the inherent capabilities of LLMs across the full spectrum of IR tasks, offering a streamlined and potentially more effective approach.",
    "evidence": [
      "Recently, information retrieval (IR) systems and large language models (LLMs) have witnessed a growing synergy, with advancements in one field driving progress in the other [13, 56]. On one hand, IR systems have proven effective in augmenting LLMs and mitigating challenges such as hallucinations and outdated knowledge [22, 16]. By providing accurate, up-to-date external knowledge, IR systems significantly enhance the reliability and performance of LLMs. On the other hand, the powerful language understanding and generation capabilities of LLMs have been leveraged to enhance almost all components of traditional IR systems-indexing, retrieval [42, 9, 26], and reranking [58, 27, 40]. Through the integration of LLMs into the IR pipeline, these systems achieve substantially improved retrieval accuracy [57, 1].\n\nHowever, current IR systems typically adopt a pipeline architecture where different components operate in isolation, limiting LLMs' role to specific components rather than leveraging their full\n\n∗ Equally Contribution.\n\n† Corresponding authors.\n\n3 The code of this work is available at https://github.com/icip-cas/SelfRetrieval .\n\n: The Self-Retrieval framework consists of three key components: (1) corpus indexing through self-supervised learning, (2) passage generation via constrained decoding, (3) passage ranking using self-assessment scoring.\n\n<!-- image -->\n\npotential across the entire system. This fragmented approach creates several challenges: it hinders knowledge sharing between components, prevents deep integration of LLMs' diverse capabilities, and results in complex implementations with potentially sub-optimal performance. These limitations underscore the need for a more unified approach that fully integrates LLMs across all components of the IR system. Such an approach would not only maximize the utility of LLMs' capabilities but also simplify system implementation while potentially achieving better performance through enhanced component synergy.",
      "LLMfor IR Recent studies have explored leveraging LLMs to enhance various components of IR systems, including query rewriting, retrieval, and reranking. For query rewriting, LLMs have been employed to generate pseudo-documents for query expansion [46] and to rewrite queries based on conversational context [15]. In the retrieval stage, researchers have explored augmenting data by generating pseudo-queries [6, 17] or relevance labels [25] using LLMs, as well as employing LLMs directly as generative retrievers [42, 5]. Regarding reranking, LLMs have been utilized in two ways: serving as rerankers directly [27, 40] and augmenting the reranking dataset [12]. While these methods have advanced specific components within the IR pipeline, Self-Retrieval distinguishes itself by presenting an end-to-end architecture driven entirely by a single LLM, eliminating the need for external components.\n\nDense retrieval Dense retrieval models retrieve information by matching dense vector representations of queries and documents [19]. In this paradigm, an encoder transforms both queries and documents into dense vectors, with relevance determined by their vector distance. Various strategies have been proposed to enhance dense retrievers, including designing loss functions [45], multivector [38], training with synthetic queries [33, 47], and leveraging large-scale query-document pairs [30, 50]. Recent work has also explored using large language models to generate dense vectors for both queries and documents [29]. However, the fundamental limitation of dense retrieval lies in its limited interaction with LLMs, as the compression of natural language into dense vectors inherently constrains the utilization of LLMs' sophisticated language understanding and semantic inference capabilities.",
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "1\n\n## Self-Retrieval: End-to-End Information Retrieval with One Large Language Model\n\nQiaoyu Tang 1 , 2 ∗ , Jiawei Chen 1 , 2 ∗ , Zhuoqun Li 1 , 2 , Bowen Yu 3 , Yaojie Lu 1 , Cheng Fu 3 , Haiyang Yu 3 , Hongyu Lin 1 † , Fei Huang 3 , Ben He 1 , 2 , Xianpei Han 1 † , Le Sun 1 , Yongbin Li 3 † Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences 2 University of Chinese Academy of Sciences 3 Alibaba Group {tangqiaoyu2020,jiawei2020,lizhuoqun2021}@iscas.ac.cn {luyaojie,hongyu,xianpei,sunle}@iscas.ac.cn {yubowen.ybw,fucheng.fuc,yifei.yhy,f.huang,shuide.lyb}@alibaba-inc.com benhe@ucas.ac.cn\n\n## Abstract\n\nThe rise of large language models (LLMs) has significantly transformed both the construction and application of information retrieval (IR) systems. However, current interactions between IR systems and LLMs remain limited, with LLMs merely serving as part of components within IR systems, and IR systems being constructed independently of LLMs. This separated architecture restricts knowledge sharing and deep collaboration between them. In this paper, we introduce Self-Retrieval , a novel end-to-end LLM-driven information retrieval architecture. Self-Retrieval unifies all essential IR functions within a single LLM, leveraging the inherent capabilities of LLMs throughout the IR process. Specifically, Self-Retrieval internalizes the retrieval corpus through self-supervised learning, transforms the retrieval process into sequential passage generation, and performs relevance assessment for reranking. Experimental results demonstrate that SelfRetrieval not only outperforms existing retrieval approaches by a significant margin, but also substantially enhances the performance of LLM-driven downstream applications like retrieval-augmented generation. 3\n\n## 1 Introduction"
    ],
    "verification": {
      "result": "Supported",
      "justification": "Evidence shows Self-Retrieval integrates IR functions into a single LLM, enhancing performance and streamlining processes.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 4,
    "claim": "The concept of Self-Retrieval is introduced clearly, making it accessible to readers.",
    "evidence": [
      "Training Self-Retrieval unifies the three distinct tasks of information retrieval - indexing, retrieval, and reranking - into text generation tasks, trained using cross-entropy loss in an auto-regressive manner. Specifically, Self-Retrieval first internalizes the corpus into its parameters through selfsupervised learning as introduced in Section 3.1. Subsequently, in addition to a portion of selfsupervised instances, it incorporates two different types of data to build retrieval and reranking abilities:\n\n- Retrieval data: Utilizes supervised query-passage pairs from the dataset, where the model learns to generate both document titles and passage content in response to input queries.\n- Reranking data: Employs positive and negative examples to train the model in relevance assessment between queries and passages.\n\nThis auto-regressive training approach enables Self-Retrieval to integrate traditionally separate IR components into a unified language model, establishing an end-to-end IR system.\n\nFurthermore, leveraging the universal language generation capabilities of LLMs, we can seamlessly integrate downstream task components, such as readers in RAG, into Self-Retrieval. This integration can be achieved by simply appending the golden answer after the assessment in Self-Retrieval. Consequently, the LLM can function as a comprehensive RAG system, effectively reducing the knowledge gap between IR system and reader modules.\n\nInference During inference, given an input query, Self-Retrieval aims to obtain the relevant passages that are sorted based on the relevance to query. Firstly, the model generates i document titles through constrained beam search. Secondly, for each title, it generates j passages using beam search. Finally, the resulting i × j passages are scored using the self-assessment mechanism and reranked to produce the final output.\n\n## 4 Experimental Results\n\n## 4.1 Experimental Setup",
      "We evaluate Self-Retrieval on three representative retrieval benchmarks: NQ, TriviaQA, and MS MARCO. Experimental results demonstrate that Self-Retrieval substantially outperforms existing sparse retrieval, dense retrieval, and generative retrieval methods on both document-level and passage-level retrieval tasks. Furthermore, our experiments on retrieval-augmented generation tasks reveal that Self-Retrieval considerably enhances downstream performance. Additionally, larger LLMs lead to progressively better performance in Self-Retrieval, showing clear scaling benefits. These results demonstrate the effectiveness of Self-Retrieval across different retrieval tasks and application scenarios.\n\nThe potential impacts of this paper may include the following aspects. First, we introduce SelfRetrieval, an end-to-end architecture that consolidates the entire information retrieval system within a single large language model. This unified approach demonstrates substantial performance improvements over existing IR methods. Second, the corpus internalization and indexing mechanism of Self-Retrieval establishes a new paradigm to memorize, organize and retrieve the learned documents (at least part of them) during the pre-training phase, paving the way for more transparent and trustworthy text generation from LLMs. Third, as a LLM-driven retrieval system, Self-Retrieval offers inherent advantages in terms of compatibility, consistency, and interaction with LLMs' internal knowledge. Through experiments on RAG, we demonstrate how this natural compatibility leads to superior performance, suggesting broader potential for enhancing various LLM-based applications.\n\n## 2 Related Work",
      "In this section, we introduce our proposed Self-Retrieval. The overall architecture is illustrated in . Different from traditional information retrieval systems that separate indexing, retrieval, and reranking components, Self-Retrieval integrates these functionalities directly into the parameters of a single large language model:\n\n- Indexing : Self-Retrieval internalizes the entire corpus into its parameters through selfsupervised learning, enabling the model to process passages internally without relying on external indices.\n- Retrieval : Given an input query q , Self-Retrieval generates relevant passage p using the knowledge embedded within its parameters, which is different from dense retrieval or generative retrieval that rely on embedding or document identifiers as proxies of passage.\n\n- Reranking : After generating passage p , Self-Retrieval assesses its relevance to the query q through self-assessment. The output logits provide the basis for reranking candidate passages.\n\nThrough this unified approach, Self-Retrieval enables a streamlined, end-to-end process that enhances the overall effectiveness of information retrieval. In the following sections, we detail each component of our method.\n\n## 3.1 Indexing: Internalize the Corpus",
      "While our experiments demonstrate the effectiveness of Self-Retrieval, several limitations need to be addressed in future work. Our current evaluation is limited to 200K Wikipedia documents and 3M passages, and testing on larger and noisier text collections is needed. As an LLM-driven system, Self-Retrieval has lower retrieval efficiency compared to sparse or dense retrieval methods, which may limit its applications to specialized knowledge systems. Furthermore, enabling incremental learning and dynamic corpus expansion remains an important direction for future research.\n\n## Acknowledge\n\nWe sincerely thank the reviewers for their insightful comments and valuable suggestions. We are grateful to Le Yu and Xinyu Lu for their helpful feedback on the paper writing. This work was supported by the Natural Science Foundation of China (No. 62122077, 62272439), Beijing Municipal Science and Technology Project (Nos. Z231100010323002), the Basic Research Program of ISCAS (ISCAS-JCZD-202303), and CAS Project for Young Scientists in Basic Research (Grant No.YSBR-040).\n\n## References"
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence explains Self-Retrieval's technical aspects but does not explicitly state it is introduced clearly or made accessible to readers.",
      "confidence": 0.85
    }
  },
  {
    "claim_id": 5,
    "claim": "The paper presents good experimental results that demonstrate significant improvements over existing retrieval methods.",
    "evidence": [
      "Datasets and metrics We conduct main experiments on Natural Questions (NQ) [21] and TriviaQA [18] datasets, both of which are widely used retrieval benchmarks based on Wikipedia. We use their versions from the KILT benchmark [34], which consolidates these datasets into a single pre-processed Wikipedia dump, facilitating easier evaluation. Since the KILT test set is not publicly accessible, we use the development set for testing and randomly sample 2,000 instances from the training set as our development set. For our experiments, we sample approximately 40K documents\n\nTable 1: The experimental results of passage retrieval on NQ and TriviaQA test set. * indicates statistically significant improvements (p &lt; 0.01) over state-of-the-art retrieval baselines.",
      "from Wikipedia for each dataset. Each document is segmented into passages of maximum 200 words, yielding approximately 1 million passages in total. The detailed statistics of the datasets are presented in Appendix A. We use passage-level Hits@{1, 5} and Mean Reciprocal Rank (MRR)@5 as evaluation metrics.\n\nTo comprehensively compare with other generative information retrieval methods, we also conduct experiments on document retrieval. Following NCI [49], we conduct experiments on NQ320K and utilize Recall@{1, 10} and MRR@100 as the evaluation metrics. To evaluate the model's robustness in non-Wikipedia scenarios where high-quality text and titles are not available, we conduct experiments on a subset of MS MARCO [3] following the experimental setup of Ultron [55]. The performance was measured using Recall@{1,5} and MRR@10.\n\nImplementation details In this study, we employ StableLM-3B [44] and Llama2-7B [43] as passage retrieval backbones. For document retrieval, we employ StableLM-1.6B [4] for NQ320K and StableLM-3B for MS MARCO. We train the models using ZeRO stage-2 optimization on 8 NVIDIA A100 (80 GB) GPUs with the AdamW optimizer, a batch size of 16 per GPU, and BFloat16 precision. The models are trained for 3 epochs with a learning rate of 2e-5. During inference, we use beam search to generate 5 titles and 10 passages for each title, with hyperparameters τ and δ set to 0.4 across all models and datasets.",
      "Analysis on reranking In this part, we conduct an in-depth analysis of the reranking performance of Self-Retrieval reranker module in comparison with the fine-tuned BGE-Reranker. We employ DPR-FT, SEAL and GritLM to retrieve 100 passages on TriviaQA, followed by reranking the retrieved results using both approaches. We evaluate performance using MRR@5 as the metric. The experimental results are presented in . The results reveal two key findings: (1) Reranking plays a crucial role in information retrieval systems, significantly enhancing the ranking performance across all models. (2) The Self-Retrieval reranker consistently outperforms the fine-tuned BGE Reranker in most scenarios, demonstrating its robustness and effectiveness. These findings demonstrate that Self-Retrieval performs effectively both as a complete IR system and as a reranker component.\n\nIn Appendix C, we conduct additional experiments with a chunk size of 100 words, demonstrating Self-Retrieval's adaptability to different text segmentation strategies. In Appendix E, we further discuss Self-Retrieval's computational efficiency.\n\n## 5 Conclusion\n\nIn this paper, we propose Self-Retrieval, an end-to-end LLM-driven information retrieval architecture that unifies indexing, retrieval, and reranking in a single LLM. This approach enables the LLM to internalize the corpus, generate relevant content, and perform self-assessment within a unified framework. Unlike previous works that incorporate LLMs into individual IR components, SelfRetrieval provides a unified framework for the entire IR procedure, facilitating knowledge sharing and deep collaboration among different components. Experimental results demonstrate that SelfRetrieval achieves strong performance across various retrieval benchmarks and application scenarios. In future work, we plan to extend our method to further enhance the reliability and trustworthiness of LLM generation.\n\n## Limitations",
      "While our experiments demonstrate the effectiveness of Self-Retrieval, several limitations need to be addressed in future work. Our current evaluation is limited to 200K Wikipedia documents and 3M passages, and testing on larger and noisier text collections is needed. As an LLM-driven system, Self-Retrieval has lower retrieval efficiency compared to sparse or dense retrieval methods, which may limit its applications to specialized knowledge systems. Furthermore, enabling incremental learning and dynamic corpus expansion remains an important direction for future research.\n\n## Acknowledge\n\nWe sincerely thank the reviewers for their insightful comments and valuable suggestions. We are grateful to Le Yu and Xinyu Lu for their helpful feedback on the paper writing. This work was supported by the Natural Science Foundation of China (No. 62122077, 62272439), Beijing Municipal Science and Technology Project (Nos. Z231100010323002), the Basic Research Program of ISCAS (ISCAS-JCZD-202303), and CAS Project for Young Scientists in Basic Research (Grant No.YSBR-040).\n\n## References"
    ],
    "verification": {
      "result": "Supported",
      "justification": "The evidence shows significant improvements in experimental results over existing retrieval methods.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 6,
    "claim": "Weaknesses: 1.",
    "evidence": [
      "Recently, information retrieval (IR) systems and large language models (LLMs) have witnessed a growing synergy, with advancements in one field driving progress in the other [13, 56]. On one hand, IR systems have proven effective in augmenting LLMs and mitigating challenges such as hallucinations and outdated knowledge [22, 16]. By providing accurate, up-to-date external knowledge, IR systems significantly enhance the reliability and performance of LLMs. On the other hand, the powerful language understanding and generation capabilities of LLMs have been leveraged to enhance almost all components of traditional IR systems-indexing, retrieval [42, 9, 26], and reranking [58, 27, 40]. Through the integration of LLMs into the IR pipeline, these systems achieve substantially improved retrieval accuracy [57, 1].\n\nHowever, current IR systems typically adopt a pipeline architecture where different components operate in isolation, limiting LLMs' role to specific components rather than leveraging their full\n\n∗ Equally Contribution.\n\n† Corresponding authors.\n\n3 The code of this work is available at https://github.com/icip-cas/SelfRetrieval .\n\n: The Self-Retrieval framework consists of three key components: (1) corpus indexing through self-supervised learning, (2) passage generation via constrained decoding, (3) passage ranking using self-assessment scoring.\n\n<!-- image -->\n\npotential across the entire system. This fragmented approach creates several challenges: it hinders knowledge sharing between components, prevents deep integration of LLMs' diverse capabilities, and results in complex implementations with potentially sub-optimal performance. These limitations underscore the need for a more unified approach that fully integrates LLMs across all components of the IR system. Such an approach would not only maximize the utility of LLMs' capabilities but also simplify system implementation while potentially achieving better performance through enhanced component synergy.",
      "|          | NQ    | NQ    | TriviaQA   | TriviaQA   |\n|----------|-------|-------|----------|----------|\n|          | 10K   | 40K   | 10K        | 40K        |\n| BGE-FT + StableLM-FT | 43.18 | 41.24 | 56.79      | 58.15      |\n| Self-Retrieval 3B    | 44.62 | 46.11 | 64.03      | 62.69      |\n| BGE-FT + Llama2-FT   | 49.10 | 49.24 | 61.79      | 61.72      |\n| Self-Retrieval 7B    | 53.26 | 52.98 | 72.14      | 70.40      |\n\nThe end-to-end architecture of Self-Retrieval seamlessly integrates retrieval and answer generation into a single inference process. To evaluate its effectiveness in RAG, we compare Self-Retrieval models with a strong baseline that combines BGE-FT for retrieval and fine-tuned versions of StableLM3B and LLaMA2-7B as readers. We conduct experiments on subsets of NQ and TriviaQA using 10K and 40K documents for each dataset. We utilize the top-1 retrieved passage as the context and measure performance using the Exact Match (EM) metric. As shown in Table 5, Self-Retrieval significantly outperforms the baseline on both datasets across different model scales. Unlike other RAG pipelines that separate retrieval and generation, Self-Retrieval integrates the entire process within the LLM framework, enabling more accurate and coherent responses through end-to-end modeling.\n\n## 4.4 Detailed Analysis\n\nScaling model capacity To explore the impact of model scale on retrieval performance, we evaluate Self-Retrieval with various backbone models of different sizes, including StableLM (1.6B, 3B) [4, 44], Llama2 (7B, 13B) [43], and Qwen-1.5 (4B, 7B, 14B) [2].  presents the results on NQ, showing that Self-Retrieval's retrieval performance benefits from the general capabilities of larger language models. For models within the same series, as the model size increases, we observe consistent improvements in both Hits@1 and Hits@5, indicating strong scaling properties of the Self-Retrieval architecture.",
      "Scaling corpus size Recent studies [35, 53] have demonstrated that generative retrieval methods such as DSI or NCI experience more significant performance degradation compared to dense retrieval methods when scaled to larger corpora. To explore the impact of corpus size on Self-Retrieval,\n\n: Impact of model capacity on SelfRetrieval performance.\n\n<!-- image -->\n\n<!-- image -->\n\n: Reranking performance comparison when processing top-100 passages.\n\n<!-- image -->\n\n: Scalability analysis of retrieval performance for Self-Retrieval and BGE-FT across varying corpus sizes.\n\nwe expand our experiments from 10K to 200K documents, scaling the number of passages from 290K to 3M.  illustrates the performance trends of BGE-FT and our Self-Retrieval 3B model on the NQ and TriviaQA datasets with increasing corpus sizes. While both models show performance decrease with larger corpus sizes, Self-Retrieval maintains a degradation rate comparable to BGE-FT. As the number of documents continues to increase, the degradation rate gradually diminishes, demonstrating Self-Retrieval's potential scalability to larger document collections. This observation indicates that Self-Retrieval effectively addresses some of the inherent limitations of generative retrieval approaches in large-scale scenarios.",
      "|          |        | NQ      | NQ    | NQ      | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|--------|---------|-------|---------|----------|----------|----------|\n| Model          | Params | H@1     | H@5   | M@5     | H@1        | H@5        | M@5        |\n| Sparse Retrieval BM25 [37]       | -      | 14.54   | 32.71 | 21.13   | 20.09      | 42.73      | 28.35      |\n| Dense Retrieval DPR [19]         | 110M   | 40.41   | 61.79 | 48.80   | 35.57      | 57.39      | 43.93      |\n| DPR-FT [19]          | 110M   | 42.21   | 60.45 | 49.33   | 36.58      | 53.05      | 42.91      |\n| BGE [50]          | 335M   | 36.30   | 66.95 | 48.05   | 46.97      | 70.14      | 55.95      |\n| BGE-FT [50]          | 335M   | 53.42   | 80.15 | 63.99   | 52.70      | 75.22      | 61.65      |\n| BGE-FT + BGE-Reranker-FT         | 770M   | 52.15   | 76.15 | 61.37   | 44.87      | 67.39      | 53.39      |\n| GTR-XL [32]          | 1.24B  | 37.64   | 66.84 | 48.94   | 35.97      | 63.75      | 46.67      |\n| GTR-XL + BGE-Reranker-FT         | 1.57B  | 57.50   | 78.92 | 66.06   | 58.56      | 77.65      | 66.22      |\n| GTR-XXL [32]          | 4.86B  | 39.21   | 69.72 | 50.88   | 35.97      | 64.15      | 46.83      |\n| text-embedding-ada-002          | -      | 34.28   | 62.28 | 44.64   | 35.09      | 62.00      | 45.15      |\n| GritLM [29]          | 7.24B  | 44.67   | 76.00 | 57.03   | 39.91      | 69.34      | 51.14      |\n| GritLM + BGE-Reranker-FT         | 7.57B  | 57.57   | 81.35 | 66.98   | 58.60      | 80.54      | 67.21      |\n| Generative retrieval DSI-XL [42] | 2.85B  | 43.03   | 60.26 | 49.47   | 29.64      | 46.74      | 36.12      |\n| DSI-XXL [42]          | 11.3B  | 43.81   | 60.45 | 50.20   | 30.55      | 46.67      | 36.56      |\n| SEAL [5]          | 406M   | 36.79   | 61.35 | 45.88   | 36.88      | 61.66      | 46.29      |\n| DSI-QG [59]          | 2.85B  | 34.88   | 56.60 | 43.33   | 29.15      | 45.53      | 35.20      |\n| NCI + BGE-Reranker-FT          | 1.07B  | 50.86   | 70.27 | 58.53   | 28.42      | 42.18      | 33.62      |\n| Self-Retrieval (StableLM)        | 2.8B   | 62.16 ∗ | 79.28 | 69.45 ∗ | 58.69 ∗    | 78.39 ∗    | 66.72 ∗    |\n| Self-Retrieval (Llama 2)         | 6.74B  | 63.44 ∗ | 79.29 | 70.00 ∗ | 59.94 ∗    | 81.06 ∗    | 68.74 ∗    |"
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence discusses weaknesses of IR systems but does not directly address the specific claim about 'Weaknesses: 1.'",
      "confidence": 0.75
    }
  },
  {
    "claim_id": 7,
    "claim": "- It is better to include more experimental results on the full KILT datasets as many existing studies for a fair comparison.",
    "evidence": [
      "Datasets and metrics We conduct main experiments on Natural Questions (NQ) [21] and TriviaQA [18] datasets, both of which are widely used retrieval benchmarks based on Wikipedia. We use their versions from the KILT benchmark [34], which consolidates these datasets into a single pre-processed Wikipedia dump, facilitating easier evaluation. Since the KILT test set is not publicly accessible, we use the development set for testing and randomly sample 2,000 instances from the training set as our development set. For our experiments, we sample approximately 40K documents\n\nTable 1: The experimental results of passage retrieval on NQ and TriviaQA test set. * indicates statistically significant improvements (p &lt; 0.01) over state-of-the-art retrieval baselines.",
      "## A Dataset Statistics\n\nTable 6 presents the statistics of the NQ and TriviaQA datasets used in our experiments.\n\n| Dataset   | Natural Questions   | Natural Questions   | TriviaQA   | TriviaQA   |\n|----------|----------|----------|----------|----------|\n|          | 10K          | 40K          | 10K        | 40K        |\n| # doc     | 10,000          | 37,202          | 10,000     | 38,399     |\n| # psg     | 291,506          | 979,804          | 390,586    | 1,193,047  |\n| # train   | 32,163          | 72.716          | 29,038     | 51,166     |\n| # dev     | 2,000          | 2,000          | 2,000      | 2,000      |\n| # test    | 2,837          | 2,837          | 5,355      | 5,355      |\n\nTable 6: Statistics of the experimental datasets. #doc/#psg denotes number of documents/passages; #train/#dev/#test denotes size of training/development/test set. Training instances without querydocument pairs are removed.\n\n## B Baselines\n\nThe sparse retrieval baselines are as follows:\n\n- BM25 [37] is a classical sparse retrieval algorithm based on probabilistic relevance framework and term frequency statistics.\n- DocT5Query [28] expands documents by generating potential queries using a fine-tuned T5 model.\n\nThe dense retrieval baselines are as follows:",
      "|          |        | NQ      | NQ    | NQ      | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|--------|---------|-------|---------|----------|----------|----------|\n| Model          | Params | H@1     | H@5   | M@5     | H@1        | H@5        | M@5        |\n| Sparse Retrieval BM25 [37]       | -      | 14.54   | 32.71 | 21.13   | 20.09      | 42.73      | 28.35      |\n| Dense Retrieval DPR [19]         | 110M   | 40.41   | 61.79 | 48.80   | 35.57      | 57.39      | 43.93      |\n| DPR-FT [19]          | 110M   | 42.21   | 60.45 | 49.33   | 36.58      | 53.05      | 42.91      |\n| BGE [50]          | 335M   | 36.30   | 66.95 | 48.05   | 46.97      | 70.14      | 55.95      |\n| BGE-FT [50]          | 335M   | 53.42   | 80.15 | 63.99   | 52.70      | 75.22      | 61.65      |\n| BGE-FT + BGE-Reranker-FT         | 770M   | 52.15   | 76.15 | 61.37   | 44.87      | 67.39      | 53.39      |\n| GTR-XL [32]          | 1.24B  | 37.64   | 66.84 | 48.94   | 35.97      | 63.75      | 46.67      |\n| GTR-XL + BGE-Reranker-FT         | 1.57B  | 57.50   | 78.92 | 66.06   | 58.56      | 77.65      | 66.22      |\n| GTR-XXL [32]          | 4.86B  | 39.21   | 69.72 | 50.88   | 35.97      | 64.15      | 46.83      |\n| text-embedding-ada-002          | -      | 34.28   | 62.28 | 44.64   | 35.09      | 62.00      | 45.15      |\n| GritLM [29]          | 7.24B  | 44.67   | 76.00 | 57.03   | 39.91      | 69.34      | 51.14      |\n| GritLM + BGE-Reranker-FT         | 7.57B  | 57.57   | 81.35 | 66.98   | 58.60      | 80.54      | 67.21      |\n| Generative retrieval DSI-XL [42] | 2.85B  | 43.03   | 60.26 | 49.47   | 29.64      | 46.74      | 36.12      |\n| DSI-XXL [42]          | 11.3B  | 43.81   | 60.45 | 50.20   | 30.55      | 46.67      | 36.56      |\n| SEAL [5]          | 406M   | 36.79   | 61.35 | 45.88   | 36.88      | 61.66      | 46.29      |\n| DSI-QG [59]          | 2.85B  | 34.88   | 56.60 | 43.33   | 29.15      | 45.53      | 35.20      |\n| NCI + BGE-Reranker-FT          | 1.07B  | 50.86   | 70.27 | 58.53   | 28.42      | 42.18      | 33.62      |\n| Self-Retrieval (StableLM)        | 2.8B   | 62.16 ∗ | 79.28 | 69.45 ∗ | 58.69 ∗    | 78.39 ∗    | 66.72 ∗    |\n| Self-Retrieval (Llama 2)         | 6.74B  | 63.44 ∗ | 79.29 | 70.00 ∗ | 59.94 ∗    | 81.06 ∗    | 68.74 ∗    |",
      "Table 4: Ablation study on NQ and TriviaQA.\n\n| Method          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| Self-Retrieval (base) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| w/o indexing          | 53.05 | 67.16 | 58.95 | 54.45      | 70.64      | 60.98      |\n| w/o title          | 47.81 | 60.90 | 52.81 | 52.32      | 67.91      | 58.48      |\n| w/o self-assessment   | 54.80 | 75.21 | 62.77 | 46.67      | 70.79      | 55.92      |\n\nperformance, with each ablation resulting in substantial performance degradation. Specifically, removing the indexing mechanism restricts the model to internalizing only the documents encountered during training, leading to poor performance on unseen passages. Without titles, we directly generate passages with constrained decoding. The absence of document titles significantly degrades performance, as titles provide critical global information that guides the LLM in generating relevant content. Furthermore, removing the self-assessment mechanism leads to a significant decrease in both datasets. Without self-assessment, the model cannot effectively evaluate and refine its initial retrieved passages, leading to less accurate document rankings. This degradation directly impacts downstream applications such as RAG, where precise passage ranking is crucial for generating highquality responses. These ablation results show that each component of Self-Retrieval addresses a specific challenge in the retrieval process, contributing to its overall effectiveness.\n\n## 4.3 Performance on Retrieval-Augmented Generation\n\nTable 5: The performance on retrieval-augmented generation. For baseline, we use BGE-FT as the retriever and a fine-tuned LLM as reader. Results are reported using Exact Match (EM) scores."
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The claim suggests more experimental results on full KILT datasets are needed, but the evidence only provides results on NQ and TriviaQA subsets.",
      "confidence": 0.85
    }
  },
  {
    "claim_id": 8,
    "claim": "Ensuring consistent use of key terms throughout the paper would improve its readability and professionalism.",
    "evidence": [
      "While our experiments demonstrate the effectiveness of Self-Retrieval, several limitations need to be addressed in future work. Our current evaluation is limited to 200K Wikipedia documents and 3M passages, and testing on larger and noisier text collections is needed. As an LLM-driven system, Self-Retrieval has lower retrieval efficiency compared to sparse or dense retrieval methods, which may limit its applications to specialized knowledge systems. Furthermore, enabling incremental learning and dynamic corpus expansion remains an important direction for future research.\n\n## Acknowledge\n\nWe sincerely thank the reviewers for their insightful comments and valuable suggestions. We are grateful to Le Yu and Xinyu Lu for their helpful feedback on the paper writing. This work was supported by the Natural Science Foundation of China (No. 62122077, 62272439), Beijing Municipal Science and Technology Project (Nos. Z231100010323002), the Basic Research Program of ISCAS (ISCAS-JCZD-202303), and CAS Project for Young Scientists in Basic Research (Grant No.YSBR-040).\n\n## References",
      "| [19]   | Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6769- 6781, Online, November 2020. Association for Computational Linguistics. |\n|--------|----------|\n| [20]   | Varsha Kishore, Chao gang Wan, Justin Lovelace, Yoav Artzi, and Kilian Q. Weinberger. In- cdsi: Incrementally updatable document retrieval. ArXiv , abs/2307.10323, 2023. |\n| [21]   | Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transac- tions of the Association of Computational Linguistics , 2019. |\n| [22]   | Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neu- ral Information Processing Systems , volume 33, pages 9459-9474. Curran Associates, Inc., 2020. |\n| [23]   | Xiaoxi Li, Yujia Zhou, and Zhicheng Dou. Unigen: A unified generative framework for re- trieval and question answering with large language models. ArXiv , abs/2312.11036, 2023. |\n| [24]   | Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong Tang, Annan Li, Le Sun, Meng Liao, and Shaoyi Chen. Text2Event: Controllable sequence-to-structure generation for end-to-end event extraction.",
      "During training, we utilize the gold passage from the supervision data as the positive instance, while sampling negative instances from both the same and different documents. This training strategy conditions the LLM to accurately discern and verify the relevance of its outputs, thereby enhancing its autonomous relevance assessment capabilities and improving the overall precision of the retrieval process.\n\nDuring inference, the overall relevance score S is composed of the document title score S T and the self-assessment score S P . Specifically, the document title score is derived from the title generation probability, while the self-assessment score is calculated based on the probability of the language model rejecting the passage. Formally, for a set of generated titles and passages { ( t 1 , p 1 ) , ( t 2 , p 2 ) , . . . , ( t n , p n ) } , the title score for each ( t i , p i ) is given by:\n\n$$\\mathcal { S } _ { i } ^ { T } = \\text {Softmax} ( P ( t _ { i } | q ; \\theta ) / \\tau ) \\\\$$\n\nand the assessment score is:\n\n$$\\mathcal { S } _ { i } ^ { P } = \\text {softmax} ( ( 1 - P ( \\text {reaction response} | q , t _ { i } , p _ { i } ; \\theta ) ) / \\delta ) \\\\ \\\\ + \\text {a} \\, .$$\n\nwhere τ and δ are temperature parameters used to scale the logits. Based on preliminary experiments on the development set, we simply set τ = δ = 0 . 4 for the main passage retrieval experiments.\n\nThe final relevance score is computed as the product of these two components:\n\n$$\\mathcal { S } = \\mathcal { S } ^ { T } \\cdot \\mathcal { S } ^ { P }$$\n\nThis combined score is then used to rerank the passage set, producing a more refined ordering based on relevance.\n\n## 3.4 Training &amp; Inference",
      "In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 2795-2806, Online, August 2021. Association for Computational Linguistics. |\n| [25]   | Guangyuan Ma, Xing Wu, Peng Wang, Zijia Lin, and Songlin Hu. Pre-training with large lan- guage model-based document expansion for dense passage retrieval. ArXiv , abs/2308.08285, 2023. |\n| [26]   | Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. ArXiv , abs/2310.08319, 2023. |\n| [27]   | Xueguang Ma, Xinyu Crystina Zhang, Ronak Pradeep, and Jimmy J. Lin. Zero-shot listwise document reranking with a large language model. ArXiv , abs/2305.02156, 2023. |\n| [28]   | Antonio Mallia, O. Khattab, Nicola Tonellotto, and Torsten Suel. Learning passage impacts for inverted indexes. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval , 2021. |\n| [29]   | Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning, 2024. |\n| [30]   | Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005 , 2022. |\n| [31]   | Jianmo Ni, Gustavo Hernández Abrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Matthew Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. ArXiv , abs/2108.08877, 2021. |\n| [32]   | Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers."
    ],
    "verification": {
      "result": "Undetermined",
      "justification": "The evidence does not address the claim about key term consistency affecting readability and professionalism.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 9,
    "claim": "Questions: Please see my concerns in weaknesses.",
    "evidence": [
      "|          | NQ    | NQ    | TriviaQA   | TriviaQA   |\n|----------|-------|-------|----------|----------|\n|          | 10K   | 40K   | 10K        | 40K        |\n| BGE-FT + StableLM-FT | 43.18 | 41.24 | 56.79      | 58.15      |\n| Self-Retrieval 3B    | 44.62 | 46.11 | 64.03      | 62.69      |\n| BGE-FT + Llama2-FT   | 49.10 | 49.24 | 61.79      | 61.72      |\n| Self-Retrieval 7B    | 53.26 | 52.98 | 72.14      | 70.40      |\n\nThe end-to-end architecture of Self-Retrieval seamlessly integrates retrieval and answer generation into a single inference process. To evaluate its effectiveness in RAG, we compare Self-Retrieval models with a strong baseline that combines BGE-FT for retrieval and fine-tuned versions of StableLM3B and LLaMA2-7B as readers. We conduct experiments on subsets of NQ and TriviaQA using 10K and 40K documents for each dataset. We utilize the top-1 retrieved passage as the context and measure performance using the Exact Match (EM) metric. As shown in Table 5, Self-Retrieval significantly outperforms the baseline on both datasets across different model scales. Unlike other RAG pipelines that separate retrieval and generation, Self-Retrieval integrates the entire process within the LLM framework, enabling more accurate and coherent responses through end-to-end modeling.\n\n## 4.4 Detailed Analysis\n\nScaling model capacity To explore the impact of model scale on retrieval performance, we evaluate Self-Retrieval with various backbone models of different sizes, including StableLM (1.6B, 3B) [4, 44], Llama2 (7B, 13B) [43], and Qwen-1.5 (4B, 7B, 14B) [2].  presents the results on NQ, showing that Self-Retrieval's retrieval performance benefits from the general capabilities of larger language models. For models within the same series, as the model size increases, we observe consistent improvements in both Hits@1 and Hits@5, indicating strong scaling properties of the Self-Retrieval architecture.",
      "Table 4: Ablation study on NQ and TriviaQA.\n\n| Method          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| Self-Retrieval (base) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| w/o indexing          | 53.05 | 67.16 | 58.95 | 54.45      | 70.64      | 60.98      |\n| w/o title          | 47.81 | 60.90 | 52.81 | 52.32      | 67.91      | 58.48      |\n| w/o self-assessment   | 54.80 | 75.21 | 62.77 | 46.67      | 70.79      | 55.92      |\n\nperformance, with each ablation resulting in substantial performance degradation. Specifically, removing the indexing mechanism restricts the model to internalizing only the documents encountered during training, leading to poor performance on unseen passages. Without titles, we directly generate passages with constrained decoding. The absence of document titles significantly degrades performance, as titles provide critical global information that guides the LLM in generating relevant content. Furthermore, removing the self-assessment mechanism leads to a significant decrease in both datasets. Without self-assessment, the model cannot effectively evaluate and refine its initial retrieved passages, leading to less accurate document rankings. This degradation directly impacts downstream applications such as RAG, where precise passage ranking is crucial for generating highquality responses. These ablation results show that each component of Self-Retrieval addresses a specific challenge in the retrieval process, contributing to its overall effectiveness.\n\n## 4.3 Performance on Retrieval-Augmented Generation\n\nTable 5: The performance on retrieval-augmented generation. For baseline, we use BGE-FT as the retriever and a fine-tuned LLM as reader. Results are reported using Exact Match (EM) scores.",
      "|          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| BGE-FT          | 53.42 | 80.15 | 63.99 | 52.70      | 75.22      | 61.65      |\n| BGE-FT + BGE-Reranker     | 21.91 | 54.58 | 33.33 | 45.36      | 72.16      | 55.78      |\n| BGE-FT + BGE-Reranker-FT  | 52.15 | 76.15 | 61.37 | 44.87      | 67.39      | 53.39      |\n| BGE-FT + RankGPT          | 44.21 | 73.68 | 55.51 | 48.00      | 72.00      | 57.33      |\n| GTR-XL          | 37.64 | 66.84 | 48.94 | 35.97      | 63.75      | 46.67      |\n| GTR-XL + BGE-Reranker     | 26.39 | 59.96 | 38.50 | 42.41      | 68.42      | 52.51      |\n| GTR-XL + BGE-Reranker-FT  | 57.50 | 78.92 | 66.06 | 58.56      | 77.65      | 66.22      |\n| GTR-XL + RankGPT          | 42.11 | 68.42 | 52.30 | 47.00      | 66.00      | 54.95      |\n| GritLM          | 44.67 | 76.00 | 57.03 | 39.91      | 69.34      | 51.14      |\n| GritLM + BGE-Reranker     | 30.06 | 65.87 | 43.20 | 43.64      | 70.87      | 54.23      |\n| GritLM + BGE-Reranker-FT  | 57.57 | 81.35 | 66.98 | 58.60      | 80.54      | 67.21      |\n| GritLM + RankGPT          | 37.89 | 70.53 | 51.19 | 44.00      | 66.00      | 52.70      |\n| DSI-XL          | 43.03 | 60.26 | 49.47 | 29.64      | 46.74      | 36.12      |\n| DSI-XL + BGE-Reranker     | 34.39 | 64.26 | 45.74 | 37.85      | 52.57      | 43.49      |\n| DSI-XL + BGE-Reranker-FT  | 50.02 | 68.60 | 57.43 | 36.49      | 52.40      | 42.36      |\n| DSI-XL + RankGPT          | 49.47 | 73.68 | 59.25 | 39.00      | 52.00      | 44.75      |\n| Self-Retrieval (StableLM) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| Self-Retrieval (Llama 2)  | 63.44 | 79.29 | 70.00 | 59.94      | 81.06      | 68.74      |",
      "## A Dataset Statistics\n\nTable 6 presents the statistics of the NQ and TriviaQA datasets used in our experiments.\n\n| Dataset   | Natural Questions   | Natural Questions   | TriviaQA   | TriviaQA   |\n|----------|----------|----------|----------|----------|\n|          | 10K          | 40K          | 10K        | 40K        |\n| # doc     | 10,000          | 37,202          | 10,000     | 38,399     |\n| # psg     | 291,506          | 979,804          | 390,586    | 1,193,047  |\n| # train   | 32,163          | 72.716          | 29,038     | 51,166     |\n| # dev     | 2,000          | 2,000          | 2,000      | 2,000      |\n| # test    | 2,837          | 2,837          | 5,355      | 5,355      |\n\nTable 6: Statistics of the experimental datasets. #doc/#psg denotes number of documents/passages; #train/#dev/#test denotes size of training/development/test set. Training instances without querydocument pairs are removed.\n\n## B Baselines\n\nThe sparse retrieval baselines are as follows:\n\n- BM25 [37] is a classical sparse retrieval algorithm based on probabilistic relevance framework and term frequency statistics.\n- DocT5Query [28] expands documents by generating potential queries using a fine-tuned T5 model.\n\nThe dense retrieval baselines are as follows:"
    ],
    "verification": {
      "result": "Undetermined",
      "justification": "The claim references 'weaknesses' but no specific weaknesses are discussed in the evidence.",
      "confidence": 0.8
    }
  },
  {
    "claim_id": 10,
    "claim": "Limitations: n/a\n\nSoundness: 3\n\nPresentation: 3\n\nContribution: 3\n\nEthics Review Flagged: ['No ethics review needed.']",
    "evidence": [
      "Table 4: Ablation study on NQ and TriviaQA.\n\n| Method          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| Self-Retrieval (base) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| w/o indexing          | 53.05 | 67.16 | 58.95 | 54.45      | 70.64      | 60.98      |\n| w/o title          | 47.81 | 60.90 | 52.81 | 52.32      | 67.91      | 58.48      |\n| w/o self-assessment   | 54.80 | 75.21 | 62.77 | 46.67      | 70.79      | 55.92      |\n\nperformance, with each ablation resulting in substantial performance degradation. Specifically, removing the indexing mechanism restricts the model to internalizing only the documents encountered during training, leading to poor performance on unseen passages. Without titles, we directly generate passages with constrained decoding. The absence of document titles significantly degrades performance, as titles provide critical global information that guides the LLM in generating relevant content. Furthermore, removing the self-assessment mechanism leads to a significant decrease in both datasets. Without self-assessment, the model cannot effectively evaluate and refine its initial retrieved passages, leading to less accurate document rankings. This degradation directly impacts downstream applications such as RAG, where precise passage ranking is crucial for generating highquality responses. These ablation results show that each component of Self-Retrieval addresses a specific challenge in the retrieval process, contributing to its overall effectiveness.\n\n## 4.3 Performance on Retrieval-Augmented Generation\n\nTable 5: The performance on retrieval-augmented generation. For baseline, we use BGE-FT as the retriever and a fine-tuned LLM as reader. Results are reported using Exact Match (EM) scores.",
      "from Wikipedia for each dataset. Each document is segmented into passages of maximum 200 words, yielding approximately 1 million passages in total. The detailed statistics of the datasets are presented in Appendix A. We use passage-level Hits@{1, 5} and Mean Reciprocal Rank (MRR)@5 as evaluation metrics.\n\nTo comprehensively compare with other generative information retrieval methods, we also conduct experiments on document retrieval. Following NCI [49], we conduct experiments on NQ320K and utilize Recall@{1, 10} and MRR@100 as the evaluation metrics. To evaluate the model's robustness in non-Wikipedia scenarios where high-quality text and titles are not available, we conduct experiments on a subset of MS MARCO [3] following the experimental setup of Ultron [55]. The performance was measured using Recall@{1,5} and MRR@10.\n\nImplementation details In this study, we employ StableLM-3B [44] and Llama2-7B [43] as passage retrieval backbones. For document retrieval, we employ StableLM-1.6B [4] for NQ320K and StableLM-3B for MS MARCO. We train the models using ZeRO stage-2 optimization on 8 NVIDIA A100 (80 GB) GPUs with the AdamW optimizer, a batch size of 16 per GPU, and BFloat16 precision. The models are trained for 3 epochs with a learning rate of 2e-5. During inference, we use beam search to generate 5 titles and 10 passages for each title, with hyperparameters τ and δ set to 0.4 across all models and datasets.",
      "Scaling corpus size Recent studies [35, 53] have demonstrated that generative retrieval methods such as DSI or NCI experience more significant performance degradation compared to dense retrieval methods when scaled to larger corpora. To explore the impact of corpus size on Self-Retrieval,\n\n: Impact of model capacity on SelfRetrieval performance.\n\n<!-- image -->\n\n<!-- image -->\n\n: Reranking performance comparison when processing top-100 passages.\n\n<!-- image -->\n\n: Scalability analysis of retrieval performance for Self-Retrieval and BGE-FT across varying corpus sizes.\n\nwe expand our experiments from 10K to 200K documents, scaling the number of passages from 290K to 3M.  illustrates the performance trends of BGE-FT and our Self-Retrieval 3B model on the NQ and TriviaQA datasets with increasing corpus sizes. While both models show performance decrease with larger corpus sizes, Self-Retrieval maintains a degradation rate comparable to BGE-FT. As the number of documents continues to increase, the degradation rate gradually diminishes, demonstrating Self-Retrieval's potential scalability to larger document collections. This observation indicates that Self-Retrieval effectively addresses some of the inherent limitations of generative retrieval approaches in large-scale scenarios.",
      "- [1] Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, Shengling Gao, J. Guo, Xiangnan He, Yanyan Lan, Chenliang Li, Yiqun Liu, Ziyu Lyu, Weizhi Ma, Jun Ma, Zhaochun Ren, Pengjie Ren, Zhiqiang Wang, Min Wang, Jirong Wen, Lei Wu, Xin Xin, Jun Xu, Dawei Yin, Peng Zhang, Fan Zhang, Wei na Zhang, M. Zhang, and Xiaofei Zhu. Information retrieval meets large language models: A strategic report from chinese ir community. ArXiv , abs/2307.09751, 2023.\n- [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023.\n\n[3] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 , 2016.\n\n[4] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, Meng Lee,"
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence shows performance degradation without certain components, but does not directly confirm the claim's specific metrics or context.",
      "confidence": 0.75
    }
  },
  {
    "claim_id": 11,
    "claim": "Summary: This paper proposes Self-Retrieval, an LM that retrieve, rerank passages, and generate answers using a single model.",
    "evidence": [
      "Training Self-Retrieval unifies the three distinct tasks of information retrieval - indexing, retrieval, and reranking - into text generation tasks, trained using cross-entropy loss in an auto-regressive manner. Specifically, Self-Retrieval first internalizes the corpus into its parameters through selfsupervised learning as introduced in Section 3.1. Subsequently, in addition to a portion of selfsupervised instances, it incorporates two different types of data to build retrieval and reranking abilities:\n\n- Retrieval data: Utilizes supervised query-passage pairs from the dataset, where the model learns to generate both document titles and passage content in response to input queries.\n- Reranking data: Employs positive and negative examples to train the model in relevance assessment between queries and passages.\n\nThis auto-regressive training approach enables Self-Retrieval to integrate traditionally separate IR components into a unified language model, establishing an end-to-end IR system.\n\nFurthermore, leveraging the universal language generation capabilities of LLMs, we can seamlessly integrate downstream task components, such as readers in RAG, into Self-Retrieval. This integration can be achieved by simply appending the golden answer after the assessment in Self-Retrieval. Consequently, the LLM can function as a comprehensive RAG system, effectively reducing the knowledge gap between IR system and reader modules.\n\nInference During inference, given an input query, Self-Retrieval aims to obtain the relevant passages that are sorted based on the relevance to query. Firstly, the model generates i document titles through constrained beam search. Secondly, for each title, it generates j passages using beam search. Finally, the resulting i × j passages are scored using the self-assessment mechanism and reranked to produce the final output.\n\n## 4 Experimental Results\n\n## 4.1 Experimental Setup",
      "In this section, we introduce our proposed Self-Retrieval. The overall architecture is illustrated in . Different from traditional information retrieval systems that separate indexing, retrieval, and reranking components, Self-Retrieval integrates these functionalities directly into the parameters of a single large language model:\n\n- Indexing : Self-Retrieval internalizes the entire corpus into its parameters through selfsupervised learning, enabling the model to process passages internally without relying on external indices.\n- Retrieval : Given an input query q , Self-Retrieval generates relevant passage p using the knowledge embedded within its parameters, which is different from dense retrieval or generative retrieval that rely on embedding or document identifiers as proxies of passage.\n\n- Reranking : After generating passage p , Self-Retrieval assesses its relevance to the query q through self-assessment. The output logits provide the basis for reranking candidate passages.\n\nThrough this unified approach, Self-Retrieval enables a streamlined, end-to-end process that enhances the overall effectiveness of information retrieval. In the following sections, we detail each component of our method.\n\n## 3.1 Indexing: Internalize the Corpus",
      "Analysis on reranking In this part, we conduct an in-depth analysis of the reranking performance of Self-Retrieval reranker module in comparison with the fine-tuned BGE-Reranker. We employ DPR-FT, SEAL and GritLM to retrieve 100 passages on TriviaQA, followed by reranking the retrieved results using both approaches. We evaluate performance using MRR@5 as the metric. The experimental results are presented in . The results reveal two key findings: (1) Reranking plays a crucial role in information retrieval systems, significantly enhancing the ranking performance across all models. (2) The Self-Retrieval reranker consistently outperforms the fine-tuned BGE Reranker in most scenarios, demonstrating its robustness and effectiveness. These findings demonstrate that Self-Retrieval performs effectively both as a complete IR system and as a reranker component.\n\nIn Appendix C, we conduct additional experiments with a chunk size of 100 words, demonstrating Self-Retrieval's adaptability to different text segmentation strategies. In Appendix E, we further discuss Self-Retrieval's computational efficiency.\n\n## 5 Conclusion\n\nIn this paper, we propose Self-Retrieval, an end-to-end LLM-driven information retrieval architecture that unifies indexing, retrieval, and reranking in a single LLM. This approach enables the LLM to internalize the corpus, generate relevant content, and perform self-assessment within a unified framework. Unlike previous works that incorporate LLMs into individual IR components, SelfRetrieval provides a unified framework for the entire IR procedure, facilitating knowledge sharing and deep collaboration among different components. Experimental results demonstrate that SelfRetrieval achieves strong performance across various retrieval benchmarks and application scenarios. In future work, we plan to extend our method to further enhance the reliability and trustworthiness of LLM generation.\n\n## Limitations",
      "We evaluate Self-Retrieval on three representative retrieval benchmarks: NQ, TriviaQA, and MS MARCO. Experimental results demonstrate that Self-Retrieval substantially outperforms existing sparse retrieval, dense retrieval, and generative retrieval methods on both document-level and passage-level retrieval tasks. Furthermore, our experiments on retrieval-augmented generation tasks reveal that Self-Retrieval considerably enhances downstream performance. Additionally, larger LLMs lead to progressively better performance in Self-Retrieval, showing clear scaling benefits. These results demonstrate the effectiveness of Self-Retrieval across different retrieval tasks and application scenarios.\n\nThe potential impacts of this paper may include the following aspects. First, we introduce SelfRetrieval, an end-to-end architecture that consolidates the entire information retrieval system within a single large language model. This unified approach demonstrates substantial performance improvements over existing IR methods. Second, the corpus internalization and indexing mechanism of Self-Retrieval establishes a new paradigm to memorize, organize and retrieve the learned documents (at least part of them) during the pre-training phase, paving the way for more transparent and trustworthy text generation from LLMs. Third, as a LLM-driven retrieval system, Self-Retrieval offers inherent advantages in terms of compatibility, consistency, and interaction with LLMs' internal knowledge. Through experiments on RAG, we demonstrate how this natural compatibility leads to superior performance, suggesting broader potential for enhancing various LLM-based applications.\n\n## 2 Related Work"
    ],
    "verification": {
      "result": "Supported",
      "justification": "The evidence directly supports the claim about Self-Retrieval unifying retrieval tasks into a single model.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 12,
    "claim": "Evaluation shows that Self-Retrieval outperforms previous dense and generative retrievers in retrieval tasks and achieves better EM scores in answer generation tasks.",
    "evidence": [
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "Training Self-Retrieval unifies the three distinct tasks of information retrieval - indexing, retrieval, and reranking - into text generation tasks, trained using cross-entropy loss in an auto-regressive manner. Specifically, Self-Retrieval first internalizes the corpus into its parameters through selfsupervised learning as introduced in Section 3.1. Subsequently, in addition to a portion of selfsupervised instances, it incorporates two different types of data to build retrieval and reranking abilities:\n\n- Retrieval data: Utilizes supervised query-passage pairs from the dataset, where the model learns to generate both document titles and passage content in response to input queries.\n- Reranking data: Employs positive and negative examples to train the model in relevance assessment between queries and passages.\n\nThis auto-regressive training approach enables Self-Retrieval to integrate traditionally separate IR components into a unified language model, establishing an end-to-end IR system.\n\nFurthermore, leveraging the universal language generation capabilities of LLMs, we can seamlessly integrate downstream task components, such as readers in RAG, into Self-Retrieval. This integration can be achieved by simply appending the golden answer after the assessment in Self-Retrieval. Consequently, the LLM can function as a comprehensive RAG system, effectively reducing the knowledge gap between IR system and reader modules.\n\nInference During inference, given an input query, Self-Retrieval aims to obtain the relevant passages that are sorted based on the relevance to query. Firstly, the model generates i document titles through constrained beam search. Secondly, for each title, it generates j passages using beam search. Finally, the resulting i × j passages are scored using the self-assessment mechanism and reranked to produce the final output.\n\n## 4 Experimental Results\n\n## 4.1 Experimental Setup",
      "Generative retrieval Generative retrieval methods leverage sequence-to-sequence language models to generate document identifiers for a given query [8, 42]. This paradigm is pioneered by GENRE [7], which introduces the concept of entity retrieval through constrained beam search generation of entity names. DSI [42] extends it to document retrieval by training T5 models to generate document-specific identifiers. The field has since evolved through various innovations, including query generation techniques [11, 59], sophisticated identifier design [48, 51], architectural improvements [5, 36], and continual learning strategies [20, 14].\n\nMost relevant to our work, Yu et al.[52] proposed a \"generate-then-read\" approach, advocating for the use of LLMs to directly generate documents instead of relying on a retriever. UniGen [23] proposed a unified framework that integrates generative retrieval and question answering through a dual-decoder architecture. Compared to them, Self-Retrieval ensures accurate document generation through constrained decoding and accomplishes both retrieval and answer generation in one turn.\n\nThe main distinctions between Self-Retrieval and existing generative retrieval methods can be summarized as follows: (1) Self-Retrieval enables LLMs to directly generate document content rather than relying on other text or numeric identifiers. This approach aligns naturally with LLMs' pretraining objectives, preserves their inherent knowledge, and eliminates the need for complex identifier construction schemes. (2) Self-Retrieval further integrates components such as reranking and answer generation into the framework, further expanding its scope and enhancing the retrieval performance. These distinctions highlight that Self-Retrieval represents a more natural and effective approach for leveraging the capabilities of LLMs in information retrieval.\n\n## 3 Self-Retrieval",
      "|          | NQ    | NQ    | TriviaQA   | TriviaQA   |\n|----------|-------|-------|----------|----------|\n|          | 10K   | 40K   | 10K        | 40K        |\n| BGE-FT + StableLM-FT | 43.18 | 41.24 | 56.79      | 58.15      |\n| Self-Retrieval 3B    | 44.62 | 46.11 | 64.03      | 62.69      |\n| BGE-FT + Llama2-FT   | 49.10 | 49.24 | 61.79      | 61.72      |\n| Self-Retrieval 7B    | 53.26 | 52.98 | 72.14      | 70.40      |\n\nThe end-to-end architecture of Self-Retrieval seamlessly integrates retrieval and answer generation into a single inference process. To evaluate its effectiveness in RAG, we compare Self-Retrieval models with a strong baseline that combines BGE-FT for retrieval and fine-tuned versions of StableLM3B and LLaMA2-7B as readers. We conduct experiments on subsets of NQ and TriviaQA using 10K and 40K documents for each dataset. We utilize the top-1 retrieved passage as the context and measure performance using the Exact Match (EM) metric. As shown in Table 5, Self-Retrieval significantly outperforms the baseline on both datasets across different model scales. Unlike other RAG pipelines that separate retrieval and generation, Self-Retrieval integrates the entire process within the LLM framework, enabling more accurate and coherent responses through end-to-end modeling.\n\n## 4.4 Detailed Analysis\n\nScaling model capacity To explore the impact of model scale on retrieval performance, we evaluate Self-Retrieval with various backbone models of different sizes, including StableLM (1.6B, 3B) [4, 44], Llama2 (7B, 13B) [43], and Qwen-1.5 (4B, 7B, 14B) [2].  presents the results on NQ, showing that Self-Retrieval's retrieval performance benefits from the general capabilities of larger language models. For models within the same series, as the model size increases, we observe consistent improvements in both Hits@1 and Hits@5, indicating strong scaling properties of the Self-Retrieval architecture."
    ],
    "verification": {
      "result": "Supported",
      "justification": "Evidence shows Self-Retrieval outperforms generative retrievers in MRR@5 and EM scores.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 13,
    "claim": "The concept is novel.",
    "evidence": [
      "While our experiments demonstrate the effectiveness of Self-Retrieval, several limitations need to be addressed in future work. Our current evaluation is limited to 200K Wikipedia documents and 3M passages, and testing on larger and noisier text collections is needed. As an LLM-driven system, Self-Retrieval has lower retrieval efficiency compared to sparse or dense retrieval methods, which may limit its applications to specialized knowledge systems. Furthermore, enabling incremental learning and dynamic corpus expansion remains an important direction for future research.\n\n## Acknowledge\n\nWe sincerely thank the reviewers for their insightful comments and valuable suggestions. We are grateful to Le Yu and Xinyu Lu for their helpful feedback on the paper writing. This work was supported by the Natural Science Foundation of China (No. 62122077, 62272439), Beijing Municipal Science and Technology Project (Nos. Z231100010323002), the Basic Research Program of ISCAS (ISCAS-JCZD-202303), and CAS Project for Young Scientists in Basic Research (Grant No.YSBR-040).\n\n## References",
      "| [33]   | Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. From doc2query to doctttttquery. Online preprint , 6:2, 2019. |\n|--------|----------|\n| [34]   | Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2523-2544, Online, June 2021. Association for Computational Linguistics. |\n| [35]   | Ronak Pradeep, Kai Hui, Jai Gupta, Adam Lelkes, Honglei Zhuang, Jimmy Lin, Donald Met- zler, and Vinh Tran. How does generative retrieval scale to millions of passages? In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empir- ical Methods in Natural Language Processing , pages 1305-1321, Singapore, December 2023. Association for Computational Linguistics. |\n| [36]   | Shanbao Qiao, Xuebing Liu, and Seung-Hoon Na. Diffusionret: Diffusion-enhanced gener- ative retriever using constrained decoding. In Conference on Empirical Methods in Natural Language Processing , 2023. |\n| [37]   | Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval , 3(4):333-389, 2009. |\n| [38]   | Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. ColBERTv2: Effective and efficient retrieval via lightweight late interaction. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 3715-3734, Seattle, United States, July 2022. Association for Computational Linguistics.",
      "Table 4: Ablation study on NQ and TriviaQA.\n\n| Method          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| Self-Retrieval (base) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| w/o indexing          | 53.05 | 67.16 | 58.95 | 54.45      | 70.64      | 60.98      |\n| w/o title          | 47.81 | 60.90 | 52.81 | 52.32      | 67.91      | 58.48      |\n| w/o self-assessment   | 54.80 | 75.21 | 62.77 | 46.67      | 70.79      | 55.92      |\n\nperformance, with each ablation resulting in substantial performance degradation. Specifically, removing the indexing mechanism restricts the model to internalizing only the documents encountered during training, leading to poor performance on unseen passages. Without titles, we directly generate passages with constrained decoding. The absence of document titles significantly degrades performance, as titles provide critical global information that guides the LLM in generating relevant content. Furthermore, removing the self-assessment mechanism leads to a significant decrease in both datasets. Without self-assessment, the model cannot effectively evaluate and refine its initial retrieved passages, leading to less accurate document rankings. This degradation directly impacts downstream applications such as RAG, where precise passage ranking is crucial for generating highquality responses. These ablation results show that each component of Self-Retrieval addresses a specific challenge in the retrieval process, contributing to its overall effectiveness.\n\n## 4.3 Performance on Retrieval-Augmented Generation\n\nTable 5: The performance on retrieval-augmented generation. For baseline, we use BGE-FT as the retriever and a fine-tuned LLM as reader. Results are reported using Exact Match (EM) scores.",
      "|      | Emad Mostaque, Michael Pieler, Nikhil Pinnaparju, Paulo Rocha, Harry Saini, Hannah Teufel, Niccolo Zanichelli, and Carlos Riquelme. Stable lm 2 1.6b technical report, 2024. |\n|------|----------|\n| [5]  | Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. Autoregressive search engines: Generating substrings as document identifiers. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 31668-31683. Curran Associates, Inc., 2022. |\n| [6]  | Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. In- pars: Data augmentation for information retrieval using large language models. ArXiv , abs/2202.05144, 2022. |\n| [7]  | Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity retrieval. ArXiv , abs/2010.00904, 2020. |\n| [8]  | Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity retrieval. In International Conference on Learning Representations , 2021. |\n| [9]  | Jiangui Chen, Ruqing Zhang, J. Guo, Y. Liu, Yixing Fan, and Xueqi Cheng. Corpusbrain: Pre- train a generative retrieval model for knowledge-intensive language tasks. Proceedings of the 31st ACM International Conference on Information &Knowledge Management , 2022. |\n| [10] | Pinzhen Chen, Nikolay Bogoychev, Kenneth Heafield, and Faheem Kirefu. Parallel sentence mining by constrained decoding. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computa- tional Linguistics , pages 1672-1678, Online, July 2020. Association for Computational Lin- guistics. |\n| [11] | David R. Cheriton. From doc2query to doctttttquery. 2019. |\n| [12] | Fernando Ferraretto, Thiago Laitz, Roberto de Alencar Lotufo, and Rodrigo Nogueira. Exaranker: Synthetic explanations improve neural rankers. Proceedings of the 46th Interna- tional ACM SIGIR Conference on Research and Development in Information Retrieval , 2023."
    ],
    "verification": {
      "result": "Undetermined",
      "justification": "The evidence does not directly address the novelty of the concept.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 14,
    "claim": "Self-Retrieval achieves the best performance for both passage retrieval and answer generation tasks.",
    "evidence": [
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "Training Self-Retrieval unifies the three distinct tasks of information retrieval - indexing, retrieval, and reranking - into text generation tasks, trained using cross-entropy loss in an auto-regressive manner. Specifically, Self-Retrieval first internalizes the corpus into its parameters through selfsupervised learning as introduced in Section 3.1. Subsequently, in addition to a portion of selfsupervised instances, it incorporates two different types of data to build retrieval and reranking abilities:\n\n- Retrieval data: Utilizes supervised query-passage pairs from the dataset, where the model learns to generate both document titles and passage content in response to input queries.\n- Reranking data: Employs positive and negative examples to train the model in relevance assessment between queries and passages.\n\nThis auto-regressive training approach enables Self-Retrieval to integrate traditionally separate IR components into a unified language model, establishing an end-to-end IR system.\n\nFurthermore, leveraging the universal language generation capabilities of LLMs, we can seamlessly integrate downstream task components, such as readers in RAG, into Self-Retrieval. This integration can be achieved by simply appending the golden answer after the assessment in Self-Retrieval. Consequently, the LLM can function as a comprehensive RAG system, effectively reducing the knowledge gap between IR system and reader modules.\n\nInference During inference, given an input query, Self-Retrieval aims to obtain the relevant passages that are sorted based on the relevance to query. Firstly, the model generates i document titles through constrained beam search. Secondly, for each title, it generates j passages using beam search. Finally, the resulting i × j passages are scored using the self-assessment mechanism and reranked to produce the final output.\n\n## 4 Experimental Results\n\n## 4.1 Experimental Setup",
      "We evaluate Self-Retrieval on three representative retrieval benchmarks: NQ, TriviaQA, and MS MARCO. Experimental results demonstrate that Self-Retrieval substantially outperforms existing sparse retrieval, dense retrieval, and generative retrieval methods on both document-level and passage-level retrieval tasks. Furthermore, our experiments on retrieval-augmented generation tasks reveal that Self-Retrieval considerably enhances downstream performance. Additionally, larger LLMs lead to progressively better performance in Self-Retrieval, showing clear scaling benefits. These results demonstrate the effectiveness of Self-Retrieval across different retrieval tasks and application scenarios.\n\nThe potential impacts of this paper may include the following aspects. First, we introduce SelfRetrieval, an end-to-end architecture that consolidates the entire information retrieval system within a single large language model. This unified approach demonstrates substantial performance improvements over existing IR methods. Second, the corpus internalization and indexing mechanism of Self-Retrieval establishes a new paradigm to memorize, organize and retrieve the learned documents (at least part of them) during the pre-training phase, paving the way for more transparent and trustworthy text generation from LLMs. Third, as a LLM-driven retrieval system, Self-Retrieval offers inherent advantages in terms of compatibility, consistency, and interaction with LLMs' internal knowledge. Through experiments on RAG, we demonstrate how this natural compatibility leads to superior performance, suggesting broader potential for enhancing various LLM-based applications.\n\n## 2 Related Work",
      "|          | NQ    | NQ    | TriviaQA   | TriviaQA   |\n|----------|-------|-------|----------|----------|\n|          | 10K   | 40K   | 10K        | 40K        |\n| BGE-FT + StableLM-FT | 43.18 | 41.24 | 56.79      | 58.15      |\n| Self-Retrieval 3B    | 44.62 | 46.11 | 64.03      | 62.69      |\n| BGE-FT + Llama2-FT   | 49.10 | 49.24 | 61.79      | 61.72      |\n| Self-Retrieval 7B    | 53.26 | 52.98 | 72.14      | 70.40      |\n\nThe end-to-end architecture of Self-Retrieval seamlessly integrates retrieval and answer generation into a single inference process. To evaluate its effectiveness in RAG, we compare Self-Retrieval models with a strong baseline that combines BGE-FT for retrieval and fine-tuned versions of StableLM3B and LLaMA2-7B as readers. We conduct experiments on subsets of NQ and TriviaQA using 10K and 40K documents for each dataset. We utilize the top-1 retrieved passage as the context and measure performance using the Exact Match (EM) metric. As shown in Table 5, Self-Retrieval significantly outperforms the baseline on both datasets across different model scales. Unlike other RAG pipelines that separate retrieval and generation, Self-Retrieval integrates the entire process within the LLM framework, enabling more accurate and coherent responses through end-to-end modeling.\n\n## 4.4 Detailed Analysis\n\nScaling model capacity To explore the impact of model scale on retrieval performance, we evaluate Self-Retrieval with various backbone models of different sizes, including StableLM (1.6B, 3B) [4, 44], Llama2 (7B, 13B) [43], and Qwen-1.5 (4B, 7B, 14B) [2].  presents the results on NQ, showing that Self-Retrieval's retrieval performance benefits from the general capabilities of larger language models. For models within the same series, as the model size increases, we observe consistent improvements in both Hits@1 and Hits@5, indicating strong scaling properties of the Self-Retrieval architecture."
    ],
    "verification": {
      "result": "Supported",
      "justification": "Evidence shows Self-Retrieval outperforms other generative methods in passage retrieval and answer generation.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 15,
    "claim": "Self-Retrieval shows promising results when the corpus size scales to 3 million.",
    "evidence": [
      "Scaling corpus size Recent studies [35, 53] have demonstrated that generative retrieval methods such as DSI or NCI experience more significant performance degradation compared to dense retrieval methods when scaled to larger corpora. To explore the impact of corpus size on Self-Retrieval,\n\n: Impact of model capacity on SelfRetrieval performance.\n\n<!-- image -->\n\n<!-- image -->\n\n: Reranking performance comparison when processing top-100 passages.\n\n<!-- image -->\n\n: Scalability analysis of retrieval performance for Self-Retrieval and BGE-FT across varying corpus sizes.\n\nwe expand our experiments from 10K to 200K documents, scaling the number of passages from 290K to 3M.  illustrates the performance trends of BGE-FT and our Self-Retrieval 3B model on the NQ and TriviaQA datasets with increasing corpus sizes. While both models show performance decrease with larger corpus sizes, Self-Retrieval maintains a degradation rate comparable to BGE-FT. As the number of documents continues to increase, the degradation rate gradually diminishes, demonstrating Self-Retrieval's potential scalability to larger document collections. This observation indicates that Self-Retrieval effectively addresses some of the inherent limitations of generative retrieval approaches in large-scale scenarios.",
      "While our experiments demonstrate the effectiveness of Self-Retrieval, several limitations need to be addressed in future work. Our current evaluation is limited to 200K Wikipedia documents and 3M passages, and testing on larger and noisier text collections is needed. As an LLM-driven system, Self-Retrieval has lower retrieval efficiency compared to sparse or dense retrieval methods, which may limit its applications to specialized knowledge systems. Furthermore, enabling incremental learning and dynamic corpus expansion remains an important direction for future research.\n\n## Acknowledge\n\nWe sincerely thank the reviewers for their insightful comments and valuable suggestions. We are grateful to Le Yu and Xinyu Lu for their helpful feedback on the paper writing. This work was supported by the Natural Science Foundation of China (No. 62122077, 62272439), Beijing Municipal Science and Technology Project (Nos. Z231100010323002), the Basic Research Program of ISCAS (ISCAS-JCZD-202303), and CAS Project for Young Scientists in Basic Research (Grant No.YSBR-040).\n\n## References",
      "We evaluate Self-Retrieval on three representative retrieval benchmarks: NQ, TriviaQA, and MS MARCO. Experimental results demonstrate that Self-Retrieval substantially outperforms existing sparse retrieval, dense retrieval, and generative retrieval methods on both document-level and passage-level retrieval tasks. Furthermore, our experiments on retrieval-augmented generation tasks reveal that Self-Retrieval considerably enhances downstream performance. Additionally, larger LLMs lead to progressively better performance in Self-Retrieval, showing clear scaling benefits. These results demonstrate the effectiveness of Self-Retrieval across different retrieval tasks and application scenarios.\n\nThe potential impacts of this paper may include the following aspects. First, we introduce SelfRetrieval, an end-to-end architecture that consolidates the entire information retrieval system within a single large language model. This unified approach demonstrates substantial performance improvements over existing IR methods. Second, the corpus internalization and indexing mechanism of Self-Retrieval establishes a new paradigm to memorize, organize and retrieve the learned documents (at least part of them) during the pre-training phase, paving the way for more transparent and trustworthy text generation from LLMs. Third, as a LLM-driven retrieval system, Self-Retrieval offers inherent advantages in terms of compatibility, consistency, and interaction with LLMs' internal knowledge. Through experiments on RAG, we demonstrate how this natural compatibility leads to superior performance, suggesting broader potential for enhancing various LLM-based applications.\n\n## 2 Related Work",
      "In this section, we introduce our proposed Self-Retrieval. The overall architecture is illustrated in . Different from traditional information retrieval systems that separate indexing, retrieval, and reranking components, Self-Retrieval integrates these functionalities directly into the parameters of a single large language model:\n\n- Indexing : Self-Retrieval internalizes the entire corpus into its parameters through selfsupervised learning, enabling the model to process passages internally without relying on external indices.\n- Retrieval : Given an input query q , Self-Retrieval generates relevant passage p using the knowledge embedded within its parameters, which is different from dense retrieval or generative retrieval that rely on embedding or document identifiers as proxies of passage.\n\n- Reranking : After generating passage p , Self-Retrieval assesses its relevance to the query q through self-assessment. The output logits provide the basis for reranking candidate passages.\n\nThrough this unified approach, Self-Retrieval enables a streamlined, end-to-end process that enhances the overall effectiveness of information retrieval. In the following sections, we detail each component of our method.\n\n## 3.1 Indexing: Internalize the Corpus"
    ],
    "verification": {
      "result": "Supported",
      "justification": "Evidence shows Self-Retrieval maintains performance degradation comparable to BGE-FT when scaling to 3 million passages.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 16,
    "claim": "Weaknesses: 1.",
    "evidence": [
      "Recently, information retrieval (IR) systems and large language models (LLMs) have witnessed a growing synergy, with advancements in one field driving progress in the other [13, 56]. On one hand, IR systems have proven effective in augmenting LLMs and mitigating challenges such as hallucinations and outdated knowledge [22, 16]. By providing accurate, up-to-date external knowledge, IR systems significantly enhance the reliability and performance of LLMs. On the other hand, the powerful language understanding and generation capabilities of LLMs have been leveraged to enhance almost all components of traditional IR systems-indexing, retrieval [42, 9, 26], and reranking [58, 27, 40]. Through the integration of LLMs into the IR pipeline, these systems achieve substantially improved retrieval accuracy [57, 1].\n\nHowever, current IR systems typically adopt a pipeline architecture where different components operate in isolation, limiting LLMs' role to specific components rather than leveraging their full\n\n∗ Equally Contribution.\n\n† Corresponding authors.\n\n3 The code of this work is available at https://github.com/icip-cas/SelfRetrieval .\n\n: The Self-Retrieval framework consists of three key components: (1) corpus indexing through self-supervised learning, (2) passage generation via constrained decoding, (3) passage ranking using self-assessment scoring.\n\n<!-- image -->\n\npotential across the entire system. This fragmented approach creates several challenges: it hinders knowledge sharing between components, prevents deep integration of LLMs' diverse capabilities, and results in complex implementations with potentially sub-optimal performance. These limitations underscore the need for a more unified approach that fully integrates LLMs across all components of the IR system. Such an approach would not only maximize the utility of LLMs' capabilities but also simplify system implementation while potentially achieving better performance through enhanced component synergy.",
      "|          | NQ    | NQ    | TriviaQA   | TriviaQA   |\n|----------|-------|-------|----------|----------|\n|          | 10K   | 40K   | 10K        | 40K        |\n| BGE-FT + StableLM-FT | 43.18 | 41.24 | 56.79      | 58.15      |\n| Self-Retrieval 3B    | 44.62 | 46.11 | 64.03      | 62.69      |\n| BGE-FT + Llama2-FT   | 49.10 | 49.24 | 61.79      | 61.72      |\n| Self-Retrieval 7B    | 53.26 | 52.98 | 72.14      | 70.40      |\n\nThe end-to-end architecture of Self-Retrieval seamlessly integrates retrieval and answer generation into a single inference process. To evaluate its effectiveness in RAG, we compare Self-Retrieval models with a strong baseline that combines BGE-FT for retrieval and fine-tuned versions of StableLM3B and LLaMA2-7B as readers. We conduct experiments on subsets of NQ and TriviaQA using 10K and 40K documents for each dataset. We utilize the top-1 retrieved passage as the context and measure performance using the Exact Match (EM) metric. As shown in Table 5, Self-Retrieval significantly outperforms the baseline on both datasets across different model scales. Unlike other RAG pipelines that separate retrieval and generation, Self-Retrieval integrates the entire process within the LLM framework, enabling more accurate and coherent responses through end-to-end modeling.\n\n## 4.4 Detailed Analysis\n\nScaling model capacity To explore the impact of model scale on retrieval performance, we evaluate Self-Retrieval with various backbone models of different sizes, including StableLM (1.6B, 3B) [4, 44], Llama2 (7B, 13B) [43], and Qwen-1.5 (4B, 7B, 14B) [2].  presents the results on NQ, showing that Self-Retrieval's retrieval performance benefits from the general capabilities of larger language models. For models within the same series, as the model size increases, we observe consistent improvements in both Hits@1 and Hits@5, indicating strong scaling properties of the Self-Retrieval architecture.",
      "Scaling corpus size Recent studies [35, 53] have demonstrated that generative retrieval methods such as DSI or NCI experience more significant performance degradation compared to dense retrieval methods when scaled to larger corpora. To explore the impact of corpus size on Self-Retrieval,\n\n: Impact of model capacity on SelfRetrieval performance.\n\n<!-- image -->\n\n<!-- image -->\n\n: Reranking performance comparison when processing top-100 passages.\n\n<!-- image -->\n\n: Scalability analysis of retrieval performance for Self-Retrieval and BGE-FT across varying corpus sizes.\n\nwe expand our experiments from 10K to 200K documents, scaling the number of passages from 290K to 3M.  illustrates the performance trends of BGE-FT and our Self-Retrieval 3B model on the NQ and TriviaQA datasets with increasing corpus sizes. While both models show performance decrease with larger corpus sizes, Self-Retrieval maintains a degradation rate comparable to BGE-FT. As the number of documents continues to increase, the degradation rate gradually diminishes, demonstrating Self-Retrieval's potential scalability to larger document collections. This observation indicates that Self-Retrieval effectively addresses some of the inherent limitations of generative retrieval approaches in large-scale scenarios.",
      "|          |        | NQ      | NQ    | NQ      | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|--------|---------|-------|---------|----------|----------|----------|\n| Model          | Params | H@1     | H@5   | M@5     | H@1        | H@5        | M@5        |\n| Sparse Retrieval BM25 [37]       | -      | 14.54   | 32.71 | 21.13   | 20.09      | 42.73      | 28.35      |\n| Dense Retrieval DPR [19]         | 110M   | 40.41   | 61.79 | 48.80   | 35.57      | 57.39      | 43.93      |\n| DPR-FT [19]          | 110M   | 42.21   | 60.45 | 49.33   | 36.58      | 53.05      | 42.91      |\n| BGE [50]          | 335M   | 36.30   | 66.95 | 48.05   | 46.97      | 70.14      | 55.95      |\n| BGE-FT [50]          | 335M   | 53.42   | 80.15 | 63.99   | 52.70      | 75.22      | 61.65      |\n| BGE-FT + BGE-Reranker-FT         | 770M   | 52.15   | 76.15 | 61.37   | 44.87      | 67.39      | 53.39      |\n| GTR-XL [32]          | 1.24B  | 37.64   | 66.84 | 48.94   | 35.97      | 63.75      | 46.67      |\n| GTR-XL + BGE-Reranker-FT         | 1.57B  | 57.50   | 78.92 | 66.06   | 58.56      | 77.65      | 66.22      |\n| GTR-XXL [32]          | 4.86B  | 39.21   | 69.72 | 50.88   | 35.97      | 64.15      | 46.83      |\n| text-embedding-ada-002          | -      | 34.28   | 62.28 | 44.64   | 35.09      | 62.00      | 45.15      |\n| GritLM [29]          | 7.24B  | 44.67   | 76.00 | 57.03   | 39.91      | 69.34      | 51.14      |\n| GritLM + BGE-Reranker-FT         | 7.57B  | 57.57   | 81.35 | 66.98   | 58.60      | 80.54      | 67.21      |\n| Generative retrieval DSI-XL [42] | 2.85B  | 43.03   | 60.26 | 49.47   | 29.64      | 46.74      | 36.12      |\n| DSI-XXL [42]          | 11.3B  | 43.81   | 60.45 | 50.20   | 30.55      | 46.67      | 36.56      |\n| SEAL [5]          | 406M   | 36.79   | 61.35 | 45.88   | 36.88      | 61.66      | 46.29      |\n| DSI-QG [59]          | 2.85B  | 34.88   | 56.60 | 43.33   | 29.15      | 45.53      | 35.20      |\n| NCI + BGE-Reranker-FT          | 1.07B  | 50.86   | 70.27 | 58.53   | 28.42      | 42.18      | 33.62      |\n| Self-Retrieval (StableLM)        | 2.8B   | 62.16 ∗ | 79.28 | 69.45 ∗ | 58.69 ∗    | 78.39 ∗    | 66.72 ∗    |\n| Self-Retrieval (Llama 2)         | 6.74B  | 63.44 ∗ | 79.29 | 70.00 ∗ | 59.94 ∗    | 81.06 ∗    | 68.74 ∗    |"
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence discusses weaknesses of IR systems but does not directly address the specific claim about 'Weaknesses: 1.'",
      "confidence": 0.75
    }
  },
  {
    "claim_id": 17,
    "claim": "The proposed model includes an in-domain fine-tuned reranker, while the baseline BGE-FT + reader does not have a reranking stage.",
    "evidence": [
      "Analysis on reranking In this part, we conduct an in-depth analysis of the reranking performance of Self-Retrieval reranker module in comparison with the fine-tuned BGE-Reranker. We employ DPR-FT, SEAL and GritLM to retrieve 100 passages on TriviaQA, followed by reranking the retrieved results using both approaches. We evaluate performance using MRR@5 as the metric. The experimental results are presented in . The results reveal two key findings: (1) Reranking plays a crucial role in information retrieval systems, significantly enhancing the ranking performance across all models. (2) The Self-Retrieval reranker consistently outperforms the fine-tuned BGE Reranker in most scenarios, demonstrating its robustness and effectiveness. These findings demonstrate that Self-Retrieval performs effectively both as a complete IR system and as a reranker component.\n\nIn Appendix C, we conduct additional experiments with a chunk size of 100 words, demonstrating Self-Retrieval's adaptability to different text segmentation strategies. In Appendix E, we further discuss Self-Retrieval's computational efficiency.\n\n## 5 Conclusion\n\nIn this paper, we propose Self-Retrieval, an end-to-end LLM-driven information retrieval architecture that unifies indexing, retrieval, and reranking in a single LLM. This approach enables the LLM to internalize the corpus, generate relevant content, and perform self-assessment within a unified framework. Unlike previous works that incorporate LLMs into individual IR components, SelfRetrieval provides a unified framework for the entire IR procedure, facilitating knowledge sharing and deep collaboration among different components. Experimental results demonstrate that SelfRetrieval achieves strong performance across various retrieval benchmarks and application scenarios. In future work, we plan to extend our method to further enhance the reliability and trustworthiness of LLM generation.\n\n## Limitations",
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 2795-2806, Online, August 2021. Association for Computational Linguistics. |\n| [25]   | Guangyuan Ma, Xing Wu, Peng Wang, Zijia Lin, and Songlin Hu. Pre-training with large lan- guage model-based document expansion for dense passage retrieval. ArXiv , abs/2308.08285, 2023. |\n| [26]   | Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. ArXiv , abs/2310.08319, 2023. |\n| [27]   | Xueguang Ma, Xinyu Crystina Zhang, Ronak Pradeep, and Jimmy J. Lin. Zero-shot listwise document reranking with a large language model. ArXiv , abs/2305.02156, 2023. |\n| [28]   | Antonio Mallia, O. Khattab, Nicola Tonellotto, and Torsten Suel. Learning passage impacts for inverted indexes. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval , 2021. |\n| [29]   | Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning, 2024. |\n| [30]   | Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005 , 2022. |\n| [31]   | Jianmo Ni, Gustavo Hernández Abrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Matthew Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. ArXiv , abs/2108.08877, 2021. |\n| [32]   | Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers.",
      "Baselines We evaluate Self-Retrieval models for both passage retrieval and document retrieval, comparing them with sparse, dense, and generative retrieval baselines. The sparse retrieval baselines are:BM25 [37] and DocT5Query [28]. The dense retrieval baselines include: DPR [19], Sentence-T5 [31], GTR [32], BGE [50], text-embedding-ada-002 [30], GritLM [29], and their finetuned variants, DPR-FT and BGE-FT. The generative retrieval baselines comprise: DSI [42], DSIQG [59], NCI [49], Ultron [55], DynamicRetriever [54], GenRet [39], and SEAL [5]. Additionally, to ensure a comprehensive comparison, we also evaluate combinations of strong retrieval baselines with various rerankers, including BGE-Reranker, BGE-Reranker-FT, and RankGPT [41]. In the passage retrieval task, we use the official pre-trained models for all non-fine-tuned dense retrieval baselines. For fine-tuned dense models and generative models, we use their official implementations to replicate the experiments on our dataset. In the document retrieval task, we report the baseline\n\nperformances from their original paper. For comprehensive details about these baselines, please refer to Appendix B.\n\n## 4.2 Main Results\n\nPassage retrieval In Table 1, we compare the performance of Self-Retrieval with various baselines on the NQ and TriviaQA datasets. Self-Retrieval 3B outperforms both strong pre-trained dense retrieval models, such as BGE and GritLM 7B, and other generative retrieval methods. Specifically, Self-Retrieval 3B achieves improvements of 5.46 and 5.07 in MRR@5 over the fine-tuned BGE on NQ and TriviaQA datasets, respectively."
    ],
    "verification": {
      "result": "Supported",
      "justification": "The evidence confirms Self-Retrieval includes a reranker while BGE-FT + reader lacks it.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 18,
    "claim": "This may make the comparison unfair since reranking can significantly improve RAG results.",
    "evidence": [
      "|          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| BGE-FT          | 53.42 | 80.15 | 63.99 | 52.70      | 75.22      | 61.65      |\n| BGE-FT + BGE-Reranker     | 21.91 | 54.58 | 33.33 | 45.36      | 72.16      | 55.78      |\n| BGE-FT + BGE-Reranker-FT  | 52.15 | 76.15 | 61.37 | 44.87      | 67.39      | 53.39      |\n| BGE-FT + RankGPT          | 44.21 | 73.68 | 55.51 | 48.00      | 72.00      | 57.33      |\n| GTR-XL          | 37.64 | 66.84 | 48.94 | 35.97      | 63.75      | 46.67      |\n| GTR-XL + BGE-Reranker     | 26.39 | 59.96 | 38.50 | 42.41      | 68.42      | 52.51      |\n| GTR-XL + BGE-Reranker-FT  | 57.50 | 78.92 | 66.06 | 58.56      | 77.65      | 66.22      |\n| GTR-XL + RankGPT          | 42.11 | 68.42 | 52.30 | 47.00      | 66.00      | 54.95      |\n| GritLM          | 44.67 | 76.00 | 57.03 | 39.91      | 69.34      | 51.14      |\n| GritLM + BGE-Reranker     | 30.06 | 65.87 | 43.20 | 43.64      | 70.87      | 54.23      |\n| GritLM + BGE-Reranker-FT  | 57.57 | 81.35 | 66.98 | 58.60      | 80.54      | 67.21      |\n| GritLM + RankGPT          | 37.89 | 70.53 | 51.19 | 44.00      | 66.00      | 52.70      |\n| DSI-XL          | 43.03 | 60.26 | 49.47 | 29.64      | 46.74      | 36.12      |\n| DSI-XL + BGE-Reranker     | 34.39 | 64.26 | 45.74 | 37.85      | 52.57      | 43.49      |\n| DSI-XL + BGE-Reranker-FT  | 50.02 | 68.60 | 57.43 | 36.49      | 52.40      | 42.36      |\n| DSI-XL + RankGPT          | 49.47 | 73.68 | 59.25 | 39.00      | 52.00      | 44.75      |\n| Self-Retrieval (StableLM) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| Self-Retrieval (Llama 2)  | 63.44 | 79.29 | 70.00 | 59.94      | 81.06      | 68.74      |",
      "Analysis on reranking In this part, we conduct an in-depth analysis of the reranking performance of Self-Retrieval reranker module in comparison with the fine-tuned BGE-Reranker. We employ DPR-FT, SEAL and GritLM to retrieve 100 passages on TriviaQA, followed by reranking the retrieved results using both approaches. We evaluate performance using MRR@5 as the metric. The experimental results are presented in . The results reveal two key findings: (1) Reranking plays a crucial role in information retrieval systems, significantly enhancing the ranking performance across all models. (2) The Self-Retrieval reranker consistently outperforms the fine-tuned BGE Reranker in most scenarios, demonstrating its robustness and effectiveness. These findings demonstrate that Self-Retrieval performs effectively both as a complete IR system and as a reranker component.\n\nIn Appendix C, we conduct additional experiments with a chunk size of 100 words, demonstrating Self-Retrieval's adaptability to different text segmentation strategies. In Appendix E, we further discuss Self-Retrieval's computational efficiency.\n\n## 5 Conclusion\n\nIn this paper, we propose Self-Retrieval, an end-to-end LLM-driven information retrieval architecture that unifies indexing, retrieval, and reranking in a single LLM. This approach enables the LLM to internalize the corpus, generate relevant content, and perform self-assessment within a unified framework. Unlike previous works that incorporate LLMs into individual IR components, SelfRetrieval provides a unified framework for the entire IR procedure, facilitating knowledge sharing and deep collaboration among different components. Experimental results demonstrate that SelfRetrieval achieves strong performance across various retrieval benchmarks and application scenarios. In future work, we plan to extend our method to further enhance the reliability and trustworthiness of LLM generation.\n\n## Limitations",
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "|          | NQ    | NQ    | TriviaQA   | TriviaQA   |\n|----------|-------|-------|----------|----------|\n|          | 10K   | 40K   | 10K        | 40K        |\n| BGE-FT + StableLM-FT | 43.18 | 41.24 | 56.79      | 58.15      |\n| Self-Retrieval 3B    | 44.62 | 46.11 | 64.03      | 62.69      |\n| BGE-FT + Llama2-FT   | 49.10 | 49.24 | 61.79      | 61.72      |\n| Self-Retrieval 7B    | 53.26 | 52.98 | 72.14      | 70.40      |\n\nThe end-to-end architecture of Self-Retrieval seamlessly integrates retrieval and answer generation into a single inference process. To evaluate its effectiveness in RAG, we compare Self-Retrieval models with a strong baseline that combines BGE-FT for retrieval and fine-tuned versions of StableLM3B and LLaMA2-7B as readers. We conduct experiments on subsets of NQ and TriviaQA using 10K and 40K documents for each dataset. We utilize the top-1 retrieved passage as the context and measure performance using the Exact Match (EM) metric. As shown in Table 5, Self-Retrieval significantly outperforms the baseline on both datasets across different model scales. Unlike other RAG pipelines that separate retrieval and generation, Self-Retrieval integrates the entire process within the LLM framework, enabling more accurate and coherent responses through end-to-end modeling.\n\n## 4.4 Detailed Analysis\n\nScaling model capacity To explore the impact of model scale on retrieval performance, we evaluate Self-Retrieval with various backbone models of different sizes, including StableLM (1.6B, 3B) [4, 44], Llama2 (7B, 13B) [43], and Qwen-1.5 (4B, 7B, 14B) [2].  presents the results on NQ, showing that Self-Retrieval's retrieval performance benefits from the general capabilities of larger language models. For models within the same series, as the model size increases, we observe consistent improvements in both Hits@1 and Hits@5, indicating strong scaling properties of the Self-Retrieval architecture."
    ],
    "verification": {
      "result": "Supported",
      "justification": "Evidence shows reranking improves RAG results, supporting the claim about unfair comparisons.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 19,
    "claim": "Compare the efficiency and effectiveness of using shared models versus separate models for the three steps in Self-Retrieval?",
    "evidence": [
      "We evaluate Self-Retrieval on three representative retrieval benchmarks: NQ, TriviaQA, and MS MARCO. Experimental results demonstrate that Self-Retrieval substantially outperforms existing sparse retrieval, dense retrieval, and generative retrieval methods on both document-level and passage-level retrieval tasks. Furthermore, our experiments on retrieval-augmented generation tasks reveal that Self-Retrieval considerably enhances downstream performance. Additionally, larger LLMs lead to progressively better performance in Self-Retrieval, showing clear scaling benefits. These results demonstrate the effectiveness of Self-Retrieval across different retrieval tasks and application scenarios.\n\nThe potential impacts of this paper may include the following aspects. First, we introduce SelfRetrieval, an end-to-end architecture that consolidates the entire information retrieval system within a single large language model. This unified approach demonstrates substantial performance improvements over existing IR methods. Second, the corpus internalization and indexing mechanism of Self-Retrieval establishes a new paradigm to memorize, organize and retrieve the learned documents (at least part of them) during the pre-training phase, paving the way for more transparent and trustworthy text generation from LLMs. Third, as a LLM-driven retrieval system, Self-Retrieval offers inherent advantages in terms of compatibility, consistency, and interaction with LLMs' internal knowledge. Through experiments on RAG, we demonstrate how this natural compatibility leads to superior performance, suggesting broader potential for enhancing various LLM-based applications.\n\n## 2 Related Work",
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "Recently, information retrieval (IR) systems and large language models (LLMs) have witnessed a growing synergy, with advancements in one field driving progress in the other [13, 56]. On one hand, IR systems have proven effective in augmenting LLMs and mitigating challenges such as hallucinations and outdated knowledge [22, 16]. By providing accurate, up-to-date external knowledge, IR systems significantly enhance the reliability and performance of LLMs. On the other hand, the powerful language understanding and generation capabilities of LLMs have been leveraged to enhance almost all components of traditional IR systems-indexing, retrieval [42, 9, 26], and reranking [58, 27, 40]. Through the integration of LLMs into the IR pipeline, these systems achieve substantially improved retrieval accuracy [57, 1].\n\nHowever, current IR systems typically adopt a pipeline architecture where different components operate in isolation, limiting LLMs' role to specific components rather than leveraging their full\n\n∗ Equally Contribution.\n\n† Corresponding authors.\n\n3 The code of this work is available at https://github.com/icip-cas/SelfRetrieval .\n\n: The Self-Retrieval framework consists of three key components: (1) corpus indexing through self-supervised learning, (2) passage generation via constrained decoding, (3) passage ranking using self-assessment scoring.\n\n<!-- image -->\n\npotential across the entire system. This fragmented approach creates several challenges: it hinders knowledge sharing between components, prevents deep integration of LLMs' diverse capabilities, and results in complex implementations with potentially sub-optimal performance. These limitations underscore the need for a more unified approach that fully integrates LLMs across all components of the IR system. Such an approach would not only maximize the utility of LLMs' capabilities but also simplify system implementation while potentially achieving better performance through enhanced component synergy.",
      "1\n\n## Self-Retrieval: End-to-End Information Retrieval with One Large Language Model\n\nQiaoyu Tang 1 , 2 ∗ , Jiawei Chen 1 , 2 ∗ , Zhuoqun Li 1 , 2 , Bowen Yu 3 , Yaojie Lu 1 , Cheng Fu 3 , Haiyang Yu 3 , Hongyu Lin 1 † , Fei Huang 3 , Ben He 1 , 2 , Xianpei Han 1 † , Le Sun 1 , Yongbin Li 3 † Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences 2 University of Chinese Academy of Sciences 3 Alibaba Group {tangqiaoyu2020,jiawei2020,lizhuoqun2021}@iscas.ac.cn {luyaojie,hongyu,xianpei,sunle}@iscas.ac.cn {yubowen.ybw,fucheng.fuc,yifei.yhy,f.huang,shuide.lyb}@alibaba-inc.com benhe@ucas.ac.cn\n\n## Abstract\n\nThe rise of large language models (LLMs) has significantly transformed both the construction and application of information retrieval (IR) systems. However, current interactions between IR systems and LLMs remain limited, with LLMs merely serving as part of components within IR systems, and IR systems being constructed independently of LLMs. This separated architecture restricts knowledge sharing and deep collaboration between them. In this paper, we introduce Self-Retrieval , a novel end-to-end LLM-driven information retrieval architecture. Self-Retrieval unifies all essential IR functions within a single LLM, leveraging the inherent capabilities of LLMs throughout the IR process. Specifically, Self-Retrieval internalizes the retrieval corpus through self-supervised learning, transforms the retrieval process into sequential passage generation, and performs relevance assessment for reranking. Experimental results demonstrate that SelfRetrieval not only outperforms existing retrieval approaches by a significant margin, but also substantially enhances the performance of LLM-driven downstream applications like retrieval-augmented generation. 3\n\n## 1 Introduction"
    ],
    "verification": {
      "result": "Undetermined",
      "justification": "The evidence discusses Self-Retrieval's performance but does not compare shared vs separate models for the three steps.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 20,
    "claim": "Limitations: The limitations are adequately discussed.",
    "evidence": [
      "Scaling corpus size Recent studies [35, 53] have demonstrated that generative retrieval methods such as DSI or NCI experience more significant performance degradation compared to dense retrieval methods when scaled to larger corpora. To explore the impact of corpus size on Self-Retrieval,\n\n: Impact of model capacity on SelfRetrieval performance.\n\n<!-- image -->\n\n<!-- image -->\n\n: Reranking performance comparison when processing top-100 passages.\n\n<!-- image -->\n\n: Scalability analysis of retrieval performance for Self-Retrieval and BGE-FT across varying corpus sizes.\n\nwe expand our experiments from 10K to 200K documents, scaling the number of passages from 290K to 3M.  illustrates the performance trends of BGE-FT and our Self-Retrieval 3B model on the NQ and TriviaQA datasets with increasing corpus sizes. While both models show performance decrease with larger corpus sizes, Self-Retrieval maintains a degradation rate comparable to BGE-FT. As the number of documents continues to increase, the degradation rate gradually diminishes, demonstrating Self-Retrieval's potential scalability to larger document collections. This observation indicates that Self-Retrieval effectively addresses some of the inherent limitations of generative retrieval approaches in large-scale scenarios.",
      "- [1] Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, Shengling Gao, J. Guo, Xiangnan He, Yanyan Lan, Chenliang Li, Yiqun Liu, Ziyu Lyu, Weizhi Ma, Jun Ma, Zhaochun Ren, Pengjie Ren, Zhiqiang Wang, Min Wang, Jirong Wen, Lei Wu, Xin Xin, Jun Xu, Dawei Yin, Peng Zhang, Fan Zhang, Wei na Zhang, M. Zhang, and Xiaofei Zhu. Information retrieval meets large language models: A strategic report from chinese ir community. ArXiv , abs/2307.09751, 2023.\n- [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023.\n\n[3] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 , 2016.\n\n[4] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, Meng Lee,",
      "| [33]   | Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. From doc2query to doctttttquery. Online preprint , 6:2, 2019. |\n|--------|----------|\n| [34]   | Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2523-2544, Online, June 2021. Association for Computational Linguistics. |\n| [35]   | Ronak Pradeep, Kai Hui, Jai Gupta, Adam Lelkes, Honglei Zhuang, Jimmy Lin, Donald Met- zler, and Vinh Tran. How does generative retrieval scale to millions of passages? In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empir- ical Methods in Natural Language Processing , pages 1305-1321, Singapore, December 2023. Association for Computational Linguistics. |\n| [36]   | Shanbao Qiao, Xuebing Liu, and Seung-Hoon Na. Diffusionret: Diffusion-enhanced gener- ative retriever using constrained decoding. In Conference on Empirical Methods in Natural Language Processing , 2023. |\n| [37]   | Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval , 3(4):333-389, 2009. |\n| [38]   | Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. ColBERTv2: Effective and efficient retrieval via lightweight late interaction. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 3715-3734, Seattle, United States, July 2022. Association for Computational Linguistics.",
      "While our experiments demonstrate the effectiveness of Self-Retrieval, several limitations need to be addressed in future work. Our current evaluation is limited to 200K Wikipedia documents and 3M passages, and testing on larger and noisier text collections is needed. As an LLM-driven system, Self-Retrieval has lower retrieval efficiency compared to sparse or dense retrieval methods, which may limit its applications to specialized knowledge systems. Furthermore, enabling incremental learning and dynamic corpus expansion remains an important direction for future research.\n\n## Acknowledge\n\nWe sincerely thank the reviewers for their insightful comments and valuable suggestions. We are grateful to Le Yu and Xinyu Lu for their helpful feedback on the paper writing. This work was supported by the Natural Science Foundation of China (No. 62122077, 62272439), Beijing Municipal Science and Technology Project (Nos. Z231100010323002), the Basic Research Program of ISCAS (ISCAS-JCZD-202303), and CAS Project for Young Scientists in Basic Research (Grant No.YSBR-040).\n\n## References"
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence discusses limitations but does not fully confirm the claim's adequacy.",
      "confidence": 0.75
    }
  },
  {
    "claim_id": 21,
    "claim": "Soundness: 3\n\nPresentation: 3\n\nContribution: 3\n\nEthics Review Flagged: ['No ethics review needed.']",
    "evidence": [
      "Table 4: Ablation study on NQ and TriviaQA.\n\n| Method          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| Self-Retrieval (base) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| w/o indexing          | 53.05 | 67.16 | 58.95 | 54.45      | 70.64      | 60.98      |\n| w/o title          | 47.81 | 60.90 | 52.81 | 52.32      | 67.91      | 58.48      |\n| w/o self-assessment   | 54.80 | 75.21 | 62.77 | 46.67      | 70.79      | 55.92      |\n\nperformance, with each ablation resulting in substantial performance degradation. Specifically, removing the indexing mechanism restricts the model to internalizing only the documents encountered during training, leading to poor performance on unseen passages. Without titles, we directly generate passages with constrained decoding. The absence of document titles significantly degrades performance, as titles provide critical global information that guides the LLM in generating relevant content. Furthermore, removing the self-assessment mechanism leads to a significant decrease in both datasets. Without self-assessment, the model cannot effectively evaluate and refine its initial retrieved passages, leading to less accurate document rankings. This degradation directly impacts downstream applications such as RAG, where precise passage ranking is crucial for generating highquality responses. These ablation results show that each component of Self-Retrieval addresses a specific challenge in the retrieval process, contributing to its overall effectiveness.\n\n## 4.3 Performance on Retrieval-Augmented Generation\n\nTable 5: The performance on retrieval-augmented generation. For baseline, we use BGE-FT as the retriever and a fine-tuned LLM as reader. Results are reported using Exact Match (EM) scores.",
      "Baselines We evaluate Self-Retrieval models for both passage retrieval and document retrieval, comparing them with sparse, dense, and generative retrieval baselines. The sparse retrieval baselines are:BM25 [37] and DocT5Query [28]. The dense retrieval baselines include: DPR [19], Sentence-T5 [31], GTR [32], BGE [50], text-embedding-ada-002 [30], GritLM [29], and their finetuned variants, DPR-FT and BGE-FT. The generative retrieval baselines comprise: DSI [42], DSIQG [59], NCI [49], Ultron [55], DynamicRetriever [54], GenRet [39], and SEAL [5]. Additionally, to ensure a comprehensive comparison, we also evaluate combinations of strong retrieval baselines with various rerankers, including BGE-Reranker, BGE-Reranker-FT, and RankGPT [41]. In the passage retrieval task, we use the official pre-trained models for all non-fine-tuned dense retrieval baselines. For fine-tuned dense models and generative models, we use their official implementations to replicate the experiments on our dataset. In the document retrieval task, we report the baseline\n\nperformances from their original paper. For comprehensive details about these baselines, please refer to Appendix B.\n\n## 4.2 Main Results\n\nPassage retrieval In Table 1, we compare the performance of Self-Retrieval with various baselines on the NQ and TriviaQA datasets. Self-Retrieval 3B outperforms both strong pre-trained dense retrieval models, such as BGE and GritLM 7B, and other generative retrieval methods. Specifically, Self-Retrieval 3B achieves improvements of 5.46 and 5.07 in MRR@5 over the fine-tuned BGE on NQ and TriviaQA datasets, respectively.",
      "During training, we utilize the gold passage from the supervision data as the positive instance, while sampling negative instances from both the same and different documents. This training strategy conditions the LLM to accurately discern and verify the relevance of its outputs, thereby enhancing its autonomous relevance assessment capabilities and improving the overall precision of the retrieval process.\n\nDuring inference, the overall relevance score S is composed of the document title score S T and the self-assessment score S P . Specifically, the document title score is derived from the title generation probability, while the self-assessment score is calculated based on the probability of the language model rejecting the passage. Formally, for a set of generated titles and passages { ( t 1 , p 1 ) , ( t 2 , p 2 ) , . . . , ( t n , p n ) } , the title score for each ( t i , p i ) is given by:\n\n$$\\mathcal { S } _ { i } ^ { T } = \\text {Softmax} ( P ( t _ { i } | q ; \\theta ) / \\tau ) \\\\$$\n\nand the assessment score is:\n\n$$\\mathcal { S } _ { i } ^ { P } = \\text {softmax} ( ( 1 - P ( \\text {reaction response} | q , t _ { i } , p _ { i } ; \\theta ) ) / \\delta ) \\\\ \\\\ + \\text {a} \\, .$$\n\nwhere τ and δ are temperature parameters used to scale the logits. Based on preliminary experiments on the development set, we simply set τ = δ = 0 . 4 for the main passage retrieval experiments.\n\nThe final relevance score is computed as the product of these two components:\n\n$$\\mathcal { S } = \\mathcal { S } ^ { T } \\cdot \\mathcal { S } ^ { P }$$\n\nThis combined score is then used to rerank the passage set, producing a more refined ordering based on relevance.\n\n## 3.4 Training &amp; Inference",
      "|          | NQ    | NQ    | TriviaQA   | TriviaQA   |\n|----------|-------|-------|----------|----------|\n|          | 10K   | 40K   | 10K        | 40K        |\n| BGE-FT + StableLM-FT | 43.18 | 41.24 | 56.79      | 58.15      |\n| Self-Retrieval 3B    | 44.62 | 46.11 | 64.03      | 62.69      |\n| BGE-FT + Llama2-FT   | 49.10 | 49.24 | 61.79      | 61.72      |\n| Self-Retrieval 7B    | 53.26 | 52.98 | 72.14      | 70.40      |\n\nThe end-to-end architecture of Self-Retrieval seamlessly integrates retrieval and answer generation into a single inference process. To evaluate its effectiveness in RAG, we compare Self-Retrieval models with a strong baseline that combines BGE-FT for retrieval and fine-tuned versions of StableLM3B and LLaMA2-7B as readers. We conduct experiments on subsets of NQ and TriviaQA using 10K and 40K documents for each dataset. We utilize the top-1 retrieved passage as the context and measure performance using the Exact Match (EM) metric. As shown in Table 5, Self-Retrieval significantly outperforms the baseline on both datasets across different model scales. Unlike other RAG pipelines that separate retrieval and generation, Self-Retrieval integrates the entire process within the LLM framework, enabling more accurate and coherent responses through end-to-end modeling.\n\n## 4.4 Detailed Analysis\n\nScaling model capacity To explore the impact of model scale on retrieval performance, we evaluate Self-Retrieval with various backbone models of different sizes, including StableLM (1.6B, 3B) [4, 44], Llama2 (7B, 13B) [43], and Qwen-1.5 (4B, 7B, 14B) [2].  presents the results on NQ, showing that Self-Retrieval's retrieval performance benefits from the general capabilities of larger language models. For models within the same series, as the model size increases, we observe consistent improvements in both Hits@1 and Hits@5, indicating strong scaling properties of the Self-Retrieval architecture."
    ],
    "verification": {
      "result": "Undetermined",
      "justification": "The evidence does not directly address the claim's metrics or their scores.",
      "confidence": 0.8
    }
  },
  {
    "claim_id": 22,
    "claim": "Summary: This paper introduces Self-Retrieval, a new generative retrieval architecture.",
    "evidence": [
      "We evaluate Self-Retrieval on three representative retrieval benchmarks: NQ, TriviaQA, and MS MARCO. Experimental results demonstrate that Self-Retrieval substantially outperforms existing sparse retrieval, dense retrieval, and generative retrieval methods on both document-level and passage-level retrieval tasks. Furthermore, our experiments on retrieval-augmented generation tasks reveal that Self-Retrieval considerably enhances downstream performance. Additionally, larger LLMs lead to progressively better performance in Self-Retrieval, showing clear scaling benefits. These results demonstrate the effectiveness of Self-Retrieval across different retrieval tasks and application scenarios.\n\nThe potential impacts of this paper may include the following aspects. First, we introduce SelfRetrieval, an end-to-end architecture that consolidates the entire information retrieval system within a single large language model. This unified approach demonstrates substantial performance improvements over existing IR methods. Second, the corpus internalization and indexing mechanism of Self-Retrieval establishes a new paradigm to memorize, organize and retrieve the learned documents (at least part of them) during the pre-training phase, paving the way for more transparent and trustworthy text generation from LLMs. Third, as a LLM-driven retrieval system, Self-Retrieval offers inherent advantages in terms of compatibility, consistency, and interaction with LLMs' internal knowledge. Through experiments on RAG, we demonstrate how this natural compatibility leads to superior performance, suggesting broader potential for enhancing various LLM-based applications.\n\n## 2 Related Work",
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "Generative retrieval Generative retrieval methods leverage sequence-to-sequence language models to generate document identifiers for a given query [8, 42]. This paradigm is pioneered by GENRE [7], which introduces the concept of entity retrieval through constrained beam search generation of entity names. DSI [42] extends it to document retrieval by training T5 models to generate document-specific identifiers. The field has since evolved through various innovations, including query generation techniques [11, 59], sophisticated identifier design [48, 51], architectural improvements [5, 36], and continual learning strategies [20, 14].\n\nMost relevant to our work, Yu et al.[52] proposed a \"generate-then-read\" approach, advocating for the use of LLMs to directly generate documents instead of relying on a retriever. UniGen [23] proposed a unified framework that integrates generative retrieval and question answering through a dual-decoder architecture. Compared to them, Self-Retrieval ensures accurate document generation through constrained decoding and accomplishes both retrieval and answer generation in one turn.\n\nThe main distinctions between Self-Retrieval and existing generative retrieval methods can be summarized as follows: (1) Self-Retrieval enables LLMs to directly generate document content rather than relying on other text or numeric identifiers. This approach aligns naturally with LLMs' pretraining objectives, preserves their inherent knowledge, and eliminates the need for complex identifier construction schemes. (2) Self-Retrieval further integrates components such as reranking and answer generation into the framework, further expanding its scope and enhancing the retrieval performance. These distinctions highlight that Self-Retrieval represents a more natural and effective approach for leveraging the capabilities of LLMs in information retrieval.\n\n## 3 Self-Retrieval",
      "Training Self-Retrieval unifies the three distinct tasks of information retrieval - indexing, retrieval, and reranking - into text generation tasks, trained using cross-entropy loss in an auto-regressive manner. Specifically, Self-Retrieval first internalizes the corpus into its parameters through selfsupervised learning as introduced in Section 3.1. Subsequently, in addition to a portion of selfsupervised instances, it incorporates two different types of data to build retrieval and reranking abilities:\n\n- Retrieval data: Utilizes supervised query-passage pairs from the dataset, where the model learns to generate both document titles and passage content in response to input queries.\n- Reranking data: Employs positive and negative examples to train the model in relevance assessment between queries and passages.\n\nThis auto-regressive training approach enables Self-Retrieval to integrate traditionally separate IR components into a unified language model, establishing an end-to-end IR system.\n\nFurthermore, leveraging the universal language generation capabilities of LLMs, we can seamlessly integrate downstream task components, such as readers in RAG, into Self-Retrieval. This integration can be achieved by simply appending the golden answer after the assessment in Self-Retrieval. Consequently, the LLM can function as a comprehensive RAG system, effectively reducing the knowledge gap between IR system and reader modules.\n\nInference During inference, given an input query, Self-Retrieval aims to obtain the relevant passages that are sorted based on the relevance to query. Firstly, the model generates i document titles through constrained beam search. Secondly, for each title, it generates j passages using beam search. Finally, the resulting i × j passages are scored using the self-assessment mechanism and reranked to produce the final output.\n\n## 4 Experimental Results\n\n## 4.1 Experimental Setup"
    ],
    "verification": {
      "result": "Supported",
      "justification": "The evidence directly supports the claim about Self-Retrieval's effectiveness and architecture.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 23,
    "claim": "On subset of NQ and TriviaQA, this approach significantly outperforms existing dual encoders and generative retrieval models.",
    "evidence": [
      "Generative retrieval Generative retrieval methods leverage sequence-to-sequence language models to generate document identifiers for a given query [8, 42]. This paradigm is pioneered by GENRE [7], which introduces the concept of entity retrieval through constrained beam search generation of entity names. DSI [42] extends it to document retrieval by training T5 models to generate document-specific identifiers. The field has since evolved through various innovations, including query generation techniques [11, 59], sophisticated identifier design [48, 51], architectural improvements [5, 36], and continual learning strategies [20, 14].\n\nMost relevant to our work, Yu et al.[52] proposed a \"generate-then-read\" approach, advocating for the use of LLMs to directly generate documents instead of relying on a retriever. UniGen [23] proposed a unified framework that integrates generative retrieval and question answering through a dual-decoder architecture. Compared to them, Self-Retrieval ensures accurate document generation through constrained decoding and accomplishes both retrieval and answer generation in one turn.\n\nThe main distinctions between Self-Retrieval and existing generative retrieval methods can be summarized as follows: (1) Self-Retrieval enables LLMs to directly generate document content rather than relying on other text or numeric identifiers. This approach aligns naturally with LLMs' pretraining objectives, preserves their inherent knowledge, and eliminates the need for complex identifier construction schemes. (2) Self-Retrieval further integrates components such as reranking and answer generation into the framework, further expanding its scope and enhancing the retrieval performance. These distinctions highlight that Self-Retrieval represents a more natural and effective approach for leveraging the capabilities of LLMs in information retrieval.\n\n## 3 Self-Retrieval",
      "Datasets and metrics We conduct main experiments on Natural Questions (NQ) [21] and TriviaQA [18] datasets, both of which are widely used retrieval benchmarks based on Wikipedia. We use their versions from the KILT benchmark [34], which consolidates these datasets into a single pre-processed Wikipedia dump, facilitating easier evaluation. Since the KILT test set is not publicly accessible, we use the development set for testing and randomly sample 2,000 instances from the training set as our development set. For our experiments, we sample approximately 40K documents\n\nTable 1: The experimental results of passage retrieval on NQ and TriviaQA test set. * indicates statistically significant improvements (p &lt; 0.01) over state-of-the-art retrieval baselines.",
      "|          | NQ    | NQ    | TriviaQA   | TriviaQA   |\n|----------|-------|-------|----------|----------|\n|          | 10K   | 40K   | 10K        | 40K        |\n| BGE-FT + StableLM-FT | 43.18 | 41.24 | 56.79      | 58.15      |\n| Self-Retrieval 3B    | 44.62 | 46.11 | 64.03      | 62.69      |\n| BGE-FT + Llama2-FT   | 49.10 | 49.24 | 61.79      | 61.72      |\n| Self-Retrieval 7B    | 53.26 | 52.98 | 72.14      | 70.40      |\n\nThe end-to-end architecture of Self-Retrieval seamlessly integrates retrieval and answer generation into a single inference process. To evaluate its effectiveness in RAG, we compare Self-Retrieval models with a strong baseline that combines BGE-FT for retrieval and fine-tuned versions of StableLM3B and LLaMA2-7B as readers. We conduct experiments on subsets of NQ and TriviaQA using 10K and 40K documents for each dataset. We utilize the top-1 retrieved passage as the context and measure performance using the Exact Match (EM) metric. As shown in Table 5, Self-Retrieval significantly outperforms the baseline on both datasets across different model scales. Unlike other RAG pipelines that separate retrieval and generation, Self-Retrieval integrates the entire process within the LLM framework, enabling more accurate and coherent responses through end-to-end modeling.\n\n## 4.4 Detailed Analysis\n\nScaling model capacity To explore the impact of model scale on retrieval performance, we evaluate Self-Retrieval with various backbone models of different sizes, including StableLM (1.6B, 3B) [4, 44], Llama2 (7B, 13B) [43], and Qwen-1.5 (4B, 7B, 14B) [2].  presents the results on NQ, showing that Self-Retrieval's retrieval performance benefits from the general capabilities of larger language models. For models within the same series, as the model size increases, we observe consistent improvements in both Hits@1 and Hits@5, indicating strong scaling properties of the Self-Retrieval architecture.",
      "LLMfor IR Recent studies have explored leveraging LLMs to enhance various components of IR systems, including query rewriting, retrieval, and reranking. For query rewriting, LLMs have been employed to generate pseudo-documents for query expansion [46] and to rewrite queries based on conversational context [15]. In the retrieval stage, researchers have explored augmenting data by generating pseudo-queries [6, 17] or relevance labels [25] using LLMs, as well as employing LLMs directly as generative retrievers [42, 5]. Regarding reranking, LLMs have been utilized in two ways: serving as rerankers directly [27, 40] and augmenting the reranking dataset [12]. While these methods have advanced specific components within the IR pipeline, Self-Retrieval distinguishes itself by presenting an end-to-end architecture driven entirely by a single LLM, eliminating the need for external components.\n\nDense retrieval Dense retrieval models retrieve information by matching dense vector representations of queries and documents [19]. In this paradigm, an encoder transforms both queries and documents into dense vectors, with relevance determined by their vector distance. Various strategies have been proposed to enhance dense retrievers, including designing loss functions [45], multivector [38], training with synthetic queries [33, 47], and leveraging large-scale query-document pairs [30, 50]. Recent work has also explored using large language models to generate dense vectors for both queries and documents [29]. However, the fundamental limitation of dense retrieval lies in its limited interaction with LLMs, as the compression of natural language into dense vectors inherently constrains the utilization of LLMs' sophisticated language understanding and semantic inference capabilities."
    ],
    "verification": {
      "result": "Supported",
      "justification": "The evidence shows Self-Retrieval outperforms existing models on NQ and TriviaQA subsets.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 24,
    "claim": "The paper presents a self-supervise object to help model memorize the corpus.",
    "evidence": [
      "Self-Retrieval integrates indexing into the LLM's parameters through self-supervised learning, enabling the model to internalize the entire corpus. Unlike generative retrieval methods that rely on complex document identifiers and identifier matching, Self-Retrieval employs a straightforward sentence-to-passage task to construct the index. Specifically, given a passage p = { s 1 , s 2 , ..., s L } consisting of L sentences, each sentence s i is provided as input to the LLM with parameters θ . The training objective is to generate the source passage p in an auto-regressive way, represented as P ( p | s i , θ ) . This self-supervised indexing approach offers several advantages. First, it provides a simple yet effective method for corpus indexing. Second, it naturally frames the indexing process as a retrieval-like task, enabling the model to simultaneously internalize the corpus and develop retrieval capabilities using a consistent data format. Furthermore, this indexing technique closely aligns with the pre-training processes of language models, suggesting that our method could be considered as continued pre-training on the corpus. Through this process, the LLM learns to efficiently memorize and organize corpus information within its parameters.\n\n## 3.2 Retrieval: Generate Relevant Passage through Constrained Decoding\n\nRetrieval serves as a first-pass filter to collect passages related to the input query. In Self-Retrieval, we train the LLM to directly generate relevant passages in response to queries, eliminating the need for intermediaries such as embedding in dense retrieval or document identifier in generative retrieval. Specifically, given the query q and corpus D , Self-Retrieval first generates a potential document title ˆ t as global information, formulated as P ( ˆ t | q ; θ ) . The model then generates a relevant passage, denoted as P (ˆ p | q, ˆ t ; θ ) .",
      "Training Self-Retrieval unifies the three distinct tasks of information retrieval - indexing, retrieval, and reranking - into text generation tasks, trained using cross-entropy loss in an auto-regressive manner. Specifically, Self-Retrieval first internalizes the corpus into its parameters through selfsupervised learning as introduced in Section 3.1. Subsequently, in addition to a portion of selfsupervised instances, it incorporates two different types of data to build retrieval and reranking abilities:\n\n- Retrieval data: Utilizes supervised query-passage pairs from the dataset, where the model learns to generate both document titles and passage content in response to input queries.\n- Reranking data: Employs positive and negative examples to train the model in relevance assessment between queries and passages.\n\nThis auto-regressive training approach enables Self-Retrieval to integrate traditionally separate IR components into a unified language model, establishing an end-to-end IR system.\n\nFurthermore, leveraging the universal language generation capabilities of LLMs, we can seamlessly integrate downstream task components, such as readers in RAG, into Self-Retrieval. This integration can be achieved by simply appending the golden answer after the assessment in Self-Retrieval. Consequently, the LLM can function as a comprehensive RAG system, effectively reducing the knowledge gap between IR system and reader modules.\n\nInference During inference, given an input query, Self-Retrieval aims to obtain the relevant passages that are sorted based on the relevance to query. Firstly, the model generates i document titles through constrained beam search. Secondly, for each title, it generates j passages using beam search. Finally, the resulting i × j passages are scored using the self-assessment mechanism and reranked to produce the final output.\n\n## 4 Experimental Results\n\n## 4.1 Experimental Setup",
      "In this section, we introduce our proposed Self-Retrieval. The overall architecture is illustrated in . Different from traditional information retrieval systems that separate indexing, retrieval, and reranking components, Self-Retrieval integrates these functionalities directly into the parameters of a single large language model:\n\n- Indexing : Self-Retrieval internalizes the entire corpus into its parameters through selfsupervised learning, enabling the model to process passages internally without relying on external indices.\n- Retrieval : Given an input query q , Self-Retrieval generates relevant passage p using the knowledge embedded within its parameters, which is different from dense retrieval or generative retrieval that rely on embedding or document identifiers as proxies of passage.\n\n- Reranking : After generating passage p , Self-Retrieval assesses its relevance to the query q through self-assessment. The output logits provide the basis for reranking candidate passages.\n\nThrough this unified approach, Self-Retrieval enables a streamlined, end-to-end process that enhances the overall effectiveness of information retrieval. In the following sections, we detail each component of our method.\n\n## 3.1 Indexing: Internalize the Corpus",
      "In this paper, we introduce Self-Retrieval, an end-to-end information retrieval architecture driven entirely by one large language model. This integration is not trivial due to the inherent mismatch between information retrieval tasks and text generation, particularly in ensuring accurate document generation using language models. As illustrated in , Self-Retrieval consolidates the separate components of an IR system - indexing, retrieval, and reranking - into the parameters of a single LLM. For indexing, the corpus is internalized into the LLM's parameters through self-supervised learning, enabling the model to encode and store corpus information within its internal representations. During retrieval, Self-Retrieval leverages its encoded knowledge of the corpus to semantically match the input query and directly generates the relevant documents as outputs. To ensure the generated documents exactly match those in the original corpus, we employ the constrained decoding algorithm [10, 8, 24] based on the trie of the corpus. For reranking, Self-Retrieval performs self-assessment on the retrieved documents to evaluate their relevance. The output score is used to rerank the retrieved passages. Moreover, for downstream tasks such as retrieval-augmented generation (RAG), Self-Retrieval integrates the reader component into the model, enabling direct answer generation following retrieval. Through this end-to-end approach, Self-Retrieval fully leverages LLMs' powerful capabilities in language understanding, matching, assessment, and generation to achieve unified information retrieval."
    ],
    "verification": {
      "result": "Supported",
      "justification": "The evidence confirms the paper describes a self-supervised method for corpus memorization via internalizing data into LLM parameters.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 25,
    "claim": "- Strong quality improvements.",
    "evidence": [
      "We comprehensively evaluate Self-Retrieval against various two-stage retriever-reranker pipelines. Specifically, we construct these pipelines using state-of-the-art retrievers (BGE, GTR, GritLM, and DSI-XL) combined with three different reranking approaches: BGE reranker, fine-tuned BGE reranker, and RankGPT. As shown in Table 8, Self-Retrieval achieves superior performance compared to most retriever-reranker combinations, demonstrating the effectiveness of our end-to-end approach over traditional pipeline methods.\n\n## E Efficiency Analysis\n\nWe conduct efficiency analysis on NQ dataset using an NVIDIA A100-80G GPU. Results in Table 9 illustrate that, while Self-Retrieval requires slightly higher computational resources than DSI, it provides notable performance benefits. Notably, Self-Retrieval with a beam size of 10 achieves significantly higher H@5 scores compared to DSI-XL with a beam size of 100, enabling a flexible trade-off between retrieval quality and computational efficiency. When compared to SEAL, which also employs natural language decoding, Self-Retrieval demonstrates more efficient memory usage (30MB vs 444MB) by utilizing a lightweight trie structure instead of SEAL's resource-intensive FM-Index post-processing mechanism. Furthermore, the efficiency of Self-Retrieval stands to benefit from ongoing developments in optimization techniques (e.g., quantization and attention acceleration) and hardware advancements.\n\n| Model Name     | Memory   | Beam Size   | Latency (s)   | Hits@5      |\n|----------|----------|----------|----------|----------|\n| SEAL          | 444MB    | 10 100      | 1.18 5.92     | 61.91 59.57 |\n| DSI-XL         | 0        | 10 100      | 0.23 0.45     | 60.21 60.21 |\n| Self-Retrieval | 30MB     | 10 100      | 1.44 6.06     | 76.17 81.49 |\n\nTable 9: Efficiency analysis.",
      "Scaling corpus size Recent studies [35, 53] have demonstrated that generative retrieval methods such as DSI or NCI experience more significant performance degradation compared to dense retrieval methods when scaled to larger corpora. To explore the impact of corpus size on Self-Retrieval,\n\n: Impact of model capacity on SelfRetrieval performance.\n\n<!-- image -->\n\n<!-- image -->\n\n: Reranking performance comparison when processing top-100 passages.\n\n<!-- image -->\n\n: Scalability analysis of retrieval performance for Self-Retrieval and BGE-FT across varying corpus sizes.\n\nwe expand our experiments from 10K to 200K documents, scaling the number of passages from 290K to 3M.  illustrates the performance trends of BGE-FT and our Self-Retrieval 3B model on the NQ and TriviaQA datasets with increasing corpus sizes. While both models show performance decrease with larger corpus sizes, Self-Retrieval maintains a degradation rate comparable to BGE-FT. As the number of documents continues to increase, the degradation rate gradually diminishes, demonstrating Self-Retrieval's potential scalability to larger document collections. This observation indicates that Self-Retrieval effectively addresses some of the inherent limitations of generative retrieval approaches in large-scale scenarios.",
      "|          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| BGE-FT          | 53.42 | 80.15 | 63.99 | 52.70      | 75.22      | 61.65      |\n| BGE-FT + BGE-Reranker     | 21.91 | 54.58 | 33.33 | 45.36      | 72.16      | 55.78      |\n| BGE-FT + BGE-Reranker-FT  | 52.15 | 76.15 | 61.37 | 44.87      | 67.39      | 53.39      |\n| BGE-FT + RankGPT          | 44.21 | 73.68 | 55.51 | 48.00      | 72.00      | 57.33      |\n| GTR-XL          | 37.64 | 66.84 | 48.94 | 35.97      | 63.75      | 46.67      |\n| GTR-XL + BGE-Reranker     | 26.39 | 59.96 | 38.50 | 42.41      | 68.42      | 52.51      |\n| GTR-XL + BGE-Reranker-FT  | 57.50 | 78.92 | 66.06 | 58.56      | 77.65      | 66.22      |\n| GTR-XL + RankGPT          | 42.11 | 68.42 | 52.30 | 47.00      | 66.00      | 54.95      |\n| GritLM          | 44.67 | 76.00 | 57.03 | 39.91      | 69.34      | 51.14      |\n| GritLM + BGE-Reranker     | 30.06 | 65.87 | 43.20 | 43.64      | 70.87      | 54.23      |\n| GritLM + BGE-Reranker-FT  | 57.57 | 81.35 | 66.98 | 58.60      | 80.54      | 67.21      |\n| GritLM + RankGPT          | 37.89 | 70.53 | 51.19 | 44.00      | 66.00      | 52.70      |\n| DSI-XL          | 43.03 | 60.26 | 49.47 | 29.64      | 46.74      | 36.12      |\n| DSI-XL + BGE-Reranker     | 34.39 | 64.26 | 45.74 | 37.85      | 52.57      | 43.49      |\n| DSI-XL + BGE-Reranker-FT  | 50.02 | 68.60 | 57.43 | 36.49      | 52.40      | 42.36      |\n| DSI-XL + RankGPT          | 49.47 | 73.68 | 59.25 | 39.00      | 52.00      | 44.75      |\n| Self-Retrieval (StableLM) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| Self-Retrieval (Llama 2)  | 63.44 | 79.29 | 70.00 | 59.94      | 81.06      | 68.74      |",
      "During training, we utilize the gold passage from the supervision data as the positive instance, while sampling negative instances from both the same and different documents. This training strategy conditions the LLM to accurately discern and verify the relevance of its outputs, thereby enhancing its autonomous relevance assessment capabilities and improving the overall precision of the retrieval process.\n\nDuring inference, the overall relevance score S is composed of the document title score S T and the self-assessment score S P . Specifically, the document title score is derived from the title generation probability, while the self-assessment score is calculated based on the probability of the language model rejecting the passage. Formally, for a set of generated titles and passages { ( t 1 , p 1 ) , ( t 2 , p 2 ) , . . . , ( t n , p n ) } , the title score for each ( t i , p i ) is given by:\n\n$$\\mathcal { S } _ { i } ^ { T } = \\text {Softmax} ( P ( t _ { i } | q ; \\theta ) / \\tau ) \\\\$$\n\nand the assessment score is:\n\n$$\\mathcal { S } _ { i } ^ { P } = \\text {softmax} ( ( 1 - P ( \\text {reaction response} | q , t _ { i } , p _ { i } ; \\theta ) ) / \\delta ) \\\\ \\\\ + \\text {a} \\, .$$\n\nwhere τ and δ are temperature parameters used to scale the logits. Based on preliminary experiments on the development set, we simply set τ = δ = 0 . 4 for the main passage retrieval experiments.\n\nThe final relevance score is computed as the product of these two components:\n\n$$\\mathcal { S } = \\mathcal { S } ^ { T } \\cdot \\mathcal { S } ^ { P }$$\n\nThis combined score is then used to rerank the passage set, producing a more refined ordering based on relevance.\n\n## 3.4 Training &amp; Inference"
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "Evidence shows Self-Retrieval has quality improvements over some methods, but not all aspects of the claim are explicitly confirmed.",
      "confidence": 0.85
    }
  },
  {
    "claim_id": 26,
    "claim": "The paper reports substantial improvements over previous dual-encoder approaches and generative retrieval models.",
    "evidence": [
      "Generative retrieval Generative retrieval methods leverage sequence-to-sequence language models to generate document identifiers for a given query [8, 42]. This paradigm is pioneered by GENRE [7], which introduces the concept of entity retrieval through constrained beam search generation of entity names. DSI [42] extends it to document retrieval by training T5 models to generate document-specific identifiers. The field has since evolved through various innovations, including query generation techniques [11, 59], sophisticated identifier design [48, 51], architectural improvements [5, 36], and continual learning strategies [20, 14].\n\nMost relevant to our work, Yu et al.[52] proposed a \"generate-then-read\" approach, advocating for the use of LLMs to directly generate documents instead of relying on a retriever. UniGen [23] proposed a unified framework that integrates generative retrieval and question answering through a dual-decoder architecture. Compared to them, Self-Retrieval ensures accurate document generation through constrained decoding and accomplishes both retrieval and answer generation in one turn.\n\nThe main distinctions between Self-Retrieval and existing generative retrieval methods can be summarized as follows: (1) Self-Retrieval enables LLMs to directly generate document content rather than relying on other text or numeric identifiers. This approach aligns naturally with LLMs' pretraining objectives, preserves their inherent knowledge, and eliminates the need for complex identifier construction schemes. (2) Self-Retrieval further integrates components such as reranking and answer generation into the framework, further expanding its scope and enhancing the retrieval performance. These distinctions highlight that Self-Retrieval represents a more natural and effective approach for leveraging the capabilities of LLMs in information retrieval.\n\n## 3 Self-Retrieval",
      "- DPR [19] is a dual-encoder model trained with in-batch negative sampling. We fine-tune DPR on our training datasets to obtain DPR-FT , following the official implementation and hyperparameter settings.\n- BGE [50] is a state-of-the-art universal embedding model trained on approximately 200 million text pairs using contrastive learning. We employ the bge-large-en-v1.5 variant and fine-tune it on our training datasets to obtain BGE-FT . The fine-tuning process uses a learning rate of 1e-5, batch size of 128, and runs for 10 epochs.\n- Sentence-T5 [31] employs a dual-encoder T5 architecture to generate semantic embeddings through contrastive learning for efficient retrieval.\n- GTR-XL [32] is a dense retrieval model based on Sentence-T5, pre-trained on billions of question-answer pairs.\n- Text-embedding-ada-002 is a powerful embedding model developed by OpenAI, accessible through their API service.\n- GritLM [29] is built upon the Mistral 7B language model and optimized using both embedding and generation objectives.\n\nThe generative retrieval baselines are as follows:\n\n- DSI [42] is a sequence-to-sequence model that directly maps queries to document identifiers.\n- DSI-QG [59] enhances the DSI framework by incorporating a doc2query model for dataset augmentation.\n- SEAL [5] utilizes n-gram as the document identifiers and constrains the generation process using FM-index.\n- NCI+BGE-Reranker-FT . NCI [49] employs a sequence-to-sequence architecture with a prefix-aware weight-adaptive decoder. We train the model using T5-Large for documentlevel retrieval following the official implementation. To obtain passage-level results, we further incorporate a fine-tuned BGE reranker (bge-reranker-large).",
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "Baselines We evaluate Self-Retrieval models for both passage retrieval and document retrieval, comparing them with sparse, dense, and generative retrieval baselines. The sparse retrieval baselines are:BM25 [37] and DocT5Query [28]. The dense retrieval baselines include: DPR [19], Sentence-T5 [31], GTR [32], BGE [50], text-embedding-ada-002 [30], GritLM [29], and their finetuned variants, DPR-FT and BGE-FT. The generative retrieval baselines comprise: DSI [42], DSIQG [59], NCI [49], Ultron [55], DynamicRetriever [54], GenRet [39], and SEAL [5]. Additionally, to ensure a comprehensive comparison, we also evaluate combinations of strong retrieval baselines with various rerankers, including BGE-Reranker, BGE-Reranker-FT, and RankGPT [41]. In the passage retrieval task, we use the official pre-trained models for all non-fine-tuned dense retrieval baselines. For fine-tuned dense models and generative models, we use their official implementations to replicate the experiments on our dataset. In the document retrieval task, we report the baseline\n\nperformances from their original paper. For comprehensive details about these baselines, please refer to Appendix B.\n\n## 4.2 Main Results\n\nPassage retrieval In Table 1, we compare the performance of Self-Retrieval with various baselines on the NQ and TriviaQA datasets. Self-Retrieval 3B outperforms both strong pre-trained dense retrieval models, such as BGE and GritLM 7B, and other generative retrieval methods. Specifically, Self-Retrieval 3B achieves improvements of 5.46 and 5.07 in MRR@5 over the fine-tuned BGE on NQ and TriviaQA datasets, respectively."
    ],
    "verification": {
      "result": "Supported",
      "justification": "The evidence shows Self-Retrieval outperforms generative retrieval models and dual-encoder approaches in passage retrieval.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 27,
    "claim": "- Ablations shows that the model scales well with model size.",
    "evidence": [
      "Table 4: Ablation study on NQ and TriviaQA.\n\n| Method          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| Self-Retrieval (base) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| w/o indexing          | 53.05 | 67.16 | 58.95 | 54.45      | 70.64      | 60.98      |\n| w/o title          | 47.81 | 60.90 | 52.81 | 52.32      | 67.91      | 58.48      |\n| w/o self-assessment   | 54.80 | 75.21 | 62.77 | 46.67      | 70.79      | 55.92      |\n\nperformance, with each ablation resulting in substantial performance degradation. Specifically, removing the indexing mechanism restricts the model to internalizing only the documents encountered during training, leading to poor performance on unseen passages. Without titles, we directly generate passages with constrained decoding. The absence of document titles significantly degrades performance, as titles provide critical global information that guides the LLM in generating relevant content. Furthermore, removing the self-assessment mechanism leads to a significant decrease in both datasets. Without self-assessment, the model cannot effectively evaluate and refine its initial retrieved passages, leading to less accurate document rankings. This degradation directly impacts downstream applications such as RAG, where precise passage ranking is crucial for generating highquality responses. These ablation results show that each component of Self-Retrieval addresses a specific challenge in the retrieval process, contributing to its overall effectiveness.\n\n## 4.3 Performance on Retrieval-Augmented Generation\n\nTable 5: The performance on retrieval-augmented generation. For baseline, we use BGE-FT as the retriever and a fine-tuned LLM as reader. Results are reported using Exact Match (EM) scores.",
      "|          |        | NQ      | NQ    | NQ      | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|--------|---------|-------|---------|----------|----------|----------|\n| Model          | Params | H@1     | H@5   | M@5     | H@1        | H@5        | M@5        |\n| Sparse Retrieval BM25 [37]       | -      | 14.54   | 32.71 | 21.13   | 20.09      | 42.73      | 28.35      |\n| Dense Retrieval DPR [19]         | 110M   | 40.41   | 61.79 | 48.80   | 35.57      | 57.39      | 43.93      |\n| DPR-FT [19]          | 110M   | 42.21   | 60.45 | 49.33   | 36.58      | 53.05      | 42.91      |\n| BGE [50]          | 335M   | 36.30   | 66.95 | 48.05   | 46.97      | 70.14      | 55.95      |\n| BGE-FT [50]          | 335M   | 53.42   | 80.15 | 63.99   | 52.70      | 75.22      | 61.65      |\n| BGE-FT + BGE-Reranker-FT         | 770M   | 52.15   | 76.15 | 61.37   | 44.87      | 67.39      | 53.39      |\n| GTR-XL [32]          | 1.24B  | 37.64   | 66.84 | 48.94   | 35.97      | 63.75      | 46.67      |\n| GTR-XL + BGE-Reranker-FT         | 1.57B  | 57.50   | 78.92 | 66.06   | 58.56      | 77.65      | 66.22      |\n| GTR-XXL [32]          | 4.86B  | 39.21   | 69.72 | 50.88   | 35.97      | 64.15      | 46.83      |\n| text-embedding-ada-002          | -      | 34.28   | 62.28 | 44.64   | 35.09      | 62.00      | 45.15      |\n| GritLM [29]          | 7.24B  | 44.67   | 76.00 | 57.03   | 39.91      | 69.34      | 51.14      |\n| GritLM + BGE-Reranker-FT         | 7.57B  | 57.57   | 81.35 | 66.98   | 58.60      | 80.54      | 67.21      |\n| Generative retrieval DSI-XL [42] | 2.85B  | 43.03   | 60.26 | 49.47   | 29.64      | 46.74      | 36.12      |\n| DSI-XXL [42]          | 11.3B  | 43.81   | 60.45 | 50.20   | 30.55      | 46.67      | 36.56      |\n| SEAL [5]          | 406M   | 36.79   | 61.35 | 45.88   | 36.88      | 61.66      | 46.29      |\n| DSI-QG [59]          | 2.85B  | 34.88   | 56.60 | 43.33   | 29.15      | 45.53      | 35.20      |\n| NCI + BGE-Reranker-FT          | 1.07B  | 50.86   | 70.27 | 58.53   | 28.42      | 42.18      | 33.62      |\n| Self-Retrieval (StableLM)        | 2.8B   | 62.16 ∗ | 79.28 | 69.45 ∗ | 58.69 ∗    | 78.39 ∗    | 66.72 ∗    |\n| Self-Retrieval (Llama 2)         | 6.74B  | 63.44 ∗ | 79.29 | 70.00 ∗ | 59.94 ∗    | 81.06 ∗    | 68.74 ∗    |",
      "- Ultron [55] represents documents using three types of identifiers (URL, PQ, Atomic) and trains the model through a progressive three-stage pipeline.\n- DynamicRetriever [54] parameterizes traditional static indices by embedding both tokenlevel and document-level corpus information into a pre-trained model for dynamic document identifier generation.\n- GenRet [39] employs discrete auto-encoding with progressive training and clustering techniques to learn semantic document identifiers for generative retrieval.\n\n## C Ablation on Chunk Size\n\nTo investigate the potential impact of chunk size, we conduct additional experiments comparing SelfRetrieval with strong baselines on the NQ dataset using a chunk size of 100 words, complementing our main experiments where chunk size is set to 200. The experimental results are presented in Table 7. It demonstrates that Self-Retrieval significantly outperforms the baselines with both chunk sizes settings, further validating the effectiveness of our proposed method.\n\nTable 7: Retrieval performance with chunk length of 100 words.\n\n| Model          | Params   |   Hits@1 |   Hits@5 |   MRR@5 |\n|----------|----------|----------|----------|---------|\n| BGE-FT          | 335M     |    40.79 |    58.92 |   47.76 |\n| GritLM          | 7B       |    30.95 |    51.36 |   38.77 |\n| Self-Retrieval (StableLM) | 3B       |    58.43 |    77.76 |   66.18 |\n\n## D Full Comparison with Retriever-Reranker Pipeline\n\nTable 8: Comparison between Self-Retrieval and traditional two-stage retriever-reranker pipelines.",
      "Scaling corpus size Recent studies [35, 53] have demonstrated that generative retrieval methods such as DSI or NCI experience more significant performance degradation compared to dense retrieval methods when scaled to larger corpora. To explore the impact of corpus size on Self-Retrieval,\n\n: Impact of model capacity on SelfRetrieval performance.\n\n<!-- image -->\n\n<!-- image -->\n\n: Reranking performance comparison when processing top-100 passages.\n\n<!-- image -->\n\n: Scalability analysis of retrieval performance for Self-Retrieval and BGE-FT across varying corpus sizes.\n\nwe expand our experiments from 10K to 200K documents, scaling the number of passages from 290K to 3M.  illustrates the performance trends of BGE-FT and our Self-Retrieval 3B model on the NQ and TriviaQA datasets with increasing corpus sizes. While both models show performance decrease with larger corpus sizes, Self-Retrieval maintains a degradation rate comparable to BGE-FT. As the number of documents continues to increase, the degradation rate gradually diminishes, demonstrating Self-Retrieval's potential scalability to larger document collections. This observation indicates that Self-Retrieval effectively addresses some of the inherent limitations of generative retrieval approaches in large-scale scenarios."
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "Ablation study shows performance degradation, but no direct evidence of scaling with model size.",
      "confidence": 0.75
    }
  },
  {
    "claim_id": 28,
    "claim": "Previous dense retrievers often plateau due to the bottleneck layer; the scaling curve of this new architecture is promising.",
    "evidence": [
      "We comprehensively evaluate Self-Retrieval against various two-stage retriever-reranker pipelines. Specifically, we construct these pipelines using state-of-the-art retrievers (BGE, GTR, GritLM, and DSI-XL) combined with three different reranking approaches: BGE reranker, fine-tuned BGE reranker, and RankGPT. As shown in Table 8, Self-Retrieval achieves superior performance compared to most retriever-reranker combinations, demonstrating the effectiveness of our end-to-end approach over traditional pipeline methods.\n\n## E Efficiency Analysis\n\nWe conduct efficiency analysis on NQ dataset using an NVIDIA A100-80G GPU. Results in Table 9 illustrate that, while Self-Retrieval requires slightly higher computational resources than DSI, it provides notable performance benefits. Notably, Self-Retrieval with a beam size of 10 achieves significantly higher H@5 scores compared to DSI-XL with a beam size of 100, enabling a flexible trade-off between retrieval quality and computational efficiency. When compared to SEAL, which also employs natural language decoding, Self-Retrieval demonstrates more efficient memory usage (30MB vs 444MB) by utilizing a lightweight trie structure instead of SEAL's resource-intensive FM-Index post-processing mechanism. Furthermore, the efficiency of Self-Retrieval stands to benefit from ongoing developments in optimization techniques (e.g., quantization and attention acceleration) and hardware advancements.\n\n| Model Name     | Memory   | Beam Size   | Latency (s)   | Hits@5      |\n|----------|----------|----------|----------|----------|\n| SEAL          | 444MB    | 10 100      | 1.18 5.92     | 61.91 59.57 |\n| DSI-XL         | 0        | 10 100      | 0.23 0.45     | 60.21 60.21 |\n| Self-Retrieval | 30MB     | 10 100      | 1.44 6.06     | 76.17 81.49 |\n\nTable 9: Efficiency analysis.",
      "|          |        | NQ      | NQ    | NQ      | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|--------|---------|-------|---------|----------|----------|----------|\n| Model          | Params | H@1     | H@5   | M@5     | H@1        | H@5        | M@5        |\n| Sparse Retrieval BM25 [37]       | -      | 14.54   | 32.71 | 21.13   | 20.09      | 42.73      | 28.35      |\n| Dense Retrieval DPR [19]         | 110M   | 40.41   | 61.79 | 48.80   | 35.57      | 57.39      | 43.93      |\n| DPR-FT [19]          | 110M   | 42.21   | 60.45 | 49.33   | 36.58      | 53.05      | 42.91      |\n| BGE [50]          | 335M   | 36.30   | 66.95 | 48.05   | 46.97      | 70.14      | 55.95      |\n| BGE-FT [50]          | 335M   | 53.42   | 80.15 | 63.99   | 52.70      | 75.22      | 61.65      |\n| BGE-FT + BGE-Reranker-FT         | 770M   | 52.15   | 76.15 | 61.37   | 44.87      | 67.39      | 53.39      |\n| GTR-XL [32]          | 1.24B  | 37.64   | 66.84 | 48.94   | 35.97      | 63.75      | 46.67      |\n| GTR-XL + BGE-Reranker-FT         | 1.57B  | 57.50   | 78.92 | 66.06   | 58.56      | 77.65      | 66.22      |\n| GTR-XXL [32]          | 4.86B  | 39.21   | 69.72 | 50.88   | 35.97      | 64.15      | 46.83      |\n| text-embedding-ada-002          | -      | 34.28   | 62.28 | 44.64   | 35.09      | 62.00      | 45.15      |\n| GritLM [29]          | 7.24B  | 44.67   | 76.00 | 57.03   | 39.91      | 69.34      | 51.14      |\n| GritLM + BGE-Reranker-FT         | 7.57B  | 57.57   | 81.35 | 66.98   | 58.60      | 80.54      | 67.21      |\n| Generative retrieval DSI-XL [42] | 2.85B  | 43.03   | 60.26 | 49.47   | 29.64      | 46.74      | 36.12      |\n| DSI-XXL [42]          | 11.3B  | 43.81   | 60.45 | 50.20   | 30.55      | 46.67      | 36.56      |\n| SEAL [5]          | 406M   | 36.79   | 61.35 | 45.88   | 36.88      | 61.66      | 46.29      |\n| DSI-QG [59]          | 2.85B  | 34.88   | 56.60 | 43.33   | 29.15      | 45.53      | 35.20      |\n| NCI + BGE-Reranker-FT          | 1.07B  | 50.86   | 70.27 | 58.53   | 28.42      | 42.18      | 33.62      |\n| Self-Retrieval (StableLM)        | 2.8B   | 62.16 ∗ | 79.28 | 69.45 ∗ | 58.69 ∗    | 78.39 ∗    | 66.72 ∗    |\n| Self-Retrieval (Llama 2)         | 6.74B  | 63.44 ∗ | 79.29 | 70.00 ∗ | 59.94 ∗    | 81.06 ∗    | 68.74 ∗    |",
      "Scaling corpus size Recent studies [35, 53] have demonstrated that generative retrieval methods such as DSI or NCI experience more significant performance degradation compared to dense retrieval methods when scaled to larger corpora. To explore the impact of corpus size on Self-Retrieval,\n\n: Impact of model capacity on SelfRetrieval performance.\n\n<!-- image -->\n\n<!-- image -->\n\n: Reranking performance comparison when processing top-100 passages.\n\n<!-- image -->\n\n: Scalability analysis of retrieval performance for Self-Retrieval and BGE-FT across varying corpus sizes.\n\nwe expand our experiments from 10K to 200K documents, scaling the number of passages from 290K to 3M.  illustrates the performance trends of BGE-FT and our Self-Retrieval 3B model on the NQ and TriviaQA datasets with increasing corpus sizes. While both models show performance decrease with larger corpus sizes, Self-Retrieval maintains a degradation rate comparable to BGE-FT. As the number of documents continues to increase, the degradation rate gradually diminishes, demonstrating Self-Retrieval's potential scalability to larger document collections. This observation indicates that Self-Retrieval effectively addresses some of the inherent limitations of generative retrieval approaches in large-scale scenarios.",
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |"
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence discusses scalability but does not directly address dense retrievers plateauing or the new architecture's scaling curve.",
      "confidence": 0.75
    }
  },
  {
    "claim_id": 29,
    "claim": "Weaknesses: - Experiment only used wikipedia-based datasets.",
    "evidence": [
      "Datasets and metrics We conduct main experiments on Natural Questions (NQ) [21] and TriviaQA [18] datasets, both of which are widely used retrieval benchmarks based on Wikipedia. We use their versions from the KILT benchmark [34], which consolidates these datasets into a single pre-processed Wikipedia dump, facilitating easier evaluation. Since the KILT test set is not publicly accessible, we use the development set for testing and randomly sample 2,000 instances from the training set as our development set. For our experiments, we sample approximately 40K documents\n\nTable 1: The experimental results of passage retrieval on NQ and TriviaQA test set. * indicates statistically significant improvements (p &lt; 0.01) over state-of-the-art retrieval baselines.",
      "While our experiments demonstrate the effectiveness of Self-Retrieval, several limitations need to be addressed in future work. Our current evaluation is limited to 200K Wikipedia documents and 3M passages, and testing on larger and noisier text collections is needed. As an LLM-driven system, Self-Retrieval has lower retrieval efficiency compared to sparse or dense retrieval methods, which may limit its applications to specialized knowledge systems. Furthermore, enabling incremental learning and dynamic corpus expansion remains an important direction for future research.\n\n## Acknowledge\n\nWe sincerely thank the reviewers for their insightful comments and valuable suggestions. We are grateful to Le Yu and Xinyu Lu for their helpful feedback on the paper writing. This work was supported by the Natural Science Foundation of China (No. 62122077, 62272439), Beijing Municipal Science and Technology Project (Nos. Z231100010323002), the Basic Research Program of ISCAS (ISCAS-JCZD-202303), and CAS Project for Young Scientists in Basic Research (Grant No.YSBR-040).\n\n## References",
      "from Wikipedia for each dataset. Each document is segmented into passages of maximum 200 words, yielding approximately 1 million passages in total. The detailed statistics of the datasets are presented in Appendix A. We use passage-level Hits@{1, 5} and Mean Reciprocal Rank (MRR)@5 as evaluation metrics.\n\nTo comprehensively compare with other generative information retrieval methods, we also conduct experiments on document retrieval. Following NCI [49], we conduct experiments on NQ320K and utilize Recall@{1, 10} and MRR@100 as the evaluation metrics. To evaluate the model's robustness in non-Wikipedia scenarios where high-quality text and titles are not available, we conduct experiments on a subset of MS MARCO [3] following the experimental setup of Ultron [55]. The performance was measured using Recall@{1,5} and MRR@10.\n\nImplementation details In this study, we employ StableLM-3B [44] and Llama2-7B [43] as passage retrieval backbones. For document retrieval, we employ StableLM-1.6B [4] for NQ320K and StableLM-3B for MS MARCO. We train the models using ZeRO stage-2 optimization on 8 NVIDIA A100 (80 GB) GPUs with the AdamW optimizer, a batch size of 16 per GPU, and BFloat16 precision. The models are trained for 3 epochs with a learning rate of 2e-5. During inference, we use beam search to generate 5 titles and 10 passages for each title, with hyperparameters τ and δ set to 0.4 across all models and datasets.",
      "## A Dataset Statistics\n\nTable 6 presents the statistics of the NQ and TriviaQA datasets used in our experiments.\n\n| Dataset   | Natural Questions   | Natural Questions   | TriviaQA   | TriviaQA   |\n|----------|----------|----------|----------|----------|\n|          | 10K          | 40K          | 10K        | 40K        |\n| # doc     | 10,000          | 37,202          | 10,000     | 38,399     |\n| # psg     | 291,506          | 979,804          | 390,586    | 1,193,047  |\n| # train   | 32,163          | 72.716          | 29,038     | 51,166     |\n| # dev     | 2,000          | 2,000          | 2,000      | 2,000      |\n| # test    | 2,837          | 2,837          | 5,355      | 5,355      |\n\nTable 6: Statistics of the experimental datasets. #doc/#psg denotes number of documents/passages; #train/#dev/#test denotes size of training/development/test set. Training instances without querydocument pairs are removed.\n\n## B Baselines\n\nThe sparse retrieval baselines are as follows:\n\n- BM25 [37] is a classical sparse retrieval algorithm based on probabilistic relevance framework and term frequency statistics.\n- DocT5Query [28] expands documents by generating potential queries using a fine-tuned T5 model.\n\nThe dense retrieval baselines are as follows:"
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The claim mentions using Wikipedia-based datasets, which is supported by the evidence, but lacks specific details on dataset limitations.",
      "confidence": 0.85
    }
  },
  {
    "claim_id": 30,
    "claim": "However, wikipedia is heavily used in pretraining, so it is unclear if the proposed approach can let model sufficiently memorize other datasets.",
    "evidence": [
      "While our experiments demonstrate the effectiveness of Self-Retrieval, several limitations need to be addressed in future work. Our current evaluation is limited to 200K Wikipedia documents and 3M passages, and testing on larger and noisier text collections is needed. As an LLM-driven system, Self-Retrieval has lower retrieval efficiency compared to sparse or dense retrieval methods, which may limit its applications to specialized knowledge systems. Furthermore, enabling incremental learning and dynamic corpus expansion remains an important direction for future research.\n\n## Acknowledge\n\nWe sincerely thank the reviewers for their insightful comments and valuable suggestions. We are grateful to Le Yu and Xinyu Lu for their helpful feedback on the paper writing. This work was supported by the Natural Science Foundation of China (No. 62122077, 62272439), Beijing Municipal Science and Technology Project (Nos. Z231100010323002), the Basic Research Program of ISCAS (ISCAS-JCZD-202303), and CAS Project for Young Scientists in Basic Research (Grant No.YSBR-040).\n\n## References",
      "Datasets and metrics We conduct main experiments on Natural Questions (NQ) [21] and TriviaQA [18] datasets, both of which are widely used retrieval benchmarks based on Wikipedia. We use their versions from the KILT benchmark [34], which consolidates these datasets into a single pre-processed Wikipedia dump, facilitating easier evaluation. Since the KILT test set is not publicly accessible, we use the development set for testing and randomly sample 2,000 instances from the training set as our development set. For our experiments, we sample approximately 40K documents\n\nTable 1: The experimental results of passage retrieval on NQ and TriviaQA test set. * indicates statistically significant improvements (p &lt; 0.01) over state-of-the-art retrieval baselines.",
      "Training Self-Retrieval unifies the three distinct tasks of information retrieval - indexing, retrieval, and reranking - into text generation tasks, trained using cross-entropy loss in an auto-regressive manner. Specifically, Self-Retrieval first internalizes the corpus into its parameters through selfsupervised learning as introduced in Section 3.1. Subsequently, in addition to a portion of selfsupervised instances, it incorporates two different types of data to build retrieval and reranking abilities:\n\n- Retrieval data: Utilizes supervised query-passage pairs from the dataset, where the model learns to generate both document titles and passage content in response to input queries.\n- Reranking data: Employs positive and negative examples to train the model in relevance assessment between queries and passages.\n\nThis auto-regressive training approach enables Self-Retrieval to integrate traditionally separate IR components into a unified language model, establishing an end-to-end IR system.\n\nFurthermore, leveraging the universal language generation capabilities of LLMs, we can seamlessly integrate downstream task components, such as readers in RAG, into Self-Retrieval. This integration can be achieved by simply appending the golden answer after the assessment in Self-Retrieval. Consequently, the LLM can function as a comprehensive RAG system, effectively reducing the knowledge gap between IR system and reader modules.\n\nInference During inference, given an input query, Self-Retrieval aims to obtain the relevant passages that are sorted based on the relevance to query. Firstly, the model generates i document titles through constrained beam search. Secondly, for each title, it generates j passages using beam search. Finally, the resulting i × j passages are scored using the self-assessment mechanism and reranked to produce the final output.\n\n## 4 Experimental Results\n\n## 4.1 Experimental Setup",
      "Self-Retrieval integrates indexing into the LLM's parameters through self-supervised learning, enabling the model to internalize the entire corpus. Unlike generative retrieval methods that rely on complex document identifiers and identifier matching, Self-Retrieval employs a straightforward sentence-to-passage task to construct the index. Specifically, given a passage p = { s 1 , s 2 , ..., s L } consisting of L sentences, each sentence s i is provided as input to the LLM with parameters θ . The training objective is to generate the source passage p in an auto-regressive way, represented as P ( p | s i , θ ) . This self-supervised indexing approach offers several advantages. First, it provides a simple yet effective method for corpus indexing. Second, it naturally frames the indexing process as a retrieval-like task, enabling the model to simultaneously internalize the corpus and develop retrieval capabilities using a consistent data format. Furthermore, this indexing technique closely aligns with the pre-training processes of language models, suggesting that our method could be considered as continued pre-training on the corpus. Through this process, the LLM learns to efficiently memorize and organize corpus information within its parameters.\n\n## 3.2 Retrieval: Generate Relevant Passage through Constrained Decoding\n\nRetrieval serves as a first-pass filter to collect passages related to the input query. In Self-Retrieval, we train the LLM to directly generate relevant passages in response to queries, eliminating the need for intermediaries such as embedding in dense retrieval or document identifier in generative retrieval. Specifically, given the query q and corpus D , Self-Retrieval first generates a potential document title ˆ t as global information, formulated as P ( ˆ t | q ; θ ) . The model then generates a relevant passage, denoted as P (ˆ p | q, ˆ t ; θ ) ."
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence mentions Wikipedia use in pretraining but does not confirm if the approach ensures sufficient memorization of other datasets.",
      "confidence": 0.75
    }
  },
  {
    "claim_id": 31,
    "claim": "- Missing dense retrieval + cross-attention reranking baselines.",
    "evidence": [
      "Baselines We evaluate Self-Retrieval models for both passage retrieval and document retrieval, comparing them with sparse, dense, and generative retrieval baselines. The sparse retrieval baselines are:BM25 [37] and DocT5Query [28]. The dense retrieval baselines include: DPR [19], Sentence-T5 [31], GTR [32], BGE [50], text-embedding-ada-002 [30], GritLM [29], and their finetuned variants, DPR-FT and BGE-FT. The generative retrieval baselines comprise: DSI [42], DSIQG [59], NCI [49], Ultron [55], DynamicRetriever [54], GenRet [39], and SEAL [5]. Additionally, to ensure a comprehensive comparison, we also evaluate combinations of strong retrieval baselines with various rerankers, including BGE-Reranker, BGE-Reranker-FT, and RankGPT [41]. In the passage retrieval task, we use the official pre-trained models for all non-fine-tuned dense retrieval baselines. For fine-tuned dense models and generative models, we use their official implementations to replicate the experiments on our dataset. In the document retrieval task, we report the baseline\n\nperformances from their original paper. For comprehensive details about these baselines, please refer to Appendix B.\n\n## 4.2 Main Results\n\nPassage retrieval In Table 1, we compare the performance of Self-Retrieval with various baselines on the NQ and TriviaQA datasets. Self-Retrieval 3B outperforms both strong pre-trained dense retrieval models, such as BGE and GritLM 7B, and other generative retrieval methods. Specifically, Self-Retrieval 3B achieves improvements of 5.46 and 5.07 in MRR@5 over the fine-tuned BGE on NQ and TriviaQA datasets, respectively.",
      "Training Self-Retrieval unifies the three distinct tasks of information retrieval - indexing, retrieval, and reranking - into text generation tasks, trained using cross-entropy loss in an auto-regressive manner. Specifically, Self-Retrieval first internalizes the corpus into its parameters through selfsupervised learning as introduced in Section 3.1. Subsequently, in addition to a portion of selfsupervised instances, it incorporates two different types of data to build retrieval and reranking abilities:\n\n- Retrieval data: Utilizes supervised query-passage pairs from the dataset, where the model learns to generate both document titles and passage content in response to input queries.\n- Reranking data: Employs positive and negative examples to train the model in relevance assessment between queries and passages.\n\nThis auto-regressive training approach enables Self-Retrieval to integrate traditionally separate IR components into a unified language model, establishing an end-to-end IR system.\n\nFurthermore, leveraging the universal language generation capabilities of LLMs, we can seamlessly integrate downstream task components, such as readers in RAG, into Self-Retrieval. This integration can be achieved by simply appending the golden answer after the assessment in Self-Retrieval. Consequently, the LLM can function as a comprehensive RAG system, effectively reducing the knowledge gap between IR system and reader modules.\n\nInference During inference, given an input query, Self-Retrieval aims to obtain the relevant passages that are sorted based on the relevance to query. Firstly, the model generates i document titles through constrained beam search. Secondly, for each title, it generates j passages using beam search. Finally, the resulting i × j passages are scored using the self-assessment mechanism and reranked to produce the final output.\n\n## 4 Experimental Results\n\n## 4.1 Experimental Setup",
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 2795-2806, Online, August 2021. Association for Computational Linguistics. |\n| [25]   | Guangyuan Ma, Xing Wu, Peng Wang, Zijia Lin, and Songlin Hu. Pre-training with large lan- guage model-based document expansion for dense passage retrieval. ArXiv , abs/2308.08285, 2023. |\n| [26]   | Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. ArXiv , abs/2310.08319, 2023. |\n| [27]   | Xueguang Ma, Xinyu Crystina Zhang, Ronak Pradeep, and Jimmy J. Lin. Zero-shot listwise document reranking with a large language model. ArXiv , abs/2305.02156, 2023. |\n| [28]   | Antonio Mallia, O. Khattab, Nicola Tonellotto, and Torsten Suel. Learning passage impacts for inverted indexes. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval , 2021. |\n| [29]   | Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning, 2024. |\n| [30]   | Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005 , 2022. |\n| [31]   | Jianmo Ni, Gustavo Hernández Abrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Matthew Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. ArXiv , abs/2108.08877, 2021. |\n| [32]   | Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers."
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "Evidence mentions dense retrieval baselines but not cross-attention reranking specifically.",
      "confidence": 0.85
    }
  },
  {
    "claim_id": 32,
    "claim": "Since the proposed method's reranking stage essentially uses cross attention to judge the query and the retrieved candidate passage, the computational cost of the reranking stage is similar to that of a separate cross-attention reranker.",
    "evidence": [
      "Analysis on reranking In this part, we conduct an in-depth analysis of the reranking performance of Self-Retrieval reranker module in comparison with the fine-tuned BGE-Reranker. We employ DPR-FT, SEAL and GritLM to retrieve 100 passages on TriviaQA, followed by reranking the retrieved results using both approaches. We evaluate performance using MRR@5 as the metric. The experimental results are presented in . The results reveal two key findings: (1) Reranking plays a crucial role in information retrieval systems, significantly enhancing the ranking performance across all models. (2) The Self-Retrieval reranker consistently outperforms the fine-tuned BGE Reranker in most scenarios, demonstrating its robustness and effectiveness. These findings demonstrate that Self-Retrieval performs effectively both as a complete IR system and as a reranker component.\n\nIn Appendix C, we conduct additional experiments with a chunk size of 100 words, demonstrating Self-Retrieval's adaptability to different text segmentation strategies. In Appendix E, we further discuss Self-Retrieval's computational efficiency.\n\n## 5 Conclusion\n\nIn this paper, we propose Self-Retrieval, an end-to-end LLM-driven information retrieval architecture that unifies indexing, retrieval, and reranking in a single LLM. This approach enables the LLM to internalize the corpus, generate relevant content, and perform self-assessment within a unified framework. Unlike previous works that incorporate LLMs into individual IR components, SelfRetrieval provides a unified framework for the entire IR procedure, facilitating knowledge sharing and deep collaboration among different components. Experimental results demonstrate that SelfRetrieval achieves strong performance across various retrieval benchmarks and application scenarios. In future work, we plan to extend our method to further enhance the reliability and trustworthiness of LLM generation.\n\n## Limitations",
      "LLMfor IR Recent studies have explored leveraging LLMs to enhance various components of IR systems, including query rewriting, retrieval, and reranking. For query rewriting, LLMs have been employed to generate pseudo-documents for query expansion [46] and to rewrite queries based on conversational context [15]. In the retrieval stage, researchers have explored augmenting data by generating pseudo-queries [6, 17] or relevance labels [25] using LLMs, as well as employing LLMs directly as generative retrievers [42, 5]. Regarding reranking, LLMs have been utilized in two ways: serving as rerankers directly [27, 40] and augmenting the reranking dataset [12]. While these methods have advanced specific components within the IR pipeline, Self-Retrieval distinguishes itself by presenting an end-to-end architecture driven entirely by a single LLM, eliminating the need for external components.\n\nDense retrieval Dense retrieval models retrieve information by matching dense vector representations of queries and documents [19]. In this paradigm, an encoder transforms both queries and documents into dense vectors, with relevance determined by their vector distance. Various strategies have been proposed to enhance dense retrievers, including designing loss functions [45], multivector [38], training with synthetic queries [33, 47], and leveraging large-scale query-document pairs [30, 50]. Recent work has also explored using large language models to generate dense vectors for both queries and documents [29]. However, the fundamental limitation of dense retrieval lies in its limited interaction with LLMs, as the compression of natural language into dense vectors inherently constrains the utilization of LLMs' sophisticated language understanding and semantic inference capabilities.",
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 2795-2806, Online, August 2021. Association for Computational Linguistics. |\n| [25]   | Guangyuan Ma, Xing Wu, Peng Wang, Zijia Lin, and Songlin Hu. Pre-training with large lan- guage model-based document expansion for dense passage retrieval. ArXiv , abs/2308.08285, 2023. |\n| [26]   | Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. ArXiv , abs/2310.08319, 2023. |\n| [27]   | Xueguang Ma, Xinyu Crystina Zhang, Ronak Pradeep, and Jimmy J. Lin. Zero-shot listwise document reranking with a large language model. ArXiv , abs/2305.02156, 2023. |\n| [28]   | Antonio Mallia, O. Khattab, Nicola Tonellotto, and Torsten Suel. Learning passage impacts for inverted indexes. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval , 2021. |\n| [29]   | Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning, 2024. |\n| [30]   | Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005 , 2022. |\n| [31]   | Jianmo Ni, Gustavo Hernández Abrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Matthew Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. ArXiv , abs/2108.08877, 2021. |\n| [32]   | Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers."
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence discusses computational efficiency but does not directly compare reranking costs to a separate cross-attention reranker.",
      "confidence": 0.75
    }
  },
  {
    "claim_id": 33,
    "claim": "The ablation in Table 3 seems to show that the proposed method's retrieval-alone performance is stronger than most retrieval baselines, but the paper can be more convincing if having e2e comparison to other 2-stage retrieval pipelines like BGE + BGE reranker or GTR + RankT5.",
    "evidence": [
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "Table 4: Ablation study on NQ and TriviaQA.\n\n| Method          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| Self-Retrieval (base) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| w/o indexing          | 53.05 | 67.16 | 58.95 | 54.45      | 70.64      | 60.98      |\n| w/o title          | 47.81 | 60.90 | 52.81 | 52.32      | 67.91      | 58.48      |\n| w/o self-assessment   | 54.80 | 75.21 | 62.77 | 46.67      | 70.79      | 55.92      |\n\nperformance, with each ablation resulting in substantial performance degradation. Specifically, removing the indexing mechanism restricts the model to internalizing only the documents encountered during training, leading to poor performance on unseen passages. Without titles, we directly generate passages with constrained decoding. The absence of document titles significantly degrades performance, as titles provide critical global information that guides the LLM in generating relevant content. Furthermore, removing the self-assessment mechanism leads to a significant decrease in both datasets. Without self-assessment, the model cannot effectively evaluate and refine its initial retrieved passages, leading to less accurate document rankings. This degradation directly impacts downstream applications such as RAG, where precise passage ranking is crucial for generating highquality responses. These ablation results show that each component of Self-Retrieval addresses a specific challenge in the retrieval process, contributing to its overall effectiveness.\n\n## 4.3 Performance on Retrieval-Augmented Generation\n\nTable 5: The performance on retrieval-augmented generation. For baseline, we use BGE-FT as the retriever and a fine-tuned LLM as reader. Results are reported using Exact Match (EM) scores.",
      "Baselines We evaluate Self-Retrieval models for both passage retrieval and document retrieval, comparing them with sparse, dense, and generative retrieval baselines. The sparse retrieval baselines are:BM25 [37] and DocT5Query [28]. The dense retrieval baselines include: DPR [19], Sentence-T5 [31], GTR [32], BGE [50], text-embedding-ada-002 [30], GritLM [29], and their finetuned variants, DPR-FT and BGE-FT. The generative retrieval baselines comprise: DSI [42], DSIQG [59], NCI [49], Ultron [55], DynamicRetriever [54], GenRet [39], and SEAL [5]. Additionally, to ensure a comprehensive comparison, we also evaluate combinations of strong retrieval baselines with various rerankers, including BGE-Reranker, BGE-Reranker-FT, and RankGPT [41]. In the passage retrieval task, we use the official pre-trained models for all non-fine-tuned dense retrieval baselines. For fine-tuned dense models and generative models, we use their official implementations to replicate the experiments on our dataset. In the document retrieval task, we report the baseline\n\nperformances from their original paper. For comprehensive details about these baselines, please refer to Appendix B.\n\n## 4.2 Main Results\n\nPassage retrieval In Table 1, we compare the performance of Self-Retrieval with various baselines on the NQ and TriviaQA datasets. Self-Retrieval 3B outperforms both strong pre-trained dense retrieval models, such as BGE and GritLM 7B, and other generative retrieval methods. Specifically, Self-Retrieval 3B achieves improvements of 5.46 and 5.07 in MRR@5 over the fine-tuned BGE on NQ and TriviaQA datasets, respectively.",
      "Analysis on reranking In this part, we conduct an in-depth analysis of the reranking performance of Self-Retrieval reranker module in comparison with the fine-tuned BGE-Reranker. We employ DPR-FT, SEAL and GritLM to retrieve 100 passages on TriviaQA, followed by reranking the retrieved results using both approaches. We evaluate performance using MRR@5 as the metric. The experimental results are presented in . The results reveal two key findings: (1) Reranking plays a crucial role in information retrieval systems, significantly enhancing the ranking performance across all models. (2) The Self-Retrieval reranker consistently outperforms the fine-tuned BGE Reranker in most scenarios, demonstrating its robustness and effectiveness. These findings demonstrate that Self-Retrieval performs effectively both as a complete IR system and as a reranker component.\n\nIn Appendix C, we conduct additional experiments with a chunk size of 100 words, demonstrating Self-Retrieval's adaptability to different text segmentation strategies. In Appendix E, we further discuss Self-Retrieval's computational efficiency.\n\n## 5 Conclusion\n\nIn this paper, we propose Self-Retrieval, an end-to-end LLM-driven information retrieval architecture that unifies indexing, retrieval, and reranking in a single LLM. This approach enables the LLM to internalize the corpus, generate relevant content, and perform self-assessment within a unified framework. Unlike previous works that incorporate LLMs into individual IR components, SelfRetrieval provides a unified framework for the entire IR procedure, facilitating knowledge sharing and deep collaboration among different components. Experimental results demonstrate that SelfRetrieval achieves strong performance across various retrieval benchmarks and application scenarios. In future work, we plan to extend our method to further enhance the reliability and trustworthiness of LLM generation.\n\n## Limitations"
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence supports retrieval performance but lacks explicit mention of e2e comparisons to specific 2-stage pipelines.",
      "confidence": 0.85
    }
  },
  {
    "claim_id": 34,
    "claim": "- Lacking efficiency discussion.",
    "evidence": [
      "|          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| BGE-FT          | 53.42 | 80.15 | 63.99 | 52.70      | 75.22      | 61.65      |\n| BGE-FT + BGE-Reranker     | 21.91 | 54.58 | 33.33 | 45.36      | 72.16      | 55.78      |\n| BGE-FT + BGE-Reranker-FT  | 52.15 | 76.15 | 61.37 | 44.87      | 67.39      | 53.39      |\n| BGE-FT + RankGPT          | 44.21 | 73.68 | 55.51 | 48.00      | 72.00      | 57.33      |\n| GTR-XL          | 37.64 | 66.84 | 48.94 | 35.97      | 63.75      | 46.67      |\n| GTR-XL + BGE-Reranker     | 26.39 | 59.96 | 38.50 | 42.41      | 68.42      | 52.51      |\n| GTR-XL + BGE-Reranker-FT  | 57.50 | 78.92 | 66.06 | 58.56      | 77.65      | 66.22      |\n| GTR-XL + RankGPT          | 42.11 | 68.42 | 52.30 | 47.00      | 66.00      | 54.95      |\n| GritLM          | 44.67 | 76.00 | 57.03 | 39.91      | 69.34      | 51.14      |\n| GritLM + BGE-Reranker     | 30.06 | 65.87 | 43.20 | 43.64      | 70.87      | 54.23      |\n| GritLM + BGE-Reranker-FT  | 57.57 | 81.35 | 66.98 | 58.60      | 80.54      | 67.21      |\n| GritLM + RankGPT          | 37.89 | 70.53 | 51.19 | 44.00      | 66.00      | 52.70      |\n| DSI-XL          | 43.03 | 60.26 | 49.47 | 29.64      | 46.74      | 36.12      |\n| DSI-XL + BGE-Reranker     | 34.39 | 64.26 | 45.74 | 37.85      | 52.57      | 43.49      |\n| DSI-XL + BGE-Reranker-FT  | 50.02 | 68.60 | 57.43 | 36.49      | 52.40      | 42.36      |\n| DSI-XL + RankGPT          | 49.47 | 73.68 | 59.25 | 39.00      | 52.00      | 44.75      |\n| Self-Retrieval (StableLM) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| Self-Retrieval (Llama 2)  | 63.44 | 79.29 | 70.00 | 59.94      | 81.06      | 68.74      |",
      "We comprehensively evaluate Self-Retrieval against various two-stage retriever-reranker pipelines. Specifically, we construct these pipelines using state-of-the-art retrievers (BGE, GTR, GritLM, and DSI-XL) combined with three different reranking approaches: BGE reranker, fine-tuned BGE reranker, and RankGPT. As shown in Table 8, Self-Retrieval achieves superior performance compared to most retriever-reranker combinations, demonstrating the effectiveness of our end-to-end approach over traditional pipeline methods.\n\n## E Efficiency Analysis\n\nWe conduct efficiency analysis on NQ dataset using an NVIDIA A100-80G GPU. Results in Table 9 illustrate that, while Self-Retrieval requires slightly higher computational resources than DSI, it provides notable performance benefits. Notably, Self-Retrieval with a beam size of 10 achieves significantly higher H@5 scores compared to DSI-XL with a beam size of 100, enabling a flexible trade-off between retrieval quality and computational efficiency. When compared to SEAL, which also employs natural language decoding, Self-Retrieval demonstrates more efficient memory usage (30MB vs 444MB) by utilizing a lightweight trie structure instead of SEAL's resource-intensive FM-Index post-processing mechanism. Furthermore, the efficiency of Self-Retrieval stands to benefit from ongoing developments in optimization techniques (e.g., quantization and attention acceleration) and hardware advancements.\n\n| Model Name     | Memory   | Beam Size   | Latency (s)   | Hits@5      |\n|----------|----------|----------|----------|----------|\n| SEAL          | 444MB    | 10 100      | 1.18 5.92     | 61.91 59.57 |\n| DSI-XL         | 0        | 10 100      | 0.23 0.45     | 60.21 60.21 |\n| Self-Retrieval | 30MB     | 10 100      | 1.44 6.06     | 76.17 81.49 |\n\nTable 9: Efficiency analysis.",
      "| [33]   | Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. From doc2query to doctttttquery. Online preprint , 6:2, 2019. |\n|--------|----------|\n| [34]   | Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2523-2544, Online, June 2021. Association for Computational Linguistics. |\n| [35]   | Ronak Pradeep, Kai Hui, Jai Gupta, Adam Lelkes, Honglei Zhuang, Jimmy Lin, Donald Met- zler, and Vinh Tran. How does generative retrieval scale to millions of passages? In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empir- ical Methods in Natural Language Processing , pages 1305-1321, Singapore, December 2023. Association for Computational Linguistics. |\n| [36]   | Shanbao Qiao, Xuebing Liu, and Seung-Hoon Na. Diffusionret: Diffusion-enhanced gener- ative retriever using constrained decoding. In Conference on Empirical Methods in Natural Language Processing , 2023. |\n| [37]   | Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval , 3(4):333-389, 2009. |\n| [38]   | Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. ColBERTv2: Effective and efficient retrieval via lightweight late interaction. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 3715-3734, Seattle, United States, July 2022. Association for Computational Linguistics.",
      "|          |        | NQ      | NQ    | NQ      | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|--------|---------|-------|---------|----------|----------|----------|\n| Model          | Params | H@1     | H@5   | M@5     | H@1        | H@5        | M@5        |\n| Sparse Retrieval BM25 [37]       | -      | 14.54   | 32.71 | 21.13   | 20.09      | 42.73      | 28.35      |\n| Dense Retrieval DPR [19]         | 110M   | 40.41   | 61.79 | 48.80   | 35.57      | 57.39      | 43.93      |\n| DPR-FT [19]          | 110M   | 42.21   | 60.45 | 49.33   | 36.58      | 53.05      | 42.91      |\n| BGE [50]          | 335M   | 36.30   | 66.95 | 48.05   | 46.97      | 70.14      | 55.95      |\n| BGE-FT [50]          | 335M   | 53.42   | 80.15 | 63.99   | 52.70      | 75.22      | 61.65      |\n| BGE-FT + BGE-Reranker-FT         | 770M   | 52.15   | 76.15 | 61.37   | 44.87      | 67.39      | 53.39      |\n| GTR-XL [32]          | 1.24B  | 37.64   | 66.84 | 48.94   | 35.97      | 63.75      | 46.67      |\n| GTR-XL + BGE-Reranker-FT         | 1.57B  | 57.50   | 78.92 | 66.06   | 58.56      | 77.65      | 66.22      |\n| GTR-XXL [32]          | 4.86B  | 39.21   | 69.72 | 50.88   | 35.97      | 64.15      | 46.83      |\n| text-embedding-ada-002          | -      | 34.28   | 62.28 | 44.64   | 35.09      | 62.00      | 45.15      |\n| GritLM [29]          | 7.24B  | 44.67   | 76.00 | 57.03   | 39.91      | 69.34      | 51.14      |\n| GritLM + BGE-Reranker-FT         | 7.57B  | 57.57   | 81.35 | 66.98   | 58.60      | 80.54      | 67.21      |\n| Generative retrieval DSI-XL [42] | 2.85B  | 43.03   | 60.26 | 49.47   | 29.64      | 46.74      | 36.12      |\n| DSI-XXL [42]          | 11.3B  | 43.81   | 60.45 | 50.20   | 30.55      | 46.67      | 36.56      |\n| SEAL [5]          | 406M   | 36.79   | 61.35 | 45.88   | 36.88      | 61.66      | 46.29      |\n| DSI-QG [59]          | 2.85B  | 34.88   | 56.60 | 43.33   | 29.15      | 45.53      | 35.20      |\n| NCI + BGE-Reranker-FT          | 1.07B  | 50.86   | 70.27 | 58.53   | 28.42      | 42.18      | 33.62      |\n| Self-Retrieval (StableLM)        | 2.8B   | 62.16 ∗ | 79.28 | 69.45 ∗ | 58.69 ∗    | 78.39 ∗    | 66.72 ∗    |\n| Self-Retrieval (Llama 2)         | 6.74B  | 63.44 ∗ | 79.29 | 70.00 ∗ | 59.94 ∗    | 81.06 ∗    | 68.74 ∗    |"
    ],
    "verification": {
      "result": "Undetermined",
      "justification": "The evidence does not mention efficiency discussion in the claim.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 35,
    "claim": "Limitations: NA\n\nSoundness: 3\n\nPresentation: 4\n\nContribution: 3\n\nEthics Review Flagged: ['No ethics review needed.']",
    "evidence": [
      "Table 4: Ablation study on NQ and TriviaQA.\n\n| Method          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| Self-Retrieval (base) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| w/o indexing          | 53.05 | 67.16 | 58.95 | 54.45      | 70.64      | 60.98      |\n| w/o title          | 47.81 | 60.90 | 52.81 | 52.32      | 67.91      | 58.48      |\n| w/o self-assessment   | 54.80 | 75.21 | 62.77 | 46.67      | 70.79      | 55.92      |\n\nperformance, with each ablation resulting in substantial performance degradation. Specifically, removing the indexing mechanism restricts the model to internalizing only the documents encountered during training, leading to poor performance on unseen passages. Without titles, we directly generate passages with constrained decoding. The absence of document titles significantly degrades performance, as titles provide critical global information that guides the LLM in generating relevant content. Furthermore, removing the self-assessment mechanism leads to a significant decrease in both datasets. Without self-assessment, the model cannot effectively evaluate and refine its initial retrieved passages, leading to less accurate document rankings. This degradation directly impacts downstream applications such as RAG, where precise passage ranking is crucial for generating highquality responses. These ablation results show that each component of Self-Retrieval addresses a specific challenge in the retrieval process, contributing to its overall effectiveness.\n\n## 4.3 Performance on Retrieval-Augmented Generation\n\nTable 5: The performance on retrieval-augmented generation. For baseline, we use BGE-FT as the retriever and a fine-tuned LLM as reader. Results are reported using Exact Match (EM) scores.",
      "from Wikipedia for each dataset. Each document is segmented into passages of maximum 200 words, yielding approximately 1 million passages in total. The detailed statistics of the datasets are presented in Appendix A. We use passage-level Hits@{1, 5} and Mean Reciprocal Rank (MRR)@5 as evaluation metrics.\n\nTo comprehensively compare with other generative information retrieval methods, we also conduct experiments on document retrieval. Following NCI [49], we conduct experiments on NQ320K and utilize Recall@{1, 10} and MRR@100 as the evaluation metrics. To evaluate the model's robustness in non-Wikipedia scenarios where high-quality text and titles are not available, we conduct experiments on a subset of MS MARCO [3] following the experimental setup of Ultron [55]. The performance was measured using Recall@{1,5} and MRR@10.\n\nImplementation details In this study, we employ StableLM-3B [44] and Llama2-7B [43] as passage retrieval backbones. For document retrieval, we employ StableLM-1.6B [4] for NQ320K and StableLM-3B for MS MARCO. We train the models using ZeRO stage-2 optimization on 8 NVIDIA A100 (80 GB) GPUs with the AdamW optimizer, a batch size of 16 per GPU, and BFloat16 precision. The models are trained for 3 epochs with a learning rate of 2e-5. During inference, we use beam search to generate 5 titles and 10 passages for each title, with hyperparameters τ and δ set to 0.4 across all models and datasets.",
      "- [1] Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, Shengling Gao, J. Guo, Xiangnan He, Yanyan Lan, Chenliang Li, Yiqun Liu, Ziyu Lyu, Weizhi Ma, Jun Ma, Zhaochun Ren, Pengjie Ren, Zhiqiang Wang, Min Wang, Jirong Wen, Lei Wu, Xin Xin, Jun Xu, Dawei Yin, Peng Zhang, Fan Zhang, Wei na Zhang, M. Zhang, and Xiaofei Zhu. Information retrieval meets large language models: A strategic report from chinese ir community. ArXiv , abs/2307.09751, 2023.\n- [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023.\n\n[3] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 , 2016.\n\n[4] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, Meng Lee,",
      "Baselines We evaluate Self-Retrieval models for both passage retrieval and document retrieval, comparing them with sparse, dense, and generative retrieval baselines. The sparse retrieval baselines are:BM25 [37] and DocT5Query [28]. The dense retrieval baselines include: DPR [19], Sentence-T5 [31], GTR [32], BGE [50], text-embedding-ada-002 [30], GritLM [29], and their finetuned variants, DPR-FT and BGE-FT. The generative retrieval baselines comprise: DSI [42], DSIQG [59], NCI [49], Ultron [55], DynamicRetriever [54], GenRet [39], and SEAL [5]. Additionally, to ensure a comprehensive comparison, we also evaluate combinations of strong retrieval baselines with various rerankers, including BGE-Reranker, BGE-Reranker-FT, and RankGPT [41]. In the passage retrieval task, we use the official pre-trained models for all non-fine-tuned dense retrieval baselines. For fine-tuned dense models and generative models, we use their official implementations to replicate the experiments on our dataset. In the document retrieval task, we report the baseline\n\nperformances from their original paper. For comprehensive details about these baselines, please refer to Appendix B.\n\n## 4.2 Main Results\n\nPassage retrieval In Table 1, we compare the performance of Self-Retrieval with various baselines on the NQ and TriviaQA datasets. Self-Retrieval 3B outperforms both strong pre-trained dense retrieval models, such as BGE and GritLM 7B, and other generative retrieval methods. Specifically, Self-Retrieval 3B achieves improvements of 5.46 and 5.07 in MRR@5 over the fine-tuned BGE on NQ and TriviaQA datasets, respectively."
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence shows ablation results but does not directly support the specific scores or metrics mentioned in the claim.",
      "confidence": 0.75
    }
  },
  {
    "claim_id": 36,
    "claim": "Summary: The paper proposes an approach of self-retrieval, which uses the probability of generation of the passage as the ranking criterion.",
    "evidence": [
      "In this section, we introduce our proposed Self-Retrieval. The overall architecture is illustrated in . Different from traditional information retrieval systems that separate indexing, retrieval, and reranking components, Self-Retrieval integrates these functionalities directly into the parameters of a single large language model:\n\n- Indexing : Self-Retrieval internalizes the entire corpus into its parameters through selfsupervised learning, enabling the model to process passages internally without relying on external indices.\n- Retrieval : Given an input query q , Self-Retrieval generates relevant passage p using the knowledge embedded within its parameters, which is different from dense retrieval or generative retrieval that rely on embedding or document identifiers as proxies of passage.\n\n- Reranking : After generating passage p , Self-Retrieval assesses its relevance to the query q through self-assessment. The output logits provide the basis for reranking candidate passages.\n\nThrough this unified approach, Self-Retrieval enables a streamlined, end-to-end process that enhances the overall effectiveness of information retrieval. In the following sections, we detail each component of our method.\n\n## 3.1 Indexing: Internalize the Corpus",
      "Training Self-Retrieval unifies the three distinct tasks of information retrieval - indexing, retrieval, and reranking - into text generation tasks, trained using cross-entropy loss in an auto-regressive manner. Specifically, Self-Retrieval first internalizes the corpus into its parameters through selfsupervised learning as introduced in Section 3.1. Subsequently, in addition to a portion of selfsupervised instances, it incorporates two different types of data to build retrieval and reranking abilities:\n\n- Retrieval data: Utilizes supervised query-passage pairs from the dataset, where the model learns to generate both document titles and passage content in response to input queries.\n- Reranking data: Employs positive and negative examples to train the model in relevance assessment between queries and passages.\n\nThis auto-regressive training approach enables Self-Retrieval to integrate traditionally separate IR components into a unified language model, establishing an end-to-end IR system.\n\nFurthermore, leveraging the universal language generation capabilities of LLMs, we can seamlessly integrate downstream task components, such as readers in RAG, into Self-Retrieval. This integration can be achieved by simply appending the golden answer after the assessment in Self-Retrieval. Consequently, the LLM can function as a comprehensive RAG system, effectively reducing the knowledge gap between IR system and reader modules.\n\nInference During inference, given an input query, Self-Retrieval aims to obtain the relevant passages that are sorted based on the relevance to query. Firstly, the model generates i document titles through constrained beam search. Secondly, for each title, it generates j passages using beam search. Finally, the resulting i × j passages are scored using the self-assessment mechanism and reranked to produce the final output.\n\n## 4 Experimental Results\n\n## 4.1 Experimental Setup",
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "Analysis on reranking In this part, we conduct an in-depth analysis of the reranking performance of Self-Retrieval reranker module in comparison with the fine-tuned BGE-Reranker. We employ DPR-FT, SEAL and GritLM to retrieve 100 passages on TriviaQA, followed by reranking the retrieved results using both approaches. We evaluate performance using MRR@5 as the metric. The experimental results are presented in . The results reveal two key findings: (1) Reranking plays a crucial role in information retrieval systems, significantly enhancing the ranking performance across all models. (2) The Self-Retrieval reranker consistently outperforms the fine-tuned BGE Reranker in most scenarios, demonstrating its robustness and effectiveness. These findings demonstrate that Self-Retrieval performs effectively both as a complete IR system and as a reranker component.\n\nIn Appendix C, we conduct additional experiments with a chunk size of 100 words, demonstrating Self-Retrieval's adaptability to different text segmentation strategies. In Appendix E, we further discuss Self-Retrieval's computational efficiency.\n\n## 5 Conclusion\n\nIn this paper, we propose Self-Retrieval, an end-to-end LLM-driven information retrieval architecture that unifies indexing, retrieval, and reranking in a single LLM. This approach enables the LLM to internalize the corpus, generate relevant content, and perform self-assessment within a unified framework. Unlike previous works that incorporate LLMs into individual IR components, SelfRetrieval provides a unified framework for the entire IR procedure, facilitating knowledge sharing and deep collaboration among different components. Experimental results demonstrate that SelfRetrieval achieves strong performance across various retrieval benchmarks and application scenarios. In future work, we plan to extend our method to further enhance the reliability and trustworthiness of LLM generation.\n\n## Limitations"
    ],
    "verification": {
      "result": "Supported",
      "justification": "The evidence directly supports the claim about Self-Retrieval using generation probability for ranking.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 37,
    "claim": "The proposed method outperforms the others.",
    "evidence": [
      "Table 4: Ablation study on NQ and TriviaQA.\n\n| Method          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| Self-Retrieval (base) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| w/o indexing          | 53.05 | 67.16 | 58.95 | 54.45      | 70.64      | 60.98      |\n| w/o title          | 47.81 | 60.90 | 52.81 | 52.32      | 67.91      | 58.48      |\n| w/o self-assessment   | 54.80 | 75.21 | 62.77 | 46.67      | 70.79      | 55.92      |\n\nperformance, with each ablation resulting in substantial performance degradation. Specifically, removing the indexing mechanism restricts the model to internalizing only the documents encountered during training, leading to poor performance on unseen passages. Without titles, we directly generate passages with constrained decoding. The absence of document titles significantly degrades performance, as titles provide critical global information that guides the LLM in generating relevant content. Furthermore, removing the self-assessment mechanism leads to a significant decrease in both datasets. Without self-assessment, the model cannot effectively evaluate and refine its initial retrieved passages, leading to less accurate document rankings. This degradation directly impacts downstream applications such as RAG, where precise passage ranking is crucial for generating highquality responses. These ablation results show that each component of Self-Retrieval addresses a specific challenge in the retrieval process, contributing to its overall effectiveness.\n\n## 4.3 Performance on Retrieval-Augmented Generation\n\nTable 5: The performance on retrieval-augmented generation. For baseline, we use BGE-FT as the retriever and a fine-tuned LLM as reader. Results are reported using Exact Match (EM) scores.",
      "In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 9844-9855, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguis-          |",
      "|          |        | NQ      | NQ    | NQ      | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|--------|---------|-------|---------|----------|----------|----------|\n| Model          | Params | H@1     | H@5   | M@5     | H@1        | H@5        | M@5        |\n| Sparse Retrieval BM25 [37]       | -      | 14.54   | 32.71 | 21.13   | 20.09      | 42.73      | 28.35      |\n| Dense Retrieval DPR [19]         | 110M   | 40.41   | 61.79 | 48.80   | 35.57      | 57.39      | 43.93      |\n| DPR-FT [19]          | 110M   | 42.21   | 60.45 | 49.33   | 36.58      | 53.05      | 42.91      |\n| BGE [50]          | 335M   | 36.30   | 66.95 | 48.05   | 46.97      | 70.14      | 55.95      |\n| BGE-FT [50]          | 335M   | 53.42   | 80.15 | 63.99   | 52.70      | 75.22      | 61.65      |\n| BGE-FT + BGE-Reranker-FT         | 770M   | 52.15   | 76.15 | 61.37   | 44.87      | 67.39      | 53.39      |\n| GTR-XL [32]          | 1.24B  | 37.64   | 66.84 | 48.94   | 35.97      | 63.75      | 46.67      |\n| GTR-XL + BGE-Reranker-FT         | 1.57B  | 57.50   | 78.92 | 66.06   | 58.56      | 77.65      | 66.22      |\n| GTR-XXL [32]          | 4.86B  | 39.21   | 69.72 | 50.88   | 35.97      | 64.15      | 46.83      |\n| text-embedding-ada-002          | -      | 34.28   | 62.28 | 44.64   | 35.09      | 62.00      | 45.15      |\n| GritLM [29]          | 7.24B  | 44.67   | 76.00 | 57.03   | 39.91      | 69.34      | 51.14      |\n| GritLM + BGE-Reranker-FT         | 7.57B  | 57.57   | 81.35 | 66.98   | 58.60      | 80.54      | 67.21      |\n| Generative retrieval DSI-XL [42] | 2.85B  | 43.03   | 60.26 | 49.47   | 29.64      | 46.74      | 36.12      |\n| DSI-XXL [42]          | 11.3B  | 43.81   | 60.45 | 50.20   | 30.55      | 46.67      | 36.56      |\n| SEAL [5]          | 406M   | 36.79   | 61.35 | 45.88   | 36.88      | 61.66      | 46.29      |\n| DSI-QG [59]          | 2.85B  | 34.88   | 56.60 | 43.33   | 29.15      | 45.53      | 35.20      |\n| NCI + BGE-Reranker-FT          | 1.07B  | 50.86   | 70.27 | 58.53   | 28.42      | 42.18      | 33.62      |\n| Self-Retrieval (StableLM)        | 2.8B   | 62.16 ∗ | 79.28 | 69.45 ∗ | 58.69 ∗    | 78.39 ∗    | 66.72 ∗    |\n| Self-Retrieval (Llama 2)         | 6.74B  | 63.44 ∗ | 79.29 | 70.00 ∗ | 59.94 ∗    | 81.06 ∗    | 68.74 ∗    |",
      "| Method          |   R@1 | R@5   |   M@10 |\n|----------|-------|-------|--------|\n| Sparse Retrieval BM25 [37]          |  18.9 | 42.8  |   29.2 |\n| DocT5Query [28]          |  23.3 | 49.4  |   34.8 |\n| Dense Retrieval DPR [19]          |  29.1 | 62.8  |   43.4 |\n| Sentence-T5 [31]          |  27.3 | 58.9  |   40.7 |\n| Generative Retrieval DSI-Atomic [42] |  32.5 | 63.0  |   44.3 |\n| DynamicRetriever [54]          |  29   | 64.2  |   42.5 |\n| Ultron-URL [55]          |  29.6 | 56.4  |   40   |\n| Ultron-PQ [55]          |  31.6 | 64.0  |   45.3 |\n| Ultron-Atomic [55]          |  32.8 | 64.9  |   46.9 |\n| GenRet [39]          |  47.9 | -     |   58.1 |\n| Self-Retrieval          |  47.8 | 69.9  |   57.2 |\n\nTable 2: The experimental result of document retrieval on NQ320K.\n\nTable 3: The experimental result of document retrieval on MS MARCO.\n\nDocument retrieval We present the document retrieval results on NQ320K dataset in Table 2. Self-Retrieval outperforms all other generative retrieval methods and dense retrieval baselines across all three metrics. Compared to GenRet, the previously strongest generative retrieval method, SelfRetrieval improves Hits@1 by 5.2, Hits@10 by 3.8, and MRR@100 by 4.8 points. Notably, while other methods commonly employ query generation to augment their training data, Self-Retrieval achieves these results using only the original training set.\n\nTo evaluate the effectiveness of Self-Retrieval in non-Wikipedia scenarios, we extend our experiments to MS MARCO. To address the absence of document titles in MS MARCO, we employ Llama2 to automatically generate titles. As shown in Table 3, Self-Retrieval achieves comparable performance to the SOTA model GenRet, while significantly outperforming other baselines. These results demonstrate its adaptability and robustness in non-Wikipedia and title-lacking contexts.\n\nAblation study To study the effect of each component, we conduct ablation study on both NQ and TriviaQA. Results are presented in Table 4. All components prove crucial for Self-Retrieval's"
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence shows Self-Retrieval performs well but does not directly compare all methods in the claim.",
      "confidence": 0.85
    }
  },
  {
    "claim_id": 38,
    "claim": "The proposed method thus has some novelty compared to the literature.",
    "evidence": [
      "| Method          |   R@1 | R@5   |   M@10 |\n|----------|-------|-------|--------|\n| Sparse Retrieval BM25 [37]          |  18.9 | 42.8  |   29.2 |\n| DocT5Query [28]          |  23.3 | 49.4  |   34.8 |\n| Dense Retrieval DPR [19]          |  29.1 | 62.8  |   43.4 |\n| Sentence-T5 [31]          |  27.3 | 58.9  |   40.7 |\n| Generative Retrieval DSI-Atomic [42] |  32.5 | 63.0  |   44.3 |\n| DynamicRetriever [54]          |  29   | 64.2  |   42.5 |\n| Ultron-URL [55]          |  29.6 | 56.4  |   40   |\n| Ultron-PQ [55]          |  31.6 | 64.0  |   45.3 |\n| Ultron-Atomic [55]          |  32.8 | 64.9  |   46.9 |\n| GenRet [39]          |  47.9 | -     |   58.1 |\n| Self-Retrieval          |  47.8 | 69.9  |   57.2 |\n\nTable 2: The experimental result of document retrieval on NQ320K.\n\nTable 3: The experimental result of document retrieval on MS MARCO.\n\nDocument retrieval We present the document retrieval results on NQ320K dataset in Table 2. Self-Retrieval outperforms all other generative retrieval methods and dense retrieval baselines across all three metrics. Compared to GenRet, the previously strongest generative retrieval method, SelfRetrieval improves Hits@1 by 5.2, Hits@10 by 3.8, and MRR@100 by 4.8 points. Notably, while other methods commonly employ query generation to augment their training data, Self-Retrieval achieves these results using only the original training set.\n\nTo evaluate the effectiveness of Self-Retrieval in non-Wikipedia scenarios, we extend our experiments to MS MARCO. To address the absence of document titles in MS MARCO, we employ Llama2 to automatically generate titles. As shown in Table 3, Self-Retrieval achieves comparable performance to the SOTA model GenRet, while significantly outperforming other baselines. These results demonstrate its adaptability and robustness in non-Wikipedia and title-lacking contexts.\n\nAblation study To study the effect of each component, we conduct ablation study on both NQ and TriviaQA. Results are presented in Table 4. All components prove crucial for Self-Retrieval's",
      "| [19]   | Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6769- 6781, Online, November 2020. Association for Computational Linguistics. |\n|--------|----------|\n| [20]   | Varsha Kishore, Chao gang Wan, Justin Lovelace, Yoav Artzi, and Kilian Q. Weinberger. In- cdsi: Incrementally updatable document retrieval. ArXiv , abs/2307.10323, 2023. |\n| [21]   | Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transac- tions of the Association of Computational Linguistics , 2019. |\n| [22]   | Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neu- ral Information Processing Systems , volume 33, pages 9459-9474. Curran Associates, Inc., 2020. |\n| [23]   | Xiaoxi Li, Yujia Zhou, and Zhicheng Dou. Unigen: A unified generative framework for re- trieval and question answering with large language models. ArXiv , abs/2312.11036, 2023. |\n| [24]   | Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong Tang, Annan Li, Le Sun, Meng Liao, and Shaoyi Chen. Text2Event: Controllable sequence-to-structure generation for end-to-end event extraction.",
      "Table 4: Ablation study on NQ and TriviaQA.\n\n| Method          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| Self-Retrieval (base) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| w/o indexing          | 53.05 | 67.16 | 58.95 | 54.45      | 70.64      | 60.98      |\n| w/o title          | 47.81 | 60.90 | 52.81 | 52.32      | 67.91      | 58.48      |\n| w/o self-assessment   | 54.80 | 75.21 | 62.77 | 46.67      | 70.79      | 55.92      |\n\nperformance, with each ablation resulting in substantial performance degradation. Specifically, removing the indexing mechanism restricts the model to internalizing only the documents encountered during training, leading to poor performance on unseen passages. Without titles, we directly generate passages with constrained decoding. The absence of document titles significantly degrades performance, as titles provide critical global information that guides the LLM in generating relevant content. Furthermore, removing the self-assessment mechanism leads to a significant decrease in both datasets. Without self-assessment, the model cannot effectively evaluate and refine its initial retrieved passages, leading to less accurate document rankings. This degradation directly impacts downstream applications such as RAG, where precise passage ranking is crucial for generating highquality responses. These ablation results show that each component of Self-Retrieval addresses a specific challenge in the retrieval process, contributing to its overall effectiveness.\n\n## 4.3 Performance on Retrieval-Augmented Generation\n\nTable 5: The performance on retrieval-augmented generation. For baseline, we use BGE-FT as the retriever and a fine-tuned LLM as reader. Results are reported using Exact Match (EM) scores.",
      "While our experiments demonstrate the effectiveness of Self-Retrieval, several limitations need to be addressed in future work. Our current evaluation is limited to 200K Wikipedia documents and 3M passages, and testing on larger and noisier text collections is needed. As an LLM-driven system, Self-Retrieval has lower retrieval efficiency compared to sparse or dense retrieval methods, which may limit its applications to specialized knowledge systems. Furthermore, enabling incremental learning and dynamic corpus expansion remains an important direction for future research.\n\n## Acknowledge\n\nWe sincerely thank the reviewers for their insightful comments and valuable suggestions. We are grateful to Le Yu and Xinyu Lu for their helpful feedback on the paper writing. This work was supported by the Natural Science Foundation of China (No. 62122077, 62272439), Beijing Municipal Science and Technology Project (Nos. Z231100010323002), the Basic Research Program of ISCAS (ISCAS-JCZD-202303), and CAS Project for Young Scientists in Basic Research (Grant No.YSBR-040).\n\n## References"
    ],
    "verification": {
      "result": "Supported",
      "justification": "The claim is supported by evidence showing Self-Retrieval outperforms existing methods, indicating novelty.",
      "confidence": 0.95
    }
  },
  {
    "claim_id": 39,
    "claim": "The experimental results are very good, showing improved performance on document retrieval and QA.",
    "evidence": [
      "Datasets and metrics We conduct main experiments on Natural Questions (NQ) [21] and TriviaQA [18] datasets, both of which are widely used retrieval benchmarks based on Wikipedia. We use their versions from the KILT benchmark [34], which consolidates these datasets into a single pre-processed Wikipedia dump, facilitating easier evaluation. Since the KILT test set is not publicly accessible, we use the development set for testing and randomly sample 2,000 instances from the training set as our development set. For our experiments, we sample approximately 40K documents\n\nTable 1: The experimental results of passage retrieval on NQ and TriviaQA test set. * indicates statistically significant improvements (p &lt; 0.01) over state-of-the-art retrieval baselines.",
      "from Wikipedia for each dataset. Each document is segmented into passages of maximum 200 words, yielding approximately 1 million passages in total. The detailed statistics of the datasets are presented in Appendix A. We use passage-level Hits@{1, 5} and Mean Reciprocal Rank (MRR)@5 as evaluation metrics.\n\nTo comprehensively compare with other generative information retrieval methods, we also conduct experiments on document retrieval. Following NCI [49], we conduct experiments on NQ320K and utilize Recall@{1, 10} and MRR@100 as the evaluation metrics. To evaluate the model's robustness in non-Wikipedia scenarios where high-quality text and titles are not available, we conduct experiments on a subset of MS MARCO [3] following the experimental setup of Ultron [55]. The performance was measured using Recall@{1,5} and MRR@10.\n\nImplementation details In this study, we employ StableLM-3B [44] and Llama2-7B [43] as passage retrieval backbones. For document retrieval, we employ StableLM-1.6B [4] for NQ320K and StableLM-3B for MS MARCO. We train the models using ZeRO stage-2 optimization on 8 NVIDIA A100 (80 GB) GPUs with the AdamW optimizer, a batch size of 16 per GPU, and BFloat16 precision. The models are trained for 3 epochs with a learning rate of 2e-5. During inference, we use beam search to generate 5 titles and 10 passages for each title, with hyperparameters τ and δ set to 0.4 across all models and datasets.",
      "While our experiments demonstrate the effectiveness of Self-Retrieval, several limitations need to be addressed in future work. Our current evaluation is limited to 200K Wikipedia documents and 3M passages, and testing on larger and noisier text collections is needed. As an LLM-driven system, Self-Retrieval has lower retrieval efficiency compared to sparse or dense retrieval methods, which may limit its applications to specialized knowledge systems. Furthermore, enabling incremental learning and dynamic corpus expansion remains an important direction for future research.\n\n## Acknowledge\n\nWe sincerely thank the reviewers for their insightful comments and valuable suggestions. We are grateful to Le Yu and Xinyu Lu for their helpful feedback on the paper writing. This work was supported by the Natural Science Foundation of China (No. 62122077, 62272439), Beijing Municipal Science and Technology Project (Nos. Z231100010323002), the Basic Research Program of ISCAS (ISCAS-JCZD-202303), and CAS Project for Young Scientists in Basic Research (Grant No.YSBR-040).\n\n## References",
      "In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 2795-2806, Online, August 2021. Association for Computational Linguistics. |\n| [25]   | Guangyuan Ma, Xing Wu, Peng Wang, Zijia Lin, and Songlin Hu. Pre-training with large lan- guage model-based document expansion for dense passage retrieval. ArXiv , abs/2308.08285, 2023. |\n| [26]   | Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. ArXiv , abs/2310.08319, 2023. |\n| [27]   | Xueguang Ma, Xinyu Crystina Zhang, Ronak Pradeep, and Jimmy J. Lin. Zero-shot listwise document reranking with a large language model. ArXiv , abs/2305.02156, 2023. |\n| [28]   | Antonio Mallia, O. Khattab, Nicola Tonellotto, and Torsten Suel. Learning passage impacts for inverted indexes. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval , 2021. |\n| [29]   | Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning, 2024. |\n| [30]   | Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005 , 2022. |\n| [31]   | Jianmo Ni, Gustavo Hernández Abrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Matthew Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. ArXiv , abs/2108.08877, 2021. |\n| [32]   | Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers."
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence mentions improved performance on retrieval metrics but does not explicitly state 'very good' or QA improvements.",
      "confidence": 0.85
    }
  },
  {
    "claim_id": 40,
    "claim": "Weaknesses: A key idea is the use of trie for passage generation.",
    "evidence": [
      "However, since LLMs are general-purpose pre-trained models rather than statistical frequency models, the generated passage ˆ p may not exactly match any passage in D , making it challenging to locate the corresponding passages in the corpus. To address this challenge, we employ a trie-based constrained decoding algorithm [10, 8, 24]. This approach restricts generated tokens to a dynamically constrained vocabulary. We construct a prefix tree T from corpus D , where each path from the root to a leaf node represents a unique passage in the corpus, and each node stores valid tokens for the next generation step. During inference, the vocabulary at each generation step is constrained by the valid continuations in the prefix tree. Due to the relatively short common prefixes among documents, the LLM terminates generation once it has produced sufficient tokens to uniquely identify the current document and concatenates the full document to the context. This results in document title and passage generation processes represented as P ( ˆ t | q ; θ ; T ) and P (ˆ p | q, ˆ t ; θ ; T ) . This mechanism ensures that generated passages align with existing corpus content.\n\n## 3.3 Reranking: Assess the Relevance\n\nReranking serves as a second-pass filter to precisely sort the retrieved passages based on the relevance to the query. We implement a self-assessment mechanism that leverages the Self-Retrieval model itself to evaluate the relevance of generated passages. Specifically, Self-Retrieval assesses the passage relevance by generating responses such as 'can answer the query' for relevant passages and 'cannot answer the query' for irrelevant ones. This self-assessment mechanism allows the model to generate passages and evaluate their relevance within a single inference turn.",
      "Our results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods.\n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components.\n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle.\n\n| Method          |   R@1 |   R@10 |   M@100 |\n|----------|-------|--------|---------|\n| Sparse Retrieval BM25 [37]    |  29.7 |   60.3 |    40.2 |\n| DocT5Query [28]          |  38   |   69.3 |    48.9 |\n| Dense Retrieval DPR [19]      |  50.2 |   77.7 |    59.9 |\n| Sentence-T5 [31]          |  53.6 |   83   |    64.1 |\n| GTR-Base [32]          |  56   |   84.4 |    66.2 |\n| Generative Retrieval DSI [42] |  55.2 |   67.4 |    59.6 |\n| SEAL [5]          |  59.9 |   81.2 |    67.7 |\n| DSI-QG [59]          |  63.1 |   80.7 |    69.5 |\n| NCI [49]          |  66.4 |   85.7 |    73.6 |\n| GenRet [39]          |  68.1 |   88.8 |    75.9 |\n| Self-Retrieval          |  73.3 |   92.6 |    80.7 |",
      "|\n| [13] | Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey, 2024. |\n| [14] | Jiafeng Guo, Changjiang Zhou, Ruqing Zhang, Jiangui Chen, Maarten de Rijke, Yixing Fan, and Xueqi Cheng. Corpusbrain++: A continual generative pre-training framework for knowledge-intensive language tasks. ArXiv , abs/2402.16767, 2024. |\n| [15] | Chao-Wei Huang, Chen-Yu Hsu, Tsung-Yuan Hsu, Chen-An Li, and Yun-Nung (Vivian) Chen. Converser: Few-shot conversational dense retrieval with synthetic data generation. ArXiv , abs/2309.06748, 2023. |\n| [16] | Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research , 24(251):1-43, 2023. |\n| [17] | Vitor Jeronymo, Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto de Alen- car Lotufo, Jakub Zavrel, and Rodrigo Nogueira. Inpars-v2: Large language models as efficient dataset generators for information retrieval. ArXiv , abs/2301.01820, 2023. |\n| [18] | Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1601-1611, Vancouver, Canada, 2017. Association for Computational Linguistics. |",
      "In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 2795-2806, Online, August 2021. Association for Computational Linguistics. |\n| [25]   | Guangyuan Ma, Xing Wu, Peng Wang, Zijia Lin, and Songlin Hu. Pre-training with large lan- guage model-based document expansion for dense passage retrieval. ArXiv , abs/2308.08285, 2023. |\n| [26]   | Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. ArXiv , abs/2310.08319, 2023. |\n| [27]   | Xueguang Ma, Xinyu Crystina Zhang, Ronak Pradeep, and Jimmy J. Lin. Zero-shot listwise document reranking with a large language model. ArXiv , abs/2305.02156, 2023. |\n| [28]   | Antonio Mallia, O. Khattab, Nicola Tonellotto, and Torsten Suel. Learning passage impacts for inverted indexes. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval , 2021. |\n| [29]   | Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning, 2024. |\n| [30]   | Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005 , 2022. |\n| [31]   | Jianmo Ni, Gustavo Hernández Abrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Matthew Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. ArXiv , abs/2108.08877, 2021. |\n| [32]   | Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers."
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence mentions trie usage for constrained decoding but does not directly link it to passage generation weaknesses.",
      "confidence": 0.75
    }
  },
  {
    "claim_id": 41,
    "claim": "More details should be presented.",
    "evidence": [
      "Table 4: Ablation study on NQ and TriviaQA.\n\n| Method          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| Self-Retrieval (base) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| w/o indexing          | 53.05 | 67.16 | 58.95 | 54.45      | 70.64      | 60.98      |\n| w/o title          | 47.81 | 60.90 | 52.81 | 52.32      | 67.91      | 58.48      |\n| w/o self-assessment   | 54.80 | 75.21 | 62.77 | 46.67      | 70.79      | 55.92      |\n\nperformance, with each ablation resulting in substantial performance degradation. Specifically, removing the indexing mechanism restricts the model to internalizing only the documents encountered during training, leading to poor performance on unseen passages. Without titles, we directly generate passages with constrained decoding. The absence of document titles significantly degrades performance, as titles provide critical global information that guides the LLM in generating relevant content. Furthermore, removing the self-assessment mechanism leads to a significant decrease in both datasets. Without self-assessment, the model cannot effectively evaluate and refine its initial retrieved passages, leading to less accurate document rankings. This degradation directly impacts downstream applications such as RAG, where precise passage ranking is crucial for generating highquality responses. These ablation results show that each component of Self-Retrieval addresses a specific challenge in the retrieval process, contributing to its overall effectiveness.\n\n## 4.3 Performance on Retrieval-Augmented Generation\n\nTable 5: The performance on retrieval-augmented generation. For baseline, we use BGE-FT as the retriever and a fine-tuned LLM as reader. Results are reported using Exact Match (EM) scores.",
      "|          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| BGE-FT          | 53.42 | 80.15 | 63.99 | 52.70      | 75.22      | 61.65      |\n| BGE-FT + BGE-Reranker     | 21.91 | 54.58 | 33.33 | 45.36      | 72.16      | 55.78      |\n| BGE-FT + BGE-Reranker-FT  | 52.15 | 76.15 | 61.37 | 44.87      | 67.39      | 53.39      |\n| BGE-FT + RankGPT          | 44.21 | 73.68 | 55.51 | 48.00      | 72.00      | 57.33      |\n| GTR-XL          | 37.64 | 66.84 | 48.94 | 35.97      | 63.75      | 46.67      |\n| GTR-XL + BGE-Reranker     | 26.39 | 59.96 | 38.50 | 42.41      | 68.42      | 52.51      |\n| GTR-XL + BGE-Reranker-FT  | 57.50 | 78.92 | 66.06 | 58.56      | 77.65      | 66.22      |\n| GTR-XL + RankGPT          | 42.11 | 68.42 | 52.30 | 47.00      | 66.00      | 54.95      |\n| GritLM          | 44.67 | 76.00 | 57.03 | 39.91      | 69.34      | 51.14      |\n| GritLM + BGE-Reranker     | 30.06 | 65.87 | 43.20 | 43.64      | 70.87      | 54.23      |\n| GritLM + BGE-Reranker-FT  | 57.57 | 81.35 | 66.98 | 58.60      | 80.54      | 67.21      |\n| GritLM + RankGPT          | 37.89 | 70.53 | 51.19 | 44.00      | 66.00      | 52.70      |\n| DSI-XL          | 43.03 | 60.26 | 49.47 | 29.64      | 46.74      | 36.12      |\n| DSI-XL + BGE-Reranker     | 34.39 | 64.26 | 45.74 | 37.85      | 52.57      | 43.49      |\n| DSI-XL + BGE-Reranker-FT  | 50.02 | 68.60 | 57.43 | 36.49      | 52.40      | 42.36      |\n| DSI-XL + RankGPT          | 49.47 | 73.68 | 59.25 | 39.00      | 52.00      | 44.75      |\n| Self-Retrieval (StableLM) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| Self-Retrieval (Llama 2)  | 63.44 | 79.29 | 70.00 | 59.94      | 81.06      | 68.74      |",
      "| Method          |   R@1 | R@5   |   M@10 |\n|----------|-------|-------|--------|\n| Sparse Retrieval BM25 [37]          |  18.9 | 42.8  |   29.2 |\n| DocT5Query [28]          |  23.3 | 49.4  |   34.8 |\n| Dense Retrieval DPR [19]          |  29.1 | 62.8  |   43.4 |\n| Sentence-T5 [31]          |  27.3 | 58.9  |   40.7 |\n| Generative Retrieval DSI-Atomic [42] |  32.5 | 63.0  |   44.3 |\n| DynamicRetriever [54]          |  29   | 64.2  |   42.5 |\n| Ultron-URL [55]          |  29.6 | 56.4  |   40   |\n| Ultron-PQ [55]          |  31.6 | 64.0  |   45.3 |\n| Ultron-Atomic [55]          |  32.8 | 64.9  |   46.9 |\n| GenRet [39]          |  47.9 | -     |   58.1 |\n| Self-Retrieval          |  47.8 | 69.9  |   57.2 |\n\nTable 2: The experimental result of document retrieval on NQ320K.\n\nTable 3: The experimental result of document retrieval on MS MARCO.\n\nDocument retrieval We present the document retrieval results on NQ320K dataset in Table 2. Self-Retrieval outperforms all other generative retrieval methods and dense retrieval baselines across all three metrics. Compared to GenRet, the previously strongest generative retrieval method, SelfRetrieval improves Hits@1 by 5.2, Hits@10 by 3.8, and MRR@100 by 4.8 points. Notably, while other methods commonly employ query generation to augment their training data, Self-Retrieval achieves these results using only the original training set.\n\nTo evaluate the effectiveness of Self-Retrieval in non-Wikipedia scenarios, we extend our experiments to MS MARCO. To address the absence of document titles in MS MARCO, we employ Llama2 to automatically generate titles. As shown in Table 3, Self-Retrieval achieves comparable performance to the SOTA model GenRet, while significantly outperforming other baselines. These results demonstrate its adaptability and robustness in non-Wikipedia and title-lacking contexts.\n\nAblation study To study the effect of each component, we conduct ablation study on both NQ and TriviaQA. Results are presented in Table 4. All components prove crucial for Self-Retrieval's",
      "|          |        | NQ      | NQ    | NQ      | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|--------|---------|-------|---------|----------|----------|----------|\n| Model          | Params | H@1     | H@5   | M@5     | H@1        | H@5        | M@5        |\n| Sparse Retrieval BM25 [37]       | -      | 14.54   | 32.71 | 21.13   | 20.09      | 42.73      | 28.35      |\n| Dense Retrieval DPR [19]         | 110M   | 40.41   | 61.79 | 48.80   | 35.57      | 57.39      | 43.93      |\n| DPR-FT [19]          | 110M   | 42.21   | 60.45 | 49.33   | 36.58      | 53.05      | 42.91      |\n| BGE [50]          | 335M   | 36.30   | 66.95 | 48.05   | 46.97      | 70.14      | 55.95      |\n| BGE-FT [50]          | 335M   | 53.42   | 80.15 | 63.99   | 52.70      | 75.22      | 61.65      |\n| BGE-FT + BGE-Reranker-FT         | 770M   | 52.15   | 76.15 | 61.37   | 44.87      | 67.39      | 53.39      |\n| GTR-XL [32]          | 1.24B  | 37.64   | 66.84 | 48.94   | 35.97      | 63.75      | 46.67      |\n| GTR-XL + BGE-Reranker-FT         | 1.57B  | 57.50   | 78.92 | 66.06   | 58.56      | 77.65      | 66.22      |\n| GTR-XXL [32]          | 4.86B  | 39.21   | 69.72 | 50.88   | 35.97      | 64.15      | 46.83      |\n| text-embedding-ada-002          | -      | 34.28   | 62.28 | 44.64   | 35.09      | 62.00      | 45.15      |\n| GritLM [29]          | 7.24B  | 44.67   | 76.00 | 57.03   | 39.91      | 69.34      | 51.14      |\n| GritLM + BGE-Reranker-FT         | 7.57B  | 57.57   | 81.35 | 66.98   | 58.60      | 80.54      | 67.21      |\n| Generative retrieval DSI-XL [42] | 2.85B  | 43.03   | 60.26 | 49.47   | 29.64      | 46.74      | 36.12      |\n| DSI-XXL [42]          | 11.3B  | 43.81   | 60.45 | 50.20   | 30.55      | 46.67      | 36.56      |\n| SEAL [5]          | 406M   | 36.79   | 61.35 | 45.88   | 36.88      | 61.66      | 46.29      |\n| DSI-QG [59]          | 2.85B  | 34.88   | 56.60 | 43.33   | 29.15      | 45.53      | 35.20      |\n| NCI + BGE-Reranker-FT          | 1.07B  | 50.86   | 70.27 | 58.53   | 28.42      | 42.18      | 33.62      |\n| Self-Retrieval (StableLM)        | 2.8B   | 62.16 ∗ | 79.28 | 69.45 ∗ | 58.69 ∗    | 78.39 ∗    | 66.72 ∗    |\n| Self-Retrieval (Llama 2)         | 6.74B  | 63.44 ∗ | 79.29 | 70.00 ∗ | 59.94 ∗    | 81.06 ∗    | 68.74 ∗    |"
    ],
    "verification": {
      "result": "Undetermined",
      "justification": "The claim lacks specific details to match evidence about performance or components.",
      "confidence": 0.8
    }
  },
  {
    "claim_id": 42,
    "claim": "Limitations: yes\n\nSoundness: 3\n\nPresentation: 3\n\nContribution: 3\n\nEthics Review Flagged: ['No ethics review needed.']",
    "evidence": [
      "Table 4: Ablation study on NQ and TriviaQA.\n\n| Method          | NQ    | NQ    | NQ    | TriviaQA   | TriviaQA   | TriviaQA   |\n|----------|-------|-------|-------|----------|----------|----------|\n|          | H@1   | H@5   | M@5   | H@1        | H@5        | M@5        |\n| Self-Retrieval (base) | 62.16 | 79.28 | 69.45 | 58.69      | 78.39      | 66.72      |\n| w/o indexing          | 53.05 | 67.16 | 58.95 | 54.45      | 70.64      | 60.98      |\n| w/o title          | 47.81 | 60.90 | 52.81 | 52.32      | 67.91      | 58.48      |\n| w/o self-assessment   | 54.80 | 75.21 | 62.77 | 46.67      | 70.79      | 55.92      |\n\nperformance, with each ablation resulting in substantial performance degradation. Specifically, removing the indexing mechanism restricts the model to internalizing only the documents encountered during training, leading to poor performance on unseen passages. Without titles, we directly generate passages with constrained decoding. The absence of document titles significantly degrades performance, as titles provide critical global information that guides the LLM in generating relevant content. Furthermore, removing the self-assessment mechanism leads to a significant decrease in both datasets. Without self-assessment, the model cannot effectively evaluate and refine its initial retrieved passages, leading to less accurate document rankings. This degradation directly impacts downstream applications such as RAG, where precise passage ranking is crucial for generating highquality responses. These ablation results show that each component of Self-Retrieval addresses a specific challenge in the retrieval process, contributing to its overall effectiveness.\n\n## 4.3 Performance on Retrieval-Augmented Generation\n\nTable 5: The performance on retrieval-augmented generation. For baseline, we use BGE-FT as the retriever and a fine-tuned LLM as reader. Results are reported using Exact Match (EM) scores.",
      "- [1] Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, Shengling Gao, J. Guo, Xiangnan He, Yanyan Lan, Chenliang Li, Yiqun Liu, Ziyu Lyu, Weizhi Ma, Jun Ma, Zhaochun Ren, Pengjie Ren, Zhiqiang Wang, Min Wang, Jirong Wen, Lei Wu, Xin Xin, Jun Xu, Dawei Yin, Peng Zhang, Fan Zhang, Wei na Zhang, M. Zhang, and Xiaofei Zhu. Information retrieval meets large language models: A strategic report from chinese ir community. ArXiv , abs/2307.09751, 2023.\n- [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023.\n\n[3] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 , 2016.\n\n[4] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, Meng Lee,",
      "from Wikipedia for each dataset. Each document is segmented into passages of maximum 200 words, yielding approximately 1 million passages in total. The detailed statistics of the datasets are presented in Appendix A. We use passage-level Hits@{1, 5} and Mean Reciprocal Rank (MRR)@5 as evaluation metrics.\n\nTo comprehensively compare with other generative information retrieval methods, we also conduct experiments on document retrieval. Following NCI [49], we conduct experiments on NQ320K and utilize Recall@{1, 10} and MRR@100 as the evaluation metrics. To evaluate the model's robustness in non-Wikipedia scenarios where high-quality text and titles are not available, we conduct experiments on a subset of MS MARCO [3] following the experimental setup of Ultron [55]. The performance was measured using Recall@{1,5} and MRR@10.\n\nImplementation details In this study, we employ StableLM-3B [44] and Llama2-7B [43] as passage retrieval backbones. For document retrieval, we employ StableLM-1.6B [4] for NQ320K and StableLM-3B for MS MARCO. We train the models using ZeRO stage-2 optimization on 8 NVIDIA A100 (80 GB) GPUs with the AdamW optimizer, a batch size of 16 per GPU, and BFloat16 precision. The models are trained for 3 epochs with a learning rate of 2e-5. During inference, we use beam search to generate 5 titles and 10 passages for each title, with hyperparameters τ and δ set to 0.4 across all models and datasets.",
      "During training, we utilize the gold passage from the supervision data as the positive instance, while sampling negative instances from both the same and different documents. This training strategy conditions the LLM to accurately discern and verify the relevance of its outputs, thereby enhancing its autonomous relevance assessment capabilities and improving the overall precision of the retrieval process.\n\nDuring inference, the overall relevance score S is composed of the document title score S T and the self-assessment score S P . Specifically, the document title score is derived from the title generation probability, while the self-assessment score is calculated based on the probability of the language model rejecting the passage. Formally, for a set of generated titles and passages { ( t 1 , p 1 ) , ( t 2 , p 2 ) , . . . , ( t n , p n ) } , the title score for each ( t i , p i ) is given by:\n\n$$\\mathcal { S } _ { i } ^ { T } = \\text {Softmax} ( P ( t _ { i } | q ; \\theta ) / \\tau ) \\\\$$\n\nand the assessment score is:\n\n$$\\mathcal { S } _ { i } ^ { P } = \\text {softmax} ( ( 1 - P ( \\text {reaction response} | q , t _ { i } , p _ { i } ; \\theta ) ) / \\delta ) \\\\ \\\\ + \\text {a} \\, .$$\n\nwhere τ and δ are temperature parameters used to scale the logits. Based on preliminary experiments on the development set, we simply set τ = δ = 0 . 4 for the main passage retrieval experiments.\n\nThe final relevance score is computed as the product of these two components:\n\n$$\\mathcal { S } = \\mathcal { S } ^ { T } \\cdot \\mathcal { S } ^ { P }$$\n\nThis combined score is then used to rerank the passage set, producing a more refined ordering based on relevance.\n\n## 3.4 Training &amp; Inference"
    ],
    "verification": {
      "result": "Partially Supported",
      "justification": "The evidence shows ablation results but does not directly confirm the claim's specific metrics or ratings.",
      "confidence": 0.75
    }
  }
]