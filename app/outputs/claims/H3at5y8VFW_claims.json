[
  {
    "submission_id": "H3at5y8VFW",
    "submission_title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
    "review_id": "WoHIlsOUVC",
    "reviewer": "Reviewer_uLuX",
    "invitation": "NeurIPS.cc/2024/Conference/Submission15905/-/Official_Review",
    "rating": 5,
    "confidence": 4,
    "review_text": "",
    "summary": "This paper introduces Self-Retrieval, an end-to-end IR system driven entirely by a single LLM. This model integrates all essential IR functions—indexing, retrieval, and reranking—into the LLM's architecture. By internalizing the retrieval corpus through self-supervised learning, the model transforms the retrieval process into a sequence of passage generation tasks and conducts self-assessment for reranking. The authors provide experimental evidence showing that Self-Retrieval outperforms traditional sparse, dense, and generative retrieval methods on benchmark datasets like NQ and TriviaQA.",
    "strengths": "1. The integration of all IR functions into a single LLM is a novel contribution that leverages the inherent capabilities of LLMs across the full spectrum of IR tasks, offering a streamlined and potentially more effective approach.\n2. The concept of Self-Retrieval is introduced clearly, making it accessible to readers. The detailed explanation of how the LLM handles indexing, retrieval, and reranking provides a good understanding of the system's operation.\n3. The paper presents good experimental results that demonstrate significant improvements over existing retrieval methods.",
    "weaknesses": "1. My major concern is that the experimental settings are inconsistent with existing work, making the results unconvincing. Specifically,\n  - Most existing studies conducted experiments on NQ@320k datasets, but the main experiments of this paper are conducted on NQ@40k datasets. It is important to explain the reason of this setting.\n  - According to the statistics in Table 2, each document in the dataset is split into 26~29 passages, which indicates that the passages are quite short. It is known that retrieving short passages are easier for generative retrieval methods. \n  - It is necessary to explain why different models are selected for NQ@40k, TQA@40k, and NQ@320k experiments.\n  - It is better to include more experimental results on the full KILT datasets as many existing studies for a fair comparison.\n2. In Section 3.4, the three tasks are learned in a \"1+2\" manner. Please add more explanations on this design and provide experimental evaluation on other possible strategies (e.g., training the three tasks jointly)\n3. Ensuring consistent use of key terms throughout the paper would improve its readability and professionalism. For example, all \"large language models\" should be written in \"LLMs\".",
    "questions": "Please see my concerns in weaknesses.",
    "limitations": "n/a",
    "soundness": 3,
    "presentation": 3,
    "contribution": 3,
    "flag_for_ethics_review": [
      "No ethics review needed."
    ],
    "details_of_ethics_concerns": "",
    "full_review": "Summary: This paper introduces Self-Retrieval, an end-to-end IR system driven entirely by a single LLM. This model integrates all essential IR functions—indexing, retrieval, and reranking—into the LLM's architecture. By internalizing the retrieval corpus through self-supervised learning, the model transforms the retrieval process into a sequence of passage generation tasks and conducts self-assessment for reranking. The authors provide experimental evidence showing that Self-Retrieval outperforms traditional sparse, dense, and generative retrieval methods on benchmark datasets like NQ and TriviaQA.\n\nStrengths: 1. The integration of all IR functions into a single LLM is a novel contribution that leverages the inherent capabilities of LLMs across the full spectrum of IR tasks, offering a streamlined and potentially more effective approach.\n2. The concept of Self-Retrieval is introduced clearly, making it accessible to readers. The detailed explanation of how the LLM handles indexing, retrieval, and reranking provides a good understanding of the system's operation.\n3. The paper presents good experimental results that demonstrate significant improvements over existing retrieval methods.\n\nWeaknesses: 1. My major concern is that the experimental settings are inconsistent with existing work, making the results unconvincing. Specifically,\n  - Most existing studies conducted experiments on NQ@320k datasets, but the main experiments of this paper are conducted on NQ@40k datasets. It is important to explain the reason of this setting.\n  - According to the statistics in Table 2, each document in the dataset is split into 26~29 passages, which indicates that the passages are quite short. It is known that retrieving short passages are easier for generative retrieval methods. \n  - It is necessary to explain why different models are selected for NQ@40k, TQA@40k, and NQ@320k experiments.\n  - It is better to include more experimental results on the full KILT datasets as many existing studies for a fair comparison.\n2. In Section 3.4, the three tasks are learned in a \"1+2\" manner. Please add more explanations on this design and provide experimental evaluation on other possible strategies (e.g., training the three tasks jointly)\n3. Ensuring consistent use of key terms throughout the paper would improve its readability and professionalism. For example, all \"large language models\" should be written in \"LLMs\".\n\nQuestions: Please see my concerns in weaknesses.\n\nLimitations: n/a\n\nSoundness: 3\n\nPresentation: 3\n\nContribution: 3\n\nEthics Review Flagged: ['No ethics review needed.']",
    "extracted_claims": [
      "Summary: This paper introduces Self-Retrieval, an end-to-end IR system driven entirely by a single LLM.",
      "The authors provide experimental evidence showing that Self-Retrieval outperforms traditional sparse, dense, and generative retrieval methods on benchmark datasets like NQ and TriviaQA.",
      "The integration of all IR functions into a single LLM is a novel contribution that leverages the inherent capabilities of LLMs across the full spectrum of IR tasks, offering a streamlined and potentially more effective approach.",
      "The concept of Self-Retrieval is introduced clearly, making it accessible to readers.",
      "The paper presents good experimental results that demonstrate significant improvements over existing retrieval methods.",
      "Weaknesses: 1.",
      "- It is better to include more experimental results on the full KILT datasets as many existing studies for a fair comparison.",
      "Ensuring consistent use of key terms throughout the paper would improve its readability and professionalism.",
      "Questions: Please see my concerns in weaknesses.",
      "Limitations: n/a\n\nSoundness: 3\n\nPresentation: 3\n\nContribution: 3\n\nEthics Review Flagged: ['No ethics review needed.']"
    ]
  },
  {
    "submission_id": "H3at5y8VFW",
    "submission_title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
    "review_id": "7EaZEFRp1Y",
    "reviewer": "Reviewer_iXLD",
    "invitation": "NeurIPS.cc/2024/Conference/Submission15905/-/Official_Review",
    "rating": 6,
    "confidence": 4,
    "review_text": "",
    "summary": "This paper proposes Self-Retrieval, an LM that retrieve, rerank passages, and generate answers using a single model. For the retrieval task, it adopts generative retrieval and directly generates passage text. For reranking, it utilizes the generation probability as the relevance score. For answer generation, it generates answers based on the top-1 passage. Evaluation shows that Self-Retrieval outperforms previous dense and generative retrievers in retrieval tasks and achieves better EM scores in answer generation tasks.",
    "strengths": "1. Self-Retrieval consolidates the multi-step RAG pipeline with a single model. The concept is novel.\n\n2. Self-Retrieval achieves the best performance for both passage retrieval and answer generation tasks.\n\n3. Self-Retrieval shows promising results when the corpus size scales to 3 million.",
    "weaknesses": "1. Generating passage content is time-consuming. This paper could analyze the latency of Self-Retrieval and compare it to other alternatives, such as generating spans (ref SEAL) or keywords (ref Term-Set Generation).\n\n2. The proposed model includes an in-domain fine-tuned reranker, while the baseline BGE-FT + reader does not have a reranking stage. This may make the comparison unfair since reranking can significantly improve RAG results.\n\n3. The evaluation are all based on Wikipedia. It is unclear whether the model can perform well on a corpus that is not as well-structured as Wikipedia.\n\n4. The different steps in Self-Retrieval are independently optimized, and the `knowledge sharing and deep collaboration` effects of this consolidated model have not been validated.",
    "questions": "1. Compare the efficiency and effectiveness of using shared models versus separate models for the three steps in Self-Retrieval?",
    "limitations": "The limitations are adequately discussed.",
    "soundness": 3,
    "presentation": 3,
    "contribution": 3,
    "flag_for_ethics_review": [
      "No ethics review needed."
    ],
    "details_of_ethics_concerns": "",
    "full_review": "Summary: This paper proposes Self-Retrieval, an LM that retrieve, rerank passages, and generate answers using a single model. For the retrieval task, it adopts generative retrieval and directly generates passage text. For reranking, it utilizes the generation probability as the relevance score. For answer generation, it generates answers based on the top-1 passage. Evaluation shows that Self-Retrieval outperforms previous dense and generative retrievers in retrieval tasks and achieves better EM scores in answer generation tasks.\n\nStrengths: 1. Self-Retrieval consolidates the multi-step RAG pipeline with a single model. The concept is novel.\n\n2. Self-Retrieval achieves the best performance for both passage retrieval and answer generation tasks.\n\n3. Self-Retrieval shows promising results when the corpus size scales to 3 million.\n\nWeaknesses: 1. Generating passage content is time-consuming. This paper could analyze the latency of Self-Retrieval and compare it to other alternatives, such as generating spans (ref SEAL) or keywords (ref Term-Set Generation).\n\n2. The proposed model includes an in-domain fine-tuned reranker, while the baseline BGE-FT + reader does not have a reranking stage. This may make the comparison unfair since reranking can significantly improve RAG results.\n\n3. The evaluation are all based on Wikipedia. It is unclear whether the model can perform well on a corpus that is not as well-structured as Wikipedia.\n\n4. The different steps in Self-Retrieval are independently optimized, and the `knowledge sharing and deep collaboration` effects of this consolidated model have not been validated.\n\nQuestions: 1. Compare the efficiency and effectiveness of using shared models versus separate models for the three steps in Self-Retrieval?\n\nLimitations: The limitations are adequately discussed.\n\nSoundness: 3\n\nPresentation: 3\n\nContribution: 3\n\nEthics Review Flagged: ['No ethics review needed.']",
    "extracted_claims": [
      "Summary: This paper proposes Self-Retrieval, an LM that retrieve, rerank passages, and generate answers using a single model.",
      "Evaluation shows that Self-Retrieval outperforms previous dense and generative retrievers in retrieval tasks and achieves better EM scores in answer generation tasks.",
      "The concept is novel.",
      "Self-Retrieval achieves the best performance for both passage retrieval and answer generation tasks.",
      "Self-Retrieval shows promising results when the corpus size scales to 3 million.",
      "Weaknesses: 1.",
      "The proposed model includes an in-domain fine-tuned reranker, while the baseline BGE-FT + reader does not have a reranking stage.",
      "This may make the comparison unfair since reranking can significantly improve RAG results.",
      "Compare the efficiency and effectiveness of using shared models versus separate models for the three steps in Self-Retrieval?",
      "Limitations: The limitations are adequately discussed.",
      "Soundness: 3\n\nPresentation: 3\n\nContribution: 3\n\nEthics Review Flagged: ['No ethics review needed.']"
    ]
  },
  {
    "submission_id": "H3at5y8VFW",
    "submission_title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
    "review_id": "xeR8rdcAPK",
    "reviewer": "Reviewer_g3kT",
    "invitation": "NeurIPS.cc/2024/Conference/Submission15905/-/Official_Review",
    "rating": 6,
    "confidence": 5,
    "review_text": "",
    "summary": "This paper introduces Self-Retrieval, a new generative retrieval architecture.  Self-Retrieval first memorizes the corpus into LLM's parametric knowledge using self-supervised training. Given a query, it generates the target document with constrained decoding, then re-assess document by decoding if the document can answer the query.  On subset of NQ and TriviaQA, this approach significantly outperforms existing dual encoders and generative retrieval models.",
    "strengths": "- Intuitive architecture for using LLM for retrieval. The paper presents a self-supervise object to help model memorize the corpus. Then it integrate retrieval and reranking into a single constrained decoding process. The \"reranking\" process can be viewed as self-critique / chain-of-thoughts, and may potentially unlock deeper integration between retrieval and reasoning.\n- Strong quality improvements. The paper reports substantial improvements over previous dual-encoder approaches and generative retrieval models.\n- Ablations shows that the model scales well with model size. Previous dense retrievers often plateau due to the bottleneck layer; the scaling curve of this new architecture is promising.",
    "weaknesses": "- Experiment only used wikipedia-based datasets. However, wikipedia is heavily used in pretraining, so it is unclear if the proposed approach can let model sufficiently memorize other datasets. \n- More importantly, the method relies on generating the passage title. Unlike wikipedia, many retrieval datasets do not have high-quality, natural language passage titles. It is unclear how the method works on those datasets.  It would be nice to test retrieval benchmarks like MS MARCO or BEIR/MTEB. \n- Missing dense retrieval + cross-attention reranking baselines. Such 2-staged pipeline is standard in IR. Since the proposed method's reranking stage essentially uses cross attention to judge the query and the retrieved candidate passage, the computational cost of the reranking stage is similar to that of a separate cross-attention reranker. It is fair and necessary to compare it with commonly-used rerankers such as MonoT5, RankT5, or BGE reranker.  The ablation in Table 3 seems to show that the proposed method's retrieval-alone performance is stronger than most retrieval baselines, but the paper can be more convincing if having e2e comparison to other 2-stage retrieval pipelines like BGE + BGE reranker or GTR + RankT5.\n- Lacking efficiency discussion.",
    "questions": "- Can the method scale up to the full NQ/TriviaQA? If so, would be nice to see the performance on these more standard setups. If not, what is the main bottleneck for scaling up? \n- How many candidates were considered in the reranking stage?",
    "limitations": "NA",
    "soundness": 3,
    "presentation": 4,
    "contribution": 3,
    "flag_for_ethics_review": [
      "No ethics review needed."
    ],
    "details_of_ethics_concerns": "",
    "full_review": "Summary: This paper introduces Self-Retrieval, a new generative retrieval architecture.  Self-Retrieval first memorizes the corpus into LLM's parametric knowledge using self-supervised training. Given a query, it generates the target document with constrained decoding, then re-assess document by decoding if the document can answer the query.  On subset of NQ and TriviaQA, this approach significantly outperforms existing dual encoders and generative retrieval models.\n\nStrengths: - Intuitive architecture for using LLM for retrieval. The paper presents a self-supervise object to help model memorize the corpus. Then it integrate retrieval and reranking into a single constrained decoding process. The \"reranking\" process can be viewed as self-critique / chain-of-thoughts, and may potentially unlock deeper integration between retrieval and reasoning.\n- Strong quality improvements. The paper reports substantial improvements over previous dual-encoder approaches and generative retrieval models.\n- Ablations shows that the model scales well with model size. Previous dense retrievers often plateau due to the bottleneck layer; the scaling curve of this new architecture is promising.\n\nWeaknesses: - Experiment only used wikipedia-based datasets. However, wikipedia is heavily used in pretraining, so it is unclear if the proposed approach can let model sufficiently memorize other datasets. \n- More importantly, the method relies on generating the passage title. Unlike wikipedia, many retrieval datasets do not have high-quality, natural language passage titles. It is unclear how the method works on those datasets.  It would be nice to test retrieval benchmarks like MS MARCO or BEIR/MTEB. \n- Missing dense retrieval + cross-attention reranking baselines. Such 2-staged pipeline is standard in IR. Since the proposed method's reranking stage essentially uses cross attention to judge the query and the retrieved candidate passage, the computational cost of the reranking stage is similar to that of a separate cross-attention reranker. It is fair and necessary to compare it with commonly-used rerankers such as MonoT5, RankT5, or BGE reranker.  The ablation in Table 3 seems to show that the proposed method's retrieval-alone performance is stronger than most retrieval baselines, but the paper can be more convincing if having e2e comparison to other 2-stage retrieval pipelines like BGE + BGE reranker or GTR + RankT5.\n- Lacking efficiency discussion.\n\nQuestions: - Can the method scale up to the full NQ/TriviaQA? If so, would be nice to see the performance on these more standard setups. If not, what is the main bottleneck for scaling up? \n- How many candidates were considered in the reranking stage?\n\nLimitations: NA\n\nSoundness: 3\n\nPresentation: 4\n\nContribution: 3\n\nEthics Review Flagged: ['No ethics review needed.']",
    "extracted_claims": [
      "Summary: This paper introduces Self-Retrieval, a new generative retrieval architecture.",
      "On subset of NQ and TriviaQA, this approach significantly outperforms existing dual encoders and generative retrieval models.",
      "The paper presents a self-supervise object to help model memorize the corpus.",
      "- Strong quality improvements.",
      "The paper reports substantial improvements over previous dual-encoder approaches and generative retrieval models.",
      "- Ablations shows that the model scales well with model size.",
      "Previous dense retrievers often plateau due to the bottleneck layer; the scaling curve of this new architecture is promising.",
      "Weaknesses: - Experiment only used wikipedia-based datasets.",
      "However, wikipedia is heavily used in pretraining, so it is unclear if the proposed approach can let model sufficiently memorize other datasets.",
      "- Missing dense retrieval + cross-attention reranking baselines.",
      "Since the proposed method's reranking stage essentially uses cross attention to judge the query and the retrieved candidate passage, the computational cost of the reranking stage is similar to that of a separate cross-attention reranker.",
      "The ablation in Table 3 seems to show that the proposed method's retrieval-alone performance is stronger than most retrieval baselines, but the paper can be more convincing if having e2e comparison to other 2-stage retrieval pipelines like BGE + BGE reranker or GTR + RankT5.",
      "- Lacking efficiency discussion.",
      "Limitations: NA\n\nSoundness: 3\n\nPresentation: 4\n\nContribution: 3\n\nEthics Review Flagged: ['No ethics review needed.']"
    ]
  },
  {
    "submission_id": "H3at5y8VFW",
    "submission_title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
    "review_id": "r1WHgXU4Re",
    "reviewer": "Reviewer_xB99",
    "invitation": "NeurIPS.cc/2024/Conference/Submission15905/-/Official_Review",
    "rating": 6,
    "confidence": 4,
    "review_text": "",
    "summary": "The paper proposes an approach of self-retrieval, which uses the probability of generation of the passage as the ranking criterion. To limit the generation to the existing passages, a trie-structure is used, forcing the generation to produce the existing passages.\nThe experiments compared the method with several existing approaches, including sparse retrieval, dense retrieval and generation-based retrieval. The proposed method outperforms the others.",
    "strengths": "The idea of relying on the generation of an existing passage for ranking is interesting. The use of trie structure to constrain the generation to the existing documents is also nice. The proposed method thus has some novelty compared to the literature.\nThe experimental results are very good, showing improved performance on document retrieval and QA.",
    "weaknesses": "A key idea is the use of trie for passage generation. This is described briefly. More details should be presented. If a whole passage should be generated using trie, then the depth of trie structure will be very large (equivalent to the length of the passage). What about the storage cost of trie? What is the time efficiency?",
    "questions": "Have you evaluated the space and time complexity of the approach, and compare it to the existing methods?\nHow do you deal with long document (in particular for the trie structure)?",
    "limitations": "yes",
    "soundness": 3,
    "presentation": 3,
    "contribution": 3,
    "flag_for_ethics_review": [
      "No ethics review needed."
    ],
    "details_of_ethics_concerns": "",
    "full_review": "Summary: The paper proposes an approach of self-retrieval, which uses the probability of generation of the passage as the ranking criterion. To limit the generation to the existing passages, a trie-structure is used, forcing the generation to produce the existing passages.\nThe experiments compared the method with several existing approaches, including sparse retrieval, dense retrieval and generation-based retrieval. The proposed method outperforms the others.\n\nStrengths: The idea of relying on the generation of an existing passage for ranking is interesting. The use of trie structure to constrain the generation to the existing documents is also nice. The proposed method thus has some novelty compared to the literature.\nThe experimental results are very good, showing improved performance on document retrieval and QA.\n\nWeaknesses: A key idea is the use of trie for passage generation. This is described briefly. More details should be presented. If a whole passage should be generated using trie, then the depth of trie structure will be very large (equivalent to the length of the passage). What about the storage cost of trie? What is the time efficiency?\n\nQuestions: Have you evaluated the space and time complexity of the approach, and compare it to the existing methods?\nHow do you deal with long document (in particular for the trie structure)?\n\nLimitations: yes\n\nSoundness: 3\n\nPresentation: 3\n\nContribution: 3\n\nEthics Review Flagged: ['No ethics review needed.']",
    "extracted_claims": [
      "Summary: The paper proposes an approach of self-retrieval, which uses the probability of generation of the passage as the ranking criterion.",
      "The proposed method outperforms the others.",
      "The proposed method thus has some novelty compared to the literature.",
      "The experimental results are very good, showing improved performance on document retrieval and QA.",
      "Weaknesses: A key idea is the use of trie for passage generation.",
      "More details should be presented.",
      "Limitations: yes\n\nSoundness: 3\n\nPresentation: 3\n\nContribution: 3\n\nEthics Review Flagged: ['No ethics review needed.']"
    ]
  }
]