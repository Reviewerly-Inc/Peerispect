{
  "submission_id": "qjnl1QUnFA",
  "submission_title": "High-Fidelity Audio Compression with Improved RVQGAN",
  "review_index": 3,
  "review_id": "3GDoZw7Tv9",
  "reviewer": "Reviewer_B66v",
  "invitation": "NeurIPS.cc/2023/Conference/Submission14752/-/Official_Review",
  "invitations": [
    "NeurIPS.cc/2023/Conference/Submission14752/-/Official_Review",
    "NeurIPS.cc/2023/Conference/-/Edit"
  ],
  "signatures": [
    "NeurIPS.cc/2023/Conference/Submission14752/Reviewer_B66v"
  ],
  "rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",
  "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
  "review_text": "",
  "summary": "This paper introduces a RVQGAN-based neural audio codec method, demonstrating superior audio reconstruction quality, a high compression rate, and generalization across diverse audio domains. The authors substantiate the significant performance superiority of their model over alternatives through extensive and thorough qualitative and quantitative experiments. They present and validate their technique to fully utilize residual vector quantization, alongside model, discriminator, and loss design choices for enhanced performance.",
  "strengths": "* The paper addresses some of the key challenges in the neural audio codec domain.\n* The authors conducted strong and extensive experiments, providing comprehensive results.\n* The reference list appears to be thorough and comprehensive.\n* The authors support their findings by sharing the developed model, which is beneficial for the research community.\n",
  "weaknesses": "* The authors derived the proposed methods from existing studies and experimentally validate them in the neural audio codec domain. This approach seems to compromise the scientific novelty of the research.",
  "questions": "* Could the model be applied to downstream applications such as training text-to-speech (TTS) models? Previous works like EnCodec and SoundStream utilized causal architectures to make them suitable for in-context learning or prompting in TTS tasks.",
  "limitations": "The authors have adequately addressed both the limitations of their research and its possible societal impacts.",
  "soundness": "4 excellent",
  "presentation": "3 good",
  "contribution": "4 excellent",
  "flag_for_ethics_review": [
    "No ethics review needed."
  ],
  "details_of_ethics_concerns": "",
  "combined_review_text": "This paper introduces a RVQGAN-based neural audio codec method, demonstrating superior audio reconstruction quality, a high compression rate, and generalization across diverse audio domains. The authors substantiate the significant performance superiority of their model over alternatives through extensive and thorough qualitative and quantitative experiments. They present and validate their technique to fully utilize residual vector quantization, alongside model, discriminator, and loss design choices for enhanced performance.\n4 excellent\n3 good\n4 excellent\n* The paper addresses some of the key challenges in the neural audio codec domain.\n* The authors conducted strong and extensive experiments, providing comprehensive results.\n* The reference list appears to be thorough and comprehensive.\n* The authors support their findings by sharing the developed model, which is beneficial for the research community.\n\n* The authors derived the proposed methods from existing studies and experimentally validate them in the neural audio codec domain. This approach seems to compromise the scientific novelty of the research.\n* Could the model be applied to downstream applications such as training text-to-speech (TTS) models? Previous works like EnCodec and SoundStream utilized causal architectures to make them suitable for in-context learning or prompting in TTS tasks.\nThe authors have adequately addressed both the limitations of their research and its possible societal impacts.\n7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nYes"
}