[
  {
    "submission_id": "qjnl1QUnFA",
    "submission_title": "High-Fidelity Audio Compression with Improved RVQGAN",
    "review_id": "2GorIEtexk",
    "reviewer": "Reviewer_5w24",
    "invitation": "NeurIPS.cc/2023/Conference/Submission14752/-/Official_Review",
    "rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",
    "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
    "review_text": "",
    "summary": "In the present paper, the authors introduce RVQGAN, a neural audio codec that uses a convolutional encoder / decoder along with Residual Vector Quantization as a bottleneck, with a multi scale mel reconstruction loss and different adversarial losses.\nThey show state of the art performance from 3 to 8kbps, compared with the EnCodec model [8].\n\nThe key novelties are:\n- in each VQ layer, the authors perform the retrieval of the nearest codebook entry into a lower dimension space, and use cosine similarity instead of L2 distance to boost the utilization of the codebooks.\n- the authors drop the exponential moving average rule for learning the codebooks.\n- the author notice that the original technique from Soundstream [45] to select a varying number of quantizers can hurt the full bandwidth performance, and thus select 50% of the time all the quantizers in RVQ.\n- refinement of the losses and adversaries from previous work (in particular using different weights for different frequency bands).\n- balancing of the dataset to sample more often fullband audio.\n\nThe authors provide extensive ablation studies with objective metrics, and one subjective comparison with EnCodec with various bitrates.",
    "strengths": "- great execution and illustration of the various issues tackled here and the proposed solutions.\n- quality of the final model clearly surpasses the existing state of the art.\n- detailed ablation study with objective metrics.\n- single model for fullband audio over multiple audio domains.",
    "weaknesses": "- incremental improvement over previous work: overall method is coming from [45], adversarial losses are a combination of the one from [45] and [8]. Minor changes to the objective loss compared with [15, 45]. The authors however claim novelty: l.59, \"we make impactful design changes [...]: multi scale stft discriminator, multi scale losses\".\n- some details are unclear to me, in particular, the authors mention they do not use the EMA rule from [9]. How are the codebooks updated then? The authors also mention a low dimension projection, but do not mention when and how it is computed and updated. See questions.\n- no ablation with subjective evaluations: could have been interesting to clearly identify where most of the subjective gains is coming from e.g. is it from the quantizer, adversarial losses or dataset balancing?\n- seems like the authors do a comparison of a 24kHz baseline model with a 44.1kHz, keeping the ground truth as 44.1kHz, which can have a high impact on subjective and objective metrics independently of the design choices made by the authors. In particular the visqol for Encodec in Table 3 is much lower than reported in [8].",
    "questions": "As mentioned before, I would need more clarification on the exact algorithm used for VQ. Is a PCA computed ? if so how often is it updated (as the encoder output distribution might change). How are the codebooks updated ?\n\nIn Section 3.4, the architecture for the multi scale discriminator is missing. Is it the same one as [8] or [45]?\n\nParagraph starting 193: this insight has been noted and motivated before in [45], [5] and [40], it doesn't seem like the authors bring any new material evidence here?\n\nIt would be interesting to see the breakdown of Figure 3 by the category of audio.\n\nA last question would be over learnt snake activation parameters. Do the authors have any insight over the distribution of the learnt $\\alpha$? does this vary with the layer? I'm trying to get a better sense of how exactly the model is utilizing this feature.\n\n\n",
    "limitations": "authors properly address societal impact.",
    "soundness": "3 good",
    "presentation": "3 good",
    "contribution": "2 fair",
    "flag_for_ethics_review": [
      "No ethics review needed."
    ],
    "details_of_ethics_concerns": "",
    "full_review": "Summary: In the present paper, the authors introduce RVQGAN, a neural audio codec that uses a convolutional encoder / decoder along with Residual Vector Quantization as a bottleneck, with a multi scale mel reconstruction loss and different adversarial losses.\nThey show state of the art performance from 3 to 8kbps, compared with the EnCodec model [8].\n\nThe key novelties are:\n- in each VQ layer, the authors perform the retrieval of the nearest codebook entry into a lower dimension space, and use cosine similarity instead of L2 distance to boost the utilization of the codebooks.\n- the authors drop the exponential moving average rule for learning the codebooks.\n- the author notice that the original technique from Soundstream [45] to select a varying number of quantizers can hurt the full bandwidth performance, and thus select 50% of the time all the quantizers in RVQ.\n- refinement of the losses and adversaries from previous work (in particular using different weights for different frequency bands).\n- balancing of the dataset to sample more often fullband audio.\n\nThe authors provide extensive ablation studies with objective metrics, and one subjective comparison with EnCodec with various bitrates.\n\nStrengths: - great execution and illustration of the various issues tackled here and the proposed solutions.\n- quality of the final model clearly surpasses the existing state of the art.\n- detailed ablation study with objective metrics.\n- single model for fullband audio over multiple audio domains.\n\nWeaknesses: - incremental improvement over previous work: overall method is coming from [45], adversarial losses are a combination of the one from [45] and [8]. Minor changes to the objective loss compared with [15, 45]. The authors however claim novelty: l.59, \"we make impactful design changes [...]: multi scale stft discriminator, multi scale losses\".\n- some details are unclear to me, in particular, the authors mention they do not use the EMA rule from [9]. How are the codebooks updated then? The authors also mention a low dimension projection, but do not mention when and how it is computed and updated. See questions.\n- no ablation with subjective evaluations: could have been interesting to clearly identify where most of the subjective gains is coming from e.g. is it from the quantizer, adversarial losses or dataset balancing?\n- seems like the authors do a comparison of a 24kHz baseline model with a 44.1kHz, keeping the ground truth as 44.1kHz, which can have a high impact on subjective and objective metrics independently of the design choices made by the authors. In particular the visqol for Encodec in Table 3 is much lower than reported in [8].\n\nQuestions: As mentioned before, I would need more clarification on the exact algorithm used for VQ. Is a PCA computed ? if so how often is it updated (as the encoder output distribution might change). How are the codebooks updated ?\n\nIn Section 3.4, the architecture for the multi scale discriminator is missing. Is it the same one as [8] or [45]?\n\nParagraph starting 193: this insight has been noted and motivated before in [45], [5] and [40], it doesn't seem like the authors bring any new material evidence here?\n\nIt would be interesting to see the breakdown of Figure 3 by the category of audio.\n\nA last question would be over learnt snake activation parameters. Do the authors have any insight over the distribution of the learnt $\\alpha$? does this vary with the layer? I'm trying to get a better sense of how exactly the model is utilizing this feature.\n\n\n\n\nLimitations: authors properly address societal impact.\n\nSoundness: 3 good\n\nPresentation: 3 good\n\nContribution: 2 fair\n\nEthics Review Flagged: ['No ethics review needed.']"
  },
  {
    "submission_id": "qjnl1QUnFA",
    "submission_title": "High-Fidelity Audio Compression with Improved RVQGAN",
    "review_id": "YjQqdH8Z78",
    "reviewer": "Reviewer_ZhX4",
    "invitation": "NeurIPS.cc/2023/Conference/Submission14752/-/Official_Review",
    "rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",
    "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
    "review_text": "",
    "summary": "This paper introduces a novel high-fidelity neural audio compression algorithm that achieves impressive compression ratios while maintaining audio quality. The authors combine advancements in high-fidelity audio generation with improved vector quantization techniques from the image domain, along with enhanced adversarial and reconstruction losses. Their approach achieves a remarkable 90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. One of the notable strengths of this work is its universal applicability, as it can compress various audio domains (speech, environment, music) using a single model.\n\nThe authors conduct a thorough comparison with competing audio compression algorithms and demonstrate the superior performance of their method. Furthermore, they provide detailed ablations for each design choice, allowing readers to gain insights into the effectiveness of different components. Additionally, the paper offers open-source code and trained model weights, which contribute to the reproducibility of the results.",
    "strengths": "- **Impressive compression performance**: The proposed algorithm achieves a 90x compression ratio for 44.1 KHz audio at just 8kbps bandwidth, demonstrating its effectiveness in reducing data size while preserving audio quality.\n- **Novel Method**: The proposed \"codebook collapse\" and \"quantizer dropout\" effectively address the issues in lossy audio compression. \n- **Universal applicability**: The single model's ability to compress various audio domains makes it highly versatile and applicable to generative modeling of different audio types.\n- **Comprehensive evaluation**: The authors compare their method against existing audio compression algorithms, demonstrating its superiority in terms of performance.\n- **Thorough ablations**: The paper provides detailed insights into the impact of design choices, allowing readers to understand the effectiveness of different components and their contributions to the overall results.\n- **Reproducibility**: The availability of open-source code and trained model weights enhances the reproducibility of the research, enabling other researchers to build upon and validate the findings.",
    "weaknesses": "- The novelty of the proposed model structure is a combination of existing models: \n  - factorized codes and L2-normalized codes are from Improved VQGAN image model;\n  - Snake activation function from BigVGAN\n- This paper presents a strong audio compression technique. However, since the proposed novel points are specifically tailored for a narrow domain, their impact may be limited to the machine learning community and other domains like computer vision/NLP",
    "questions": "- Have you attempted to apply a similar architecture to the vocoder in TTS? \n- Which components do you believe can be applied and generalized to other domains or tasks?",
    "limitations": "The authors have adequately addressed the limitations.",
    "soundness": "3 good",
    "presentation": "3 good",
    "contribution": "3 good",
    "flag_for_ethics_review": [
      "No ethics review needed."
    ],
    "details_of_ethics_concerns": "",
    "full_review": "Summary: This paper introduces a novel high-fidelity neural audio compression algorithm that achieves impressive compression ratios while maintaining audio quality. The authors combine advancements in high-fidelity audio generation with improved vector quantization techniques from the image domain, along with enhanced adversarial and reconstruction losses. Their approach achieves a remarkable 90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. One of the notable strengths of this work is its universal applicability, as it can compress various audio domains (speech, environment, music) using a single model.\n\nThe authors conduct a thorough comparison with competing audio compression algorithms and demonstrate the superior performance of their method. Furthermore, they provide detailed ablations for each design choice, allowing readers to gain insights into the effectiveness of different components. Additionally, the paper offers open-source code and trained model weights, which contribute to the reproducibility of the results.\n\nStrengths: - **Impressive compression performance**: The proposed algorithm achieves a 90x compression ratio for 44.1 KHz audio at just 8kbps bandwidth, demonstrating its effectiveness in reducing data size while preserving audio quality.\n- **Novel Method**: The proposed \"codebook collapse\" and \"quantizer dropout\" effectively address the issues in lossy audio compression. \n- **Universal applicability**: The single model's ability to compress various audio domains makes it highly versatile and applicable to generative modeling of different audio types.\n- **Comprehensive evaluation**: The authors compare their method against existing audio compression algorithms, demonstrating its superiority in terms of performance.\n- **Thorough ablations**: The paper provides detailed insights into the impact of design choices, allowing readers to understand the effectiveness of different components and their contributions to the overall results.\n- **Reproducibility**: The availability of open-source code and trained model weights enhances the reproducibility of the research, enabling other researchers to build upon and validate the findings.\n\nWeaknesses: - The novelty of the proposed model structure is a combination of existing models: \n  - factorized codes and L2-normalized codes are from Improved VQGAN image model;\n  - Snake activation function from BigVGAN\n- This paper presents a strong audio compression technique. However, since the proposed novel points are specifically tailored for a narrow domain, their impact may be limited to the machine learning community and other domains like computer vision/NLP\n\nQuestions: - Have you attempted to apply a similar architecture to the vocoder in TTS? \n- Which components do you believe can be applied and generalized to other domains or tasks?\n\nLimitations: The authors have adequately addressed the limitations.\n\nSoundness: 3 good\n\nPresentation: 3 good\n\nContribution: 3 good\n\nEthics Review Flagged: ['No ethics review needed.']"
  },
  {
    "submission_id": "qjnl1QUnFA",
    "submission_title": "High-Fidelity Audio Compression with Improved RVQGAN",
    "review_id": "3GDoZw7Tv9",
    "reviewer": "Reviewer_B66v",
    "invitation": "NeurIPS.cc/2023/Conference/Submission14752/-/Official_Review",
    "rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",
    "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
    "review_text": "",
    "summary": "This paper introduces a RVQGAN-based neural audio codec method, demonstrating superior audio reconstruction quality, a high compression rate, and generalization across diverse audio domains. The authors substantiate the significant performance superiority of their model over alternatives through extensive and thorough qualitative and quantitative experiments. They present and validate their technique to fully utilize residual vector quantization, alongside model, discriminator, and loss design choices for enhanced performance.",
    "strengths": "* The paper addresses some of the key challenges in the neural audio codec domain.\n* The authors conducted strong and extensive experiments, providing comprehensive results.\n* The reference list appears to be thorough and comprehensive.\n* The authors support their findings by sharing the developed model, which is beneficial for the research community.\n",
    "weaknesses": "* The authors derived the proposed methods from existing studies and experimentally validate them in the neural audio codec domain. This approach seems to compromise the scientific novelty of the research.",
    "questions": "* Could the model be applied to downstream applications such as training text-to-speech (TTS) models? Previous works like EnCodec and SoundStream utilized causal architectures to make them suitable for in-context learning or prompting in TTS tasks.",
    "limitations": "The authors have adequately addressed both the limitations of their research and its possible societal impacts.",
    "soundness": "4 excellent",
    "presentation": "3 good",
    "contribution": "4 excellent",
    "flag_for_ethics_review": [
      "No ethics review needed."
    ],
    "details_of_ethics_concerns": "",
    "full_review": "Summary: This paper introduces a RVQGAN-based neural audio codec method, demonstrating superior audio reconstruction quality, a high compression rate, and generalization across diverse audio domains. The authors substantiate the significant performance superiority of their model over alternatives through extensive and thorough qualitative and quantitative experiments. They present and validate their technique to fully utilize residual vector quantization, alongside model, discriminator, and loss design choices for enhanced performance.\n\nStrengths: * The paper addresses some of the key challenges in the neural audio codec domain.\n* The authors conducted strong and extensive experiments, providing comprehensive results.\n* The reference list appears to be thorough and comprehensive.\n* The authors support their findings by sharing the developed model, which is beneficial for the research community.\n\n\nWeaknesses: * The authors derived the proposed methods from existing studies and experimentally validate them in the neural audio codec domain. This approach seems to compromise the scientific novelty of the research.\n\nQuestions: * Could the model be applied to downstream applications such as training text-to-speech (TTS) models? Previous works like EnCodec and SoundStream utilized causal architectures to make them suitable for in-context learning or prompting in TTS tasks.\n\nLimitations: The authors have adequately addressed both the limitations of their research and its possible societal impacts.\n\nSoundness: 4 excellent\n\nPresentation: 3 good\n\nContribution: 4 excellent\n\nEthics Review Flagged: ['No ethics review needed.']"
  },
  {
    "submission_id": "qjnl1QUnFA",
    "submission_title": "High-Fidelity Audio Compression with Improved RVQGAN",
    "review_id": "VRIhZMXFoC",
    "reviewer": "Reviewer_d8D3",
    "invitation": "NeurIPS.cc/2023/Conference/Submission14752/-/Official_Review",
    "rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",
    "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
    "review_text": "",
    "summary": "The authors propose a neural audio codec model that demonstrates superior performance compared to previous works, and present experimental results.",
    "strengths": "- The authors appropriately explain the problem they aim to address.\n- Their method is adequately described.\n- The authors provide a specific implementation, ensuring reproducibility.\n- The claims made are reasonable, and the experiments and results support them.\n- The authors' various ablation studies can be helpful for future work.\n",
    "weaknesses": "- For a neural audio codec to be utilized like traditional audio codecs, it should not fail in any patterns. Data-driven neural audio codecs have not been proven to be sufficiently stable from this perspective. Although the authors divided the original dataset into a training set and evaluation set, it is necessary to validate whether the proposed audio codec works well on more diverse and completely different audio data. Additionally, finding failure cases of previous works and comparing them can serve as strong evidence supporting the superiority of the authors' proposed method.",
    "questions": "Based on the MUSHRA scores curves, it appears that higher bitrates yield better scores, and the highest quality that this method can achieve remains unconfirmed. Is there a specific reason for this?",
    "limitations": "Limitations have been well described.",
    "soundness": "3 good",
    "presentation": "3 good",
    "contribution": "3 good",
    "flag_for_ethics_review": [
      "No ethics review needed."
    ],
    "details_of_ethics_concerns": "",
    "full_review": "Summary: The authors propose a neural audio codec model that demonstrates superior performance compared to previous works, and present experimental results.\n\nStrengths: - The authors appropriately explain the problem they aim to address.\n- Their method is adequately described.\n- The authors provide a specific implementation, ensuring reproducibility.\n- The claims made are reasonable, and the experiments and results support them.\n- The authors' various ablation studies can be helpful for future work.\n\n\nWeaknesses: - For a neural audio codec to be utilized like traditional audio codecs, it should not fail in any patterns. Data-driven neural audio codecs have not been proven to be sufficiently stable from this perspective. Although the authors divided the original dataset into a training set and evaluation set, it is necessary to validate whether the proposed audio codec works well on more diverse and completely different audio data. Additionally, finding failure cases of previous works and comparing them can serve as strong evidence supporting the superiority of the authors' proposed method.\n\nQuestions: Based on the MUSHRA scores curves, it appears that higher bitrates yield better scores, and the highest quality that this method can achieve remains unconfirmed. Is there a specific reason for this?\n\nLimitations: Limitations have been well described.\n\nSoundness: 3 good\n\nPresentation: 3 good\n\nContribution: 3 good\n\nEthics Review Flagged: ['No ethics review needed.']"
  }
]