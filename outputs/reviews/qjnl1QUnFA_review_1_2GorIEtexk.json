{
  "submission_id": "qjnl1QUnFA",
  "submission_title": "High-Fidelity Audio Compression with Improved RVQGAN",
  "review_index": 1,
  "review_id": "2GorIEtexk",
  "reviewer": "Reviewer_5w24",
  "invitation": "NeurIPS.cc/2023/Conference/Submission14752/-/Official_Review",
  "invitations": [
    "NeurIPS.cc/2023/Conference/Submission14752/-/Official_Review",
    "NeurIPS.cc/2023/Conference/-/Edit"
  ],
  "signatures": [
    "NeurIPS.cc/2023/Conference/Submission14752/Reviewer_5w24"
  ],
  "rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",
  "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
  "review_text": "",
  "summary": "In the present paper, the authors introduce RVQGAN, a neural audio codec that uses a convolutional encoder / decoder along with Residual Vector Quantization as a bottleneck, with a multi scale mel reconstruction loss and different adversarial losses.\nThey show state of the art performance from 3 to 8kbps, compared with the EnCodec model [8].\n\nThe key novelties are:\n- in each VQ layer, the authors perform the retrieval of the nearest codebook entry into a lower dimension space, and use cosine similarity instead of L2 distance to boost the utilization of the codebooks.\n- the authors drop the exponential moving average rule for learning the codebooks.\n- the author notice that the original technique from Soundstream [45] to select a varying number of quantizers can hurt the full bandwidth performance, and thus select 50% of the time all the quantizers in RVQ.\n- refinement of the losses and adversaries from previous work (in particular using different weights for different frequency bands).\n- balancing of the dataset to sample more often fullband audio.\n\nThe authors provide extensive ablation studies with objective metrics, and one subjective comparison with EnCodec with various bitrates.",
  "strengths": "- great execution and illustration of the various issues tackled here and the proposed solutions.\n- quality of the final model clearly surpasses the existing state of the art.\n- detailed ablation study with objective metrics.\n- single model for fullband audio over multiple audio domains.",
  "weaknesses": "- incremental improvement over previous work: overall method is coming from [45], adversarial losses are a combination of the one from [45] and [8]. Minor changes to the objective loss compared with [15, 45]. The authors however claim novelty: l.59, \"we make impactful design changes [...]: multi scale stft discriminator, multi scale losses\".\n- some details are unclear to me, in particular, the authors mention they do not use the EMA rule from [9]. How are the codebooks updated then? The authors also mention a low dimension projection, but do not mention when and how it is computed and updated. See questions.\n- no ablation with subjective evaluations: could have been interesting to clearly identify where most of the subjective gains is coming from e.g. is it from the quantizer, adversarial losses or dataset balancing?\n- seems like the authors do a comparison of a 24kHz baseline model with a 44.1kHz, keeping the ground truth as 44.1kHz, which can have a high impact on subjective and objective metrics independently of the design choices made by the authors. In particular the visqol for Encodec in Table 3 is much lower than reported in [8].",
  "questions": "As mentioned before, I would need more clarification on the exact algorithm used for VQ. Is a PCA computed ? if so how often is it updated (as the encoder output distribution might change). How are the codebooks updated ?\n\nIn Section 3.4, the architecture for the multi scale discriminator is missing. Is it the same one as [8] or [45]?\n\nParagraph starting 193: this insight has been noted and motivated before in [45], [5] and [40], it doesn't seem like the authors bring any new material evidence here?\n\nIt would be interesting to see the breakdown of Figure 3 by the category of audio.\n\nA last question would be over learnt snake activation parameters. Do the authors have any insight over the distribution of the learnt $\\alpha$? does this vary with the layer? I'm trying to get a better sense of how exactly the model is utilizing this feature.\n\n\n",
  "limitations": "authors properly address societal impact.",
  "soundness": "3 good",
  "presentation": "3 good",
  "contribution": "2 fair",
  "flag_for_ethics_review": [
    "No ethics review needed."
  ],
  "details_of_ethics_concerns": "",
  "combined_review_text": "In the present paper, the authors introduce RVQGAN, a neural audio codec that uses a convolutional encoder / decoder along with Residual Vector Quantization as a bottleneck, with a multi scale mel reconstruction loss and different adversarial losses.\nThey show state of the art performance from 3 to 8kbps, compared with the EnCodec model [8].\n\nThe key novelties are:\n- in each VQ layer, the authors perform the retrieval of the nearest codebook entry into a lower dimension space, and use cosine similarity instead of L2 distance to boost the utilization of the codebooks.\n- the authors drop the exponential moving average rule for learning the codebooks.\n- the author notice that the original technique from Soundstream [45] to select a varying number of quantizers can hurt the full bandwidth performance, and thus select 50% of the time all the quantizers in RVQ.\n- refinement of the losses and adversaries from previous work (in particular using different weights for different frequency bands).\n- balancing of the dataset to sample more often fullband audio.\n\nThe authors provide extensive ablation studies with objective metrics, and one subjective comparison with EnCodec with various bitrates.\n3 good\n3 good\n2 fair\n- great execution and illustration of the various issues tackled here and the proposed solutions.\n- quality of the final model clearly surpasses the existing state of the art.\n- detailed ablation study with objective metrics.\n- single model for fullband audio over multiple audio domains.\n- incremental improvement over previous work: overall method is coming from [45], adversarial losses are a combination of the one from [45] and [8]. Minor changes to the objective loss compared with [15, 45]. The authors however claim novelty: l.59, \"we make impactful design changes [...]: multi scale stft discriminator, multi scale losses\".\n- some details are unclear to me, in particular, the authors mention they do not use the EMA rule from [9]. How are the codebooks updated then? The authors also mention a low dimension projection, but do not mention when and how it is computed and updated. See questions.\n- no ablation with subjective evaluations: could have been interesting to clearly identify where most of the subjective gains is coming from e.g. is it from the quantizer, adversarial losses or dataset balancing?\n- seems like the authors do a comparison of a 24kHz baseline model with a 44.1kHz, keeping the ground truth as 44.1kHz, which can have a high impact on subjective and objective metrics independently of the design choices made by the authors. In particular the visqol for Encodec in Table 3 is much lower than reported in [8].\nAs mentioned before, I would need more clarification on the exact algorithm used for VQ. Is a PCA computed ? if so how often is it updated (as the encoder output distribution might change). How are the codebooks updated ?\n\nIn Section 3.4, the architecture for the multi scale discriminator is missing. Is it the same one as [8] or [45]?\n\nParagraph starting 193: this insight has been noted and motivated before in [45], [5] and [40], it doesn't seem like the authors bring any new material evidence here?\n\nIt would be interesting to see the breakdown of Figure 3 by the category of audio.\n\nA last question would be over learnt snake activation parameters. Do the authors have any insight over the distribution of the learnt $\\alpha$? does this vary with the layer? I'm trying to get a better sense of how exactly the model is utilizing this feature.\n\n\n\nauthors properly address societal impact.\n7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nYes"
}